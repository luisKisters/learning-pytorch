{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aP5cekqQmqHj",
        "outputId": "a1728d3b-447e-423d-cdb7-827ebb81a0be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn # nn containts basic building blocks for neural networks\n",
        "import matplotlib.pyplot as plt\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create known parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# Create \n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start, end, step). unsqueeze(dim=1)\n",
        "y = weight * X + bias"
      ],
      "metadata": {
        "id": "xR4Y5ygOn0GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3fqzHGWfSvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkhFfMpht5uO",
        "outputId": "1aae2a73-5435-4b15-b767-cf9e1de7ca74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = int(0.8 * len(X))"
      ],
      "metadata": {
        "id": "vwvyNubIwbA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]"
      ],
      "metadata": {
        "id": "dQBDIGoAwf5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qSwkKdXxXTt",
        "outputId": "c1e56126-1e2b-4212-e6c9-8b59002597c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train, \n",
        "                     train_labels=y_train, \n",
        "                     test_data=X_test, \n",
        "                     test_labels=y_test, \n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "  \n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions in red (predictions were made on the test data)\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14});"
      ],
      "metadata": {
        "id": "IPLvrbcqyvgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "swIAQa-lAVmi",
        "outputId": "31ec683d-388d-4bf6-ac58-c5719648e601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Linear Regression model class\n",
        "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
        "    def __init__(self):\n",
        "        super().__init__() \n",
        "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
        "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
        "                                                requires_grad=True) # <- can we update this value with gradient descent?)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
        "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
        "                                            requires_grad=True) # <- can we update this value with gradient descent?))\n",
        "\n",
        "    # Forward defines the computation in the model\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
        "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
      ],
      "metadata": {
        "id": "dzEmMXXOIHuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a instance of the model (this is a subclass of nn.Module)\n",
        "model_0 = LinearRegressionModel()\n",
        "\n",
        "# Checkout the Parameters\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYW_lF1DVXtl",
        "outputId": "496ac1f1-966d-4ace-827c-88182367f789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True), Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List named parameters\n",
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUjsXIHpe__j",
        "outputId": "04c476d7-d924-416a-a5c2-66b73fb232c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions using torch.inference_mode()\n",
        "with torch.inference_mode():\n",
        "    y_preds = model_0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDQLOLXrI02x",
        "outputId": "da90c684-049c-4392-8875-fd7495b3b374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "c8b8I1T8MPK3",
        "outputId": "533e55d2-5819-4a58-d083-d0da16b4667e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a loss function\n",
        "loss_fn=nn.L1Loss()\n",
        "\n",
        "# Setup an optimizer \n",
        "optimizer = torch.optim.SGD(\n",
        "                            params=model_0.parameters(), \n",
        "                            lr=0.001 # the smaller the lr, the smaller the change in the parameters, the larger the lr, the bigger the change in the parameters\n",
        "                            )"
      ],
      "metadata": {
        "id": "FhQsPcCmNA1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An epoch is one loop through data... (this is hyperparameter because we set it ourselves)\n",
        "epochs = 4000\n",
        "\n",
        "# Track different values\n",
        "epoch_count = [] \n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "### Training \n",
        "# 0. Loop through the data\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  model_0.train() # train mode in PyTorch sets all parameters that require gradients \n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model_0(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer \n",
        "  optimizer.step() # by default how the optimizer changes will accumulate through the loop so... we have to zero them above step 3 for the iteration of the loop\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval() # turn off gradient tracking\n",
        "  with torch.inference_mode(): # turns off gradient tracking & a couple more things behind the scenes - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=aftDZicoiUGiklEP179x7A\n",
        "    # with torch.no_grad(): # you may also see torch.no_grad() in older PyTorch code\n",
        "    # 1. Do the forward pass \n",
        "    test_preds = model_0(X_test)\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    test_loss = loss_fn(test_preds, y_test)\n",
        "\n",
        "    # Print out what's happenin'    \n",
        "    if epoch % (epochs//10) == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")    \n",
        "      epoch_count.append(epoch)\n",
        "      loss_values.append(loss)\n",
        "      test_loss_values.append(test_loss)\n",
        "\n",
        "print(f\"Epoch: {epochs} | Loss: {loss} | Test loss: {test_loss}\") "
      ],
      "metadata": {
        "id": "2eWr-QEkQipx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d13859b-dac9-469a-fcc4-9cd761119fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 0.3117292523384094 | Test loss: 0.4918419420719147\n",
            "Epoch: 2 | Loss: 0.3105771541595459 | Test loss: 0.49049490690231323\n",
            "Epoch: 3 | Loss: 0.3094250559806824 | Test loss: 0.48914775252342224\n",
            "Epoch: 4 | Loss: 0.30827295780181885 | Test loss: 0.4878006875514984\n",
            "Epoch: 5 | Loss: 0.3071208596229553 | Test loss: 0.4864535331726074\n",
            "Epoch: 6 | Loss: 0.3059687614440918 | Test loss: 0.4851064682006836\n",
            "Epoch: 7 | Loss: 0.3048166334629059 | Test loss: 0.48375946283340454\n",
            "Epoch: 8 | Loss: 0.30366456508636475 | Test loss: 0.48241233825683594\n",
            "Epoch: 9 | Loss: 0.3025124669075012 | Test loss: 0.48106521368026733\n",
            "Epoch: 10 | Loss: 0.3013603389263153 | Test loss: 0.4797181189060211\n",
            "Epoch: 11 | Loss: 0.30020827054977417 | Test loss: 0.4783710837364197\n",
            "Epoch: 12 | Loss: 0.29905614256858826 | Test loss: 0.4770239293575287\n",
            "Epoch: 13 | Loss: 0.2979040741920471 | Test loss: 0.47567683458328247\n",
            "Epoch: 14 | Loss: 0.2967519462108612 | Test loss: 0.47432971000671387\n",
            "Epoch: 15 | Loss: 0.29559987783432007 | Test loss: 0.47298264503479004\n",
            "Epoch: 16 | Loss: 0.29444774985313416 | Test loss: 0.47163552045822144\n",
            "Epoch: 17 | Loss: 0.293295681476593 | Test loss: 0.4702884256839752\n",
            "Epoch: 18 | Loss: 0.2921435832977295 | Test loss: 0.4689413607120514\n",
            "Epoch: 19 | Loss: 0.2909914553165436 | Test loss: 0.4675942361354828\n",
            "Epoch: 20 | Loss: 0.28983938694000244 | Test loss: 0.4662471413612366\n",
            "Epoch: 21 | Loss: 0.28868725895881653 | Test loss: 0.46490007638931274\n",
            "Epoch: 22 | Loss: 0.287535160779953 | Test loss: 0.46355295181274414\n",
            "Epoch: 23 | Loss: 0.2863830626010895 | Test loss: 0.4622058868408203\n",
            "Epoch: 24 | Loss: 0.28523099422454834 | Test loss: 0.4608587324619293\n",
            "Epoch: 25 | Loss: 0.2840788662433624 | Test loss: 0.4595116674900055\n",
            "Epoch: 26 | Loss: 0.2829267382621765 | Test loss: 0.4581645429134369\n",
            "Epoch: 27 | Loss: 0.281774640083313 | Test loss: 0.45681753754615784\n",
            "Epoch: 28 | Loss: 0.28062254190444946 | Test loss: 0.45547038316726685\n",
            "Epoch: 29 | Loss: 0.2794705033302307 | Test loss: 0.454123318195343\n",
            "Epoch: 30 | Loss: 0.2783183455467224 | Test loss: 0.4527761936187744\n",
            "Epoch: 31 | Loss: 0.2771662771701813 | Test loss: 0.4514291286468506\n",
            "Epoch: 32 | Loss: 0.27601414918899536 | Test loss: 0.450082004070282\n",
            "Epoch: 33 | Loss: 0.2748620808124542 | Test loss: 0.44873490929603577\n",
            "Epoch: 34 | Loss: 0.2737099528312683 | Test loss: 0.44738784432411194\n",
            "Epoch: 35 | Loss: 0.2725578844547272 | Test loss: 0.44604071974754333\n",
            "Epoch: 36 | Loss: 0.27140578627586365 | Test loss: 0.4446936249732971\n",
            "Epoch: 37 | Loss: 0.2702536880970001 | Test loss: 0.4433465003967285\n",
            "Epoch: 38 | Loss: 0.2691015899181366 | Test loss: 0.4419994354248047\n",
            "Epoch: 39 | Loss: 0.26794949173927307 | Test loss: 0.44065237045288086\n",
            "Epoch: 40 | Loss: 0.26679736375808716 | Test loss: 0.43930521607398987\n",
            "Epoch: 41 | Loss: 0.265645295381546 | Test loss: 0.43795815110206604\n",
            "Epoch: 42 | Loss: 0.2644931674003601 | Test loss: 0.43661102652549744\n",
            "Epoch: 43 | Loss: 0.2633410692214966 | Test loss: 0.4352639615535736\n",
            "Epoch: 44 | Loss: 0.26218897104263306 | Test loss: 0.4339168071746826\n",
            "Epoch: 45 | Loss: 0.26103687286376953 | Test loss: 0.43256980180740356\n",
            "Epoch: 46 | Loss: 0.259884774684906 | Test loss: 0.4312226176261902\n",
            "Epoch: 47 | Loss: 0.2587326467037201 | Test loss: 0.42987555265426636\n",
            "Epoch: 48 | Loss: 0.25758057832717896 | Test loss: 0.42852845788002014\n",
            "Epoch: 49 | Loss: 0.2564285099506378 | Test loss: 0.4271813929080963\n",
            "Epoch: 50 | Loss: 0.2552763819694519 | Test loss: 0.4258342385292053\n",
            "Epoch: 51 | Loss: 0.2541242837905884 | Test loss: 0.4244872033596039\n",
            "Epoch: 52 | Loss: 0.25297218561172485 | Test loss: 0.4231400489807129\n",
            "Epoch: 53 | Loss: 0.2518201172351837 | Test loss: 0.42179298400878906\n",
            "Epoch: 54 | Loss: 0.2506679892539978 | Test loss: 0.42044591903686523\n",
            "Epoch: 55 | Loss: 0.24951589107513428 | Test loss: 0.41909879446029663\n",
            "Epoch: 56 | Loss: 0.24836377799510956 | Test loss: 0.4177517294883728\n",
            "Epoch: 57 | Loss: 0.24721169471740723 | Test loss: 0.4164046347141266\n",
            "Epoch: 58 | Loss: 0.2460596114397049 | Test loss: 0.415057510137558\n",
            "Epoch: 59 | Loss: 0.2449074685573578 | Test loss: 0.41371044516563416\n",
            "Epoch: 60 | Loss: 0.24375538527965546 | Test loss: 0.41236335039138794\n",
            "Epoch: 61 | Loss: 0.24260330200195312 | Test loss: 0.41101622581481934\n",
            "Epoch: 62 | Loss: 0.2414511889219284 | Test loss: 0.40966910123825073\n",
            "Epoch: 63 | Loss: 0.2402990758419037 | Test loss: 0.4083220064640045\n",
            "Epoch: 64 | Loss: 0.23914699256420135 | Test loss: 0.4069749414920807\n",
            "Epoch: 65 | Loss: 0.23799486458301544 | Test loss: 0.4056278169155121\n",
            "Epoch: 66 | Loss: 0.2368427962064743 | Test loss: 0.40428075194358826\n",
            "Epoch: 67 | Loss: 0.23569071292877197 | Test loss: 0.40293365716934204\n",
            "Epoch: 68 | Loss: 0.23453858494758606 | Test loss: 0.4015865921974182\n",
            "Epoch: 69 | Loss: 0.23338651657104492 | Test loss: 0.4002394676208496\n",
            "Epoch: 70 | Loss: 0.232234388589859 | Test loss: 0.398892343044281\n",
            "Epoch: 71 | Loss: 0.23108229041099548 | Test loss: 0.3975452482700348\n",
            "Epoch: 72 | Loss: 0.22993019223213196 | Test loss: 0.3961981534957886\n",
            "Epoch: 73 | Loss: 0.22877809405326843 | Test loss: 0.39485102891921997\n",
            "Epoch: 74 | Loss: 0.2276259958744049 | Test loss: 0.39350399374961853\n",
            "Epoch: 75 | Loss: 0.22647389769554138 | Test loss: 0.3921568691730499\n",
            "Epoch: 76 | Loss: 0.22532181441783905 | Test loss: 0.3908098042011261\n",
            "Epoch: 77 | Loss: 0.22416970133781433 | Test loss: 0.3894627094268799\n",
            "Epoch: 78 | Loss: 0.223017618060112 | Test loss: 0.3881155848503113\n",
            "Epoch: 79 | Loss: 0.2218654900789261 | Test loss: 0.38676849007606506\n",
            "Epoch: 80 | Loss: 0.22071340680122375 | Test loss: 0.38542139530181885\n",
            "Epoch: 81 | Loss: 0.21956129372119904 | Test loss: 0.38407427072525024\n",
            "Epoch: 82 | Loss: 0.2184092104434967 | Test loss: 0.3827272057533264\n",
            "Epoch: 83 | Loss: 0.21725709736347198 | Test loss: 0.3813800811767578\n",
            "Epoch: 84 | Loss: 0.21610502898693085 | Test loss: 0.38003304600715637\n",
            "Epoch: 85 | Loss: 0.21495290100574493 | Test loss: 0.3786858916282654\n",
            "Epoch: 86 | Loss: 0.2138008177280426 | Test loss: 0.37733882665634155\n",
            "Epoch: 87 | Loss: 0.21264871954917908 | Test loss: 0.37599173188209534\n",
            "Epoch: 88 | Loss: 0.21149662137031555 | Test loss: 0.3746446371078491\n",
            "Epoch: 89 | Loss: 0.21034450829029083 | Test loss: 0.3732975423336029\n",
            "Epoch: 90 | Loss: 0.2091923952102661 | Test loss: 0.3719504475593567\n",
            "Epoch: 91 | Loss: 0.20804031193256378 | Test loss: 0.3706033527851105\n",
            "Epoch: 92 | Loss: 0.20688819885253906 | Test loss: 0.36925622820854187\n",
            "Epoch: 93 | Loss: 0.20573611557483673 | Test loss: 0.36790916323661804\n",
            "Epoch: 94 | Loss: 0.2045840322971344 | Test loss: 0.3665620684623718\n",
            "Epoch: 95 | Loss: 0.20343191921710968 | Test loss: 0.36521491408348083\n",
            "Epoch: 96 | Loss: 0.20227983593940735 | Test loss: 0.3638678789138794\n",
            "Epoch: 97 | Loss: 0.20112769305706024 | Test loss: 0.3625207543373108\n",
            "Epoch: 98 | Loss: 0.1999756097793579 | Test loss: 0.36117368936538696\n",
            "Epoch: 99 | Loss: 0.1988234966993332 | Test loss: 0.35982653498649597\n",
            "Epoch: 100 | Loss: 0.19767141342163086 | Test loss: 0.35847947001457214\n",
            "Epoch: 101 | Loss: 0.19651933014392853 | Test loss: 0.3571323752403259\n",
            "Epoch: 102 | Loss: 0.19536720216274261 | Test loss: 0.3557852506637573\n",
            "Epoch: 103 | Loss: 0.19421511888504028 | Test loss: 0.3544381558895111\n",
            "Epoch: 104 | Loss: 0.19306302070617676 | Test loss: 0.35309112071990967\n",
            "Epoch: 105 | Loss: 0.19191092252731323 | Test loss: 0.35174399614334106\n",
            "Epoch: 106 | Loss: 0.19075879454612732 | Test loss: 0.35039690136909485\n",
            "Epoch: 107 | Loss: 0.189606711268425 | Test loss: 0.34904980659484863\n",
            "Epoch: 108 | Loss: 0.18845462799072266 | Test loss: 0.3477027118206024\n",
            "Epoch: 109 | Loss: 0.18730251491069794 | Test loss: 0.3463556468486786\n",
            "Epoch: 110 | Loss: 0.18615040183067322 | Test loss: 0.34500852227211\n",
            "Epoch: 111 | Loss: 0.18499833345413208 | Test loss: 0.34366142749786377\n",
            "Epoch: 112 | Loss: 0.18384622037410736 | Test loss: 0.34231433272361755\n",
            "Epoch: 113 | Loss: 0.18269412219524384 | Test loss: 0.3409672677516937\n",
            "Epoch: 114 | Loss: 0.1815420240163803 | Test loss: 0.3396201729774475\n",
            "Epoch: 115 | Loss: 0.18038992583751678 | Test loss: 0.3382730484008789\n",
            "Epoch: 116 | Loss: 0.17923781275749207 | Test loss: 0.3369259238243103\n",
            "Epoch: 117 | Loss: 0.17808572947978973 | Test loss: 0.3355788290500641\n",
            "Epoch: 118 | Loss: 0.17693361639976501 | Test loss: 0.33423173427581787\n",
            "Epoch: 119 | Loss: 0.1757815182209015 | Test loss: 0.33288463950157166\n",
            "Epoch: 120 | Loss: 0.17462942004203796 | Test loss: 0.33153754472732544\n",
            "Epoch: 121 | Loss: 0.17347732186317444 | Test loss: 0.3301904797554016\n",
            "Epoch: 122 | Loss: 0.1723252385854721 | Test loss: 0.3288434147834778\n",
            "Epoch: 123 | Loss: 0.17117314040660858 | Test loss: 0.3274962902069092\n",
            "Epoch: 124 | Loss: 0.17002105712890625 | Test loss: 0.3261492848396301\n",
            "Epoch: 125 | Loss: 0.1688689887523651 | Test loss: 0.3248021602630615\n",
            "Epoch: 126 | Loss: 0.16771690547466278 | Test loss: 0.3234550952911377\n",
            "Epoch: 127 | Loss: 0.16656479239463806 | Test loss: 0.32210803031921387\n",
            "Epoch: 128 | Loss: 0.16541273891925812 | Test loss: 0.32076090574264526\n",
            "Epoch: 129 | Loss: 0.1642606407403946 | Test loss: 0.31941384077072144\n",
            "Epoch: 130 | Loss: 0.16310855746269226 | Test loss: 0.3180667459964752\n",
            "Epoch: 131 | Loss: 0.16195647418498993 | Test loss: 0.3167196810245514\n",
            "Epoch: 132 | Loss: 0.1608043909072876 | Test loss: 0.3153725862503052\n",
            "Epoch: 133 | Loss: 0.15965232253074646 | Test loss: 0.31402549147605896\n",
            "Epoch: 134 | Loss: 0.15850022435188293 | Test loss: 0.31267839670181274\n",
            "Epoch: 135 | Loss: 0.1573481261730194 | Test loss: 0.3113313317298889\n",
            "Epoch: 136 | Loss: 0.15619607269763947 | Test loss: 0.3099842667579651\n",
            "Epoch: 137 | Loss: 0.15504398941993713 | Test loss: 0.30863717198371887\n",
            "Epoch: 138 | Loss: 0.1538918912410736 | Test loss: 0.30729013681411743\n",
            "Epoch: 139 | Loss: 0.15273980796337128 | Test loss: 0.30594301223754883\n",
            "Epoch: 140 | Loss: 0.15158770978450775 | Test loss: 0.304595947265625\n",
            "Epoch: 141 | Loss: 0.1504356414079666 | Test loss: 0.30324888229370117\n",
            "Epoch: 142 | Loss: 0.14928355813026428 | Test loss: 0.30190175771713257\n",
            "Epoch: 143 | Loss: 0.14813147485256195 | Test loss: 0.30055469274520874\n",
            "Epoch: 144 | Loss: 0.14697939157485962 | Test loss: 0.2992075979709625\n",
            "Epoch: 145 | Loss: 0.1458273082971573 | Test loss: 0.2978605329990387\n",
            "Epoch: 146 | Loss: 0.14467521011829376 | Test loss: 0.2965134382247925\n",
            "Epoch: 147 | Loss: 0.14352312684059143 | Test loss: 0.29516634345054626\n",
            "Epoch: 148 | Loss: 0.1423710286617279 | Test loss: 0.29381924867630005\n",
            "Epoch: 149 | Loss: 0.14121896028518677 | Test loss: 0.292472243309021\n",
            "Epoch: 150 | Loss: 0.14006686210632324 | Test loss: 0.2911251187324524\n",
            "Epoch: 151 | Loss: 0.1389147937297821 | Test loss: 0.28977805376052856\n",
            "Epoch: 152 | Loss: 0.13776269555091858 | Test loss: 0.28843095898628235\n",
            "Epoch: 153 | Loss: 0.13661061227321625 | Test loss: 0.28708386421203613\n",
            "Epoch: 154 | Loss: 0.1354585438966751 | Test loss: 0.2857367992401123\n",
            "Epoch: 155 | Loss: 0.13430646061897278 | Test loss: 0.2843897342681885\n",
            "Epoch: 156 | Loss: 0.13315437734127045 | Test loss: 0.28304266929626465\n",
            "Epoch: 157 | Loss: 0.13200227916240692 | Test loss: 0.28169554471969604\n",
            "Epoch: 158 | Loss: 0.13085021078586578 | Test loss: 0.2803484797477722\n",
            "Epoch: 159 | Loss: 0.12969812750816345 | Test loss: 0.279001384973526\n",
            "Epoch: 160 | Loss: 0.12854602932929993 | Test loss: 0.2776543200016022\n",
            "Epoch: 161 | Loss: 0.1273939311504364 | Test loss: 0.27630725502967834\n",
            "Epoch: 162 | Loss: 0.12624187767505646 | Test loss: 0.27496016025543213\n",
            "Epoch: 163 | Loss: 0.12508977949619293 | Test loss: 0.2736130952835083\n",
            "Epoch: 164 | Loss: 0.12393768876791 | Test loss: 0.2722659707069397\n",
            "Epoch: 165 | Loss: 0.12278560549020767 | Test loss: 0.27091890573501587\n",
            "Epoch: 166 | Loss: 0.12163352966308594 | Test loss: 0.26957181096076965\n",
            "Epoch: 167 | Loss: 0.1204814463853836 | Test loss: 0.2682247459888458\n",
            "Epoch: 168 | Loss: 0.11932935565710068 | Test loss: 0.2668776512145996\n",
            "Epoch: 169 | Loss: 0.11817727237939835 | Test loss: 0.2655305862426758\n",
            "Epoch: 170 | Loss: 0.11702518165111542 | Test loss: 0.26418352127075195\n",
            "Epoch: 171 | Loss: 0.11587311327457428 | Test loss: 0.26283639669418335\n",
            "Epoch: 172 | Loss: 0.11476147174835205 | Test loss: 0.2615393400192261\n",
            "Epoch: 173 | Loss: 0.11370686441659927 | Test loss: 0.260242223739624\n",
            "Epoch: 174 | Loss: 0.1126522570848465 | Test loss: 0.258945107460022\n",
            "Epoch: 175 | Loss: 0.11159764230251312 | Test loss: 0.2576480209827423\n",
            "Epoch: 176 | Loss: 0.11054304987192154 | Test loss: 0.25635093450546265\n",
            "Epoch: 177 | Loss: 0.10948844254016876 | Test loss: 0.2550538182258606\n",
            "Epoch: 178 | Loss: 0.10846539586782455 | Test loss: 0.25380760431289673\n",
            "Epoch: 179 | Loss: 0.10750406980514526 | Test loss: 0.2525613605976105\n",
            "Epoch: 180 | Loss: 0.10654274374246597 | Test loss: 0.251315176486969\n",
            "Epoch: 181 | Loss: 0.10558142513036728 | Test loss: 0.25006893277168274\n",
            "Epoch: 182 | Loss: 0.10462009906768799 | Test loss: 0.24882273375988007\n",
            "Epoch: 183 | Loss: 0.1036587730050087 | Test loss: 0.247576504945755\n",
            "Epoch: 184 | Loss: 0.10270978510379791 | Test loss: 0.24638208746910095\n",
            "Epoch: 185 | Loss: 0.10183751583099365 | Test loss: 0.2451876401901245\n",
            "Epoch: 186 | Loss: 0.10096526145935059 | Test loss: 0.24399319291114807\n",
            "Epoch: 187 | Loss: 0.10009298473596573 | Test loss: 0.24279877543449402\n",
            "Epoch: 188 | Loss: 0.09922071546316147 | Test loss: 0.24160432815551758\n",
            "Epoch: 189 | Loss: 0.09834843873977661 | Test loss: 0.24040989577770233\n",
            "Epoch: 190 | Loss: 0.09747617691755295 | Test loss: 0.2392154484987259\n",
            "Epoch: 191 | Loss: 0.09663032740354538 | Test loss: 0.2380737066268921\n",
            "Epoch: 192 | Loss: 0.09584285318851471 | Test loss: 0.23693189024925232\n",
            "Epoch: 193 | Loss: 0.09505538642406464 | Test loss: 0.23579013347625732\n",
            "Epoch: 194 | Loss: 0.09426790475845337 | Test loss: 0.23464834690093994\n",
            "Epoch: 195 | Loss: 0.09348044544458389 | Test loss: 0.23350660502910614\n",
            "Epoch: 196 | Loss: 0.09269297122955322 | Test loss: 0.23236480355262756\n",
            "Epoch: 197 | Loss: 0.09190551191568375 | Test loss: 0.23122303187847137\n",
            "Epoch: 198 | Loss: 0.09114636480808258 | Test loss: 0.23013481497764587\n",
            "Epoch: 199 | Loss: 0.09043945372104645 | Test loss: 0.22904661297798157\n",
            "Epoch: 200 | Loss: 0.08973254263401031 | Test loss: 0.22795839607715607\n",
            "Epoch: 201 | Loss: 0.08902563154697418 | Test loss: 0.22687020897865295\n",
            "Epoch: 202 | Loss: 0.08831872791051865 | Test loss: 0.22578194737434387\n",
            "Epoch: 203 | Loss: 0.08761182427406311 | Test loss: 0.22469374537467957\n",
            "Epoch: 204 | Loss: 0.08690490573644638 | Test loss: 0.22360554337501526\n",
            "Epoch: 205 | Loss: 0.08621595799922943 | Test loss: 0.22257176041603088\n",
            "Epoch: 206 | Loss: 0.0855853408575058 | Test loss: 0.22153803706169128\n",
            "Epoch: 207 | Loss: 0.08495471626520157 | Test loss: 0.2205042839050293\n",
            "Epoch: 208 | Loss: 0.08432409167289734 | Test loss: 0.2194705307483673\n",
            "Epoch: 209 | Loss: 0.08369346708059311 | Test loss: 0.21843676269054413\n",
            "Epoch: 210 | Loss: 0.08306284993886948 | Test loss: 0.21740305423736572\n",
            "Epoch: 211 | Loss: 0.08243221789598465 | Test loss: 0.21636930108070374\n",
            "Epoch: 212 | Loss: 0.08180160820484161 | Test loss: 0.21533556282520294\n",
            "Epoch: 213 | Loss: 0.08120343089103699 | Test loss: 0.21435710787773132\n",
            "Epoch: 214 | Loss: 0.08064477890729904 | Test loss: 0.21337871253490448\n",
            "Epoch: 215 | Loss: 0.0800861120223999 | Test loss: 0.21240031719207764\n",
            "Epoch: 216 | Loss: 0.07952746003866196 | Test loss: 0.2114218920469284\n",
            "Epoch: 217 | Loss: 0.07896880060434341 | Test loss: 0.21044349670410156\n",
            "Epoch: 218 | Loss: 0.07841013371944427 | Test loss: 0.20946505665779114\n",
            "Epoch: 219 | Loss: 0.07785148918628693 | Test loss: 0.2084866464138031\n",
            "Epoch: 220 | Loss: 0.07729282230138779 | Test loss: 0.20750825107097626\n",
            "Epoch: 221 | Loss: 0.07676678895950317 | Test loss: 0.206586092710495\n",
            "Epoch: 222 | Loss: 0.07627572864294052 | Test loss: 0.20566387474536896\n",
            "Epoch: 223 | Loss: 0.07578467577695847 | Test loss: 0.20474162697792053\n",
            "Epoch: 224 | Loss: 0.07529362291097641 | Test loss: 0.20381946861743927\n",
            "Epoch: 225 | Loss: 0.07480257004499435 | Test loss: 0.20289726555347443\n",
            "Epoch: 226 | Loss: 0.0743115097284317 | Test loss: 0.20197506248950958\n",
            "Epoch: 227 | Loss: 0.07382047176361084 | Test loss: 0.20105287432670593\n",
            "Epoch: 228 | Loss: 0.07332941144704819 | Test loss: 0.2001306712627411\n",
            "Epoch: 229 | Loss: 0.072856605052948 | Test loss: 0.19926562905311584\n",
            "Epoch: 230 | Loss: 0.07242877781391144 | Test loss: 0.19840054214000702\n",
            "Epoch: 231 | Loss: 0.07200097292661667 | Test loss: 0.19753551483154297\n",
            "Epoch: 232 | Loss: 0.0715731531381607 | Test loss: 0.19667045772075653\n",
            "Epoch: 233 | Loss: 0.07114534080028534 | Test loss: 0.19580543041229248\n",
            "Epoch: 234 | Loss: 0.07071752846240997 | Test loss: 0.19494035840034485\n",
            "Epoch: 235 | Loss: 0.0702897235751152 | Test loss: 0.19407527148723602\n",
            "Epoch: 236 | Loss: 0.06986190378665924 | Test loss: 0.19321021437644958\n",
            "Epoch: 237 | Loss: 0.06943409144878387 | Test loss: 0.19234518706798553\n",
            "Epoch: 238 | Loss: 0.06902603805065155 | Test loss: 0.19153812527656555\n",
            "Epoch: 239 | Loss: 0.06865701824426651 | Test loss: 0.19073110818862915\n",
            "Epoch: 240 | Loss: 0.06828799843788147 | Test loss: 0.18992407619953156\n",
            "Epoch: 241 | Loss: 0.06791899353265762 | Test loss: 0.18911704421043396\n",
            "Epoch: 242 | Loss: 0.06754995882511139 | Test loss: 0.18830999732017517\n",
            "Epoch: 243 | Loss: 0.06718094646930695 | Test loss: 0.18750298023223877\n",
            "Epoch: 244 | Loss: 0.0668119341135025 | Test loss: 0.18669594824314117\n",
            "Epoch: 245 | Loss: 0.06644289940595627 | Test loss: 0.18588891625404358\n",
            "Epoch: 246 | Loss: 0.06607388705015182 | Test loss: 0.1850818693637848\n",
            "Epoch: 247 | Loss: 0.06570921093225479 | Test loss: 0.18433372676372528\n",
            "Epoch: 248 | Loss: 0.06539449840784073 | Test loss: 0.18358558416366577\n",
            "Epoch: 249 | Loss: 0.06507977098226547 | Test loss: 0.18283741176128387\n",
            "Epoch: 250 | Loss: 0.06476505100727081 | Test loss: 0.18208928406238556\n",
            "Epoch: 251 | Loss: 0.06445033103227615 | Test loss: 0.18134114146232605\n",
            "Epoch: 252 | Loss: 0.0641356036067009 | Test loss: 0.18059298396110535\n",
            "Epoch: 253 | Loss: 0.06382088363170624 | Test loss: 0.17984485626220703\n",
            "Epoch: 254 | Loss: 0.06350617110729218 | Test loss: 0.17909672856330872\n",
            "Epoch: 255 | Loss: 0.06319145113229752 | Test loss: 0.178348571062088\n",
            "Epoch: 256 | Loss: 0.06287673115730286 | Test loss: 0.1776004284620285\n",
            "Epoch: 257 | Loss: 0.0625620037317276 | Test loss: 0.1768522709608078\n",
            "Epoch: 258 | Loss: 0.062271296977996826 | Test loss: 0.1761639267206192\n",
            "Epoch: 259 | Loss: 0.06200631707906723 | Test loss: 0.17547550797462463\n",
            "Epoch: 260 | Loss: 0.06174134090542793 | Test loss: 0.17478716373443604\n",
            "Epoch: 261 | Loss: 0.06147634983062744 | Test loss: 0.17409880459308624\n",
            "Epoch: 262 | Loss: 0.061211369931697845 | Test loss: 0.17341041564941406\n",
            "Epoch: 263 | Loss: 0.06094638630747795 | Test loss: 0.17272204160690308\n",
            "Epoch: 264 | Loss: 0.06068141385912895 | Test loss: 0.1720336377620697\n",
            "Epoch: 265 | Loss: 0.06041641905903816 | Test loss: 0.1713452786207199\n",
            "Epoch: 266 | Loss: 0.060151439160108566 | Test loss: 0.17065691947937012\n",
            "Epoch: 267 | Loss: 0.05988645553588867 | Test loss: 0.16996853053569794\n",
            "Epoch: 268 | Loss: 0.05962147191166878 | Test loss: 0.16928015649318695\n",
            "Epoch: 269 | Loss: 0.05937860533595085 | Test loss: 0.1686524599790573\n",
            "Epoch: 270 | Loss: 0.05915876477956772 | Test loss: 0.16802480816841125\n",
            "Epoch: 271 | Loss: 0.058938927948474884 | Test loss: 0.16739711165428162\n",
            "Epoch: 272 | Loss: 0.05871908739209175 | Test loss: 0.16676943004131317\n",
            "Epoch: 273 | Loss: 0.05849923565983772 | Test loss: 0.16614174842834473\n",
            "Epoch: 274 | Loss: 0.05827939510345459 | Test loss: 0.16551408171653748\n",
            "Epoch: 275 | Loss: 0.05805954337120056 | Test loss: 0.16488640010356903\n",
            "Epoch: 276 | Loss: 0.05783969908952713 | Test loss: 0.16425874829292297\n",
            "Epoch: 277 | Loss: 0.057619858533144 | Test loss: 0.16363103687763214\n",
            "Epoch: 278 | Loss: 0.05740001052618027 | Test loss: 0.1630033701658249\n",
            "Epoch: 279 | Loss: 0.05718017369508743 | Test loss: 0.16237568855285645\n",
            "Epoch: 280 | Loss: 0.0569603331387043 | Test loss: 0.161748006939888\n",
            "Epoch: 281 | Loss: 0.05676015466451645 | Test loss: 0.16118189692497253\n",
            "Epoch: 282 | Loss: 0.05658075958490372 | Test loss: 0.16061577200889587\n",
            "Epoch: 283 | Loss: 0.05640135332942009 | Test loss: 0.1600496768951416\n",
            "Epoch: 284 | Loss: 0.05622195079922676 | Test loss: 0.15948358178138733\n",
            "Epoch: 285 | Loss: 0.05604255199432373 | Test loss: 0.15891747176647186\n",
            "Epoch: 286 | Loss: 0.0558631531894207 | Test loss: 0.1583513766527176\n",
            "Epoch: 287 | Loss: 0.05568375438451767 | Test loss: 0.15778525173664093\n",
            "Epoch: 288 | Loss: 0.05550435930490494 | Test loss: 0.15721914172172546\n",
            "Epoch: 289 | Loss: 0.05532495304942131 | Test loss: 0.1566530466079712\n",
            "Epoch: 290 | Loss: 0.05514555424451828 | Test loss: 0.15608695149421692\n",
            "Epoch: 291 | Loss: 0.05496615171432495 | Test loss: 0.15552084147930145\n",
            "Epoch: 292 | Loss: 0.05478675290942192 | Test loss: 0.1549547165632248\n",
            "Epoch: 293 | Loss: 0.05460735410451889 | Test loss: 0.15438862144947052\n",
            "Epoch: 294 | Loss: 0.05443967133760452 | Test loss: 0.15388496220111847\n",
            "Epoch: 295 | Loss: 0.05429593846201897 | Test loss: 0.15338130295276642\n",
            "Epoch: 296 | Loss: 0.05415221303701401 | Test loss: 0.15287765860557556\n",
            "Epoch: 297 | Loss: 0.05400847643613815 | Test loss: 0.15237398445606232\n",
            "Epoch: 298 | Loss: 0.05386475846171379 | Test loss: 0.15187029540538788\n",
            "Epoch: 299 | Loss: 0.05372103303670883 | Test loss: 0.15136666595935822\n",
            "Epoch: 300 | Loss: 0.05357731133699417 | Test loss: 0.15086300671100616\n",
            "Epoch: 301 | Loss: 0.05343357473611832 | Test loss: 0.1503593474626541\n",
            "Epoch: 302 | Loss: 0.05328984931111336 | Test loss: 0.14985567331314087\n",
            "Epoch: 303 | Loss: 0.0531461238861084 | Test loss: 0.14935199916362762\n",
            "Epoch: 304 | Loss: 0.05300239473581314 | Test loss: 0.14884835481643677\n",
            "Epoch: 305 | Loss: 0.052858661860227585 | Test loss: 0.14834468066692352\n",
            "Epoch: 306 | Loss: 0.052714936435222626 | Test loss: 0.14784102141857147\n",
            "Epoch: 307 | Loss: 0.052571214735507965 | Test loss: 0.1473373919725418\n",
            "Epoch: 308 | Loss: 0.05242748185992241 | Test loss: 0.14683370292186737\n",
            "Epoch: 309 | Loss: 0.05229362100362778 | Test loss: 0.14639338850975037\n",
            "Epoch: 310 | Loss: 0.05218071490526199 | Test loss: 0.14595307409763336\n",
            "Epoch: 311 | Loss: 0.05206780880689621 | Test loss: 0.14551277458667755\n",
            "Epoch: 312 | Loss: 0.05195491388440132 | Test loss: 0.14507248997688293\n",
            "Epoch: 313 | Loss: 0.05184202268719673 | Test loss: 0.14463214576244354\n",
            "Epoch: 314 | Loss: 0.05172910541296005 | Test loss: 0.14419183135032654\n",
            "Epoch: 315 | Loss: 0.05161619931459427 | Test loss: 0.14375153183937073\n",
            "Epoch: 316 | Loss: 0.05150330066680908 | Test loss: 0.14331121742725372\n",
            "Epoch: 317 | Loss: 0.0513903982937336 | Test loss: 0.14287090301513672\n",
            "Epoch: 318 | Loss: 0.05127749592065811 | Test loss: 0.14243058860301971\n",
            "Epoch: 319 | Loss: 0.05116458982229233 | Test loss: 0.1419902741909027\n",
            "Epoch: 320 | Loss: 0.05105169489979744 | Test loss: 0.1415499746799469\n",
            "Epoch: 321 | Loss: 0.05093878507614136 | Test loss: 0.1411096602678299\n",
            "Epoch: 322 | Loss: 0.05082588270306587 | Test loss: 0.1406693160533905\n",
            "Epoch: 323 | Loss: 0.05071298032999039 | Test loss: 0.1402290314435959\n",
            "Epoch: 324 | Loss: 0.0506000742316246 | Test loss: 0.13978871703147888\n",
            "Epoch: 325 | Loss: 0.050487179309129715 | Test loss: 0.13934841752052307\n",
            "Epoch: 326 | Loss: 0.050378382205963135 | Test loss: 0.13897234201431274\n",
            "Epoch: 327 | Loss: 0.050291359424591064 | Test loss: 0.1385962963104248\n",
            "Epoch: 328 | Loss: 0.050204355269670486 | Test loss: 0.13822023570537567\n",
            "Epoch: 329 | Loss: 0.050117332488298416 | Test loss: 0.13784417510032654\n",
            "Epoch: 330 | Loss: 0.050030313432216644 | Test loss: 0.1374681293964386\n",
            "Epoch: 331 | Loss: 0.049943309277296066 | Test loss: 0.13709203898906708\n",
            "Epoch: 332 | Loss: 0.0498562827706337 | Test loss: 0.13671600818634033\n",
            "Epoch: 333 | Loss: 0.049769263714551926 | Test loss: 0.13633993268013\n",
            "Epoch: 334 | Loss: 0.04968224838376045 | Test loss: 0.13596387207508087\n",
            "Epoch: 335 | Loss: 0.04959522932767868 | Test loss: 0.13558778166770935\n",
            "Epoch: 336 | Loss: 0.049508217722177505 | Test loss: 0.1352117508649826\n",
            "Epoch: 337 | Loss: 0.04942120611667633 | Test loss: 0.13483569025993347\n",
            "Epoch: 338 | Loss: 0.04933418333530426 | Test loss: 0.13445964455604553\n",
            "Epoch: 339 | Loss: 0.04924716800451279 | Test loss: 0.1340835690498352\n",
            "Epoch: 340 | Loss: 0.049160152673721313 | Test loss: 0.13370750844478607\n",
            "Epoch: 341 | Loss: 0.04907313734292984 | Test loss: 0.13333146274089813\n",
            "Epoch: 342 | Loss: 0.04898611828684807 | Test loss: 0.1329553872346878\n",
            "Epoch: 343 | Loss: 0.048899102956056595 | Test loss: 0.13257935643196106\n",
            "Epoch: 344 | Loss: 0.04881208389997482 | Test loss: 0.13220326602458954\n",
            "Epoch: 345 | Loss: 0.04872506856918335 | Test loss: 0.1318272203207016\n",
            "Epoch: 346 | Loss: 0.048638053238391876 | Test loss: 0.13145115971565247\n",
            "Epoch: 347 | Loss: 0.04855869710445404 | Test loss: 0.13114026188850403\n",
            "Epoch: 348 | Loss: 0.048492539674043655 | Test loss: 0.1308293491601944\n",
            "Epoch: 349 | Loss: 0.04842637851834297 | Test loss: 0.13051843643188477\n",
            "Epoch: 350 | Loss: 0.04836020991206169 | Test loss: 0.13020753860473633\n",
            "Epoch: 351 | Loss: 0.04829404503107071 | Test loss: 0.1298966407775879\n",
            "Epoch: 352 | Loss: 0.04822787642478943 | Test loss: 0.12958571314811707\n",
            "Epoch: 353 | Loss: 0.048161718994379044 | Test loss: 0.12927480041980743\n",
            "Epoch: 354 | Loss: 0.048095546662807465 | Test loss: 0.128963902592659\n",
            "Epoch: 355 | Loss: 0.04802938550710678 | Test loss: 0.12865300476551056\n",
            "Epoch: 356 | Loss: 0.0479632243514061 | Test loss: 0.12834210693836212\n",
            "Epoch: 357 | Loss: 0.04789705574512482 | Test loss: 0.1280311793088913\n",
            "Epoch: 358 | Loss: 0.04783089831471443 | Test loss: 0.12772028148174286\n",
            "Epoch: 359 | Loss: 0.04776472598314285 | Test loss: 0.12740938365459442\n",
            "Epoch: 360 | Loss: 0.04769856110215187 | Test loss: 0.12709848582744598\n",
            "Epoch: 361 | Loss: 0.04763239622116089 | Test loss: 0.12678757309913635\n",
            "Epoch: 362 | Loss: 0.04756623134016991 | Test loss: 0.12647667527198792\n",
            "Epoch: 363 | Loss: 0.04750007390975952 | Test loss: 0.12616576254367828\n",
            "Epoch: 364 | Loss: 0.04743390530347824 | Test loss: 0.12585484981536865\n",
            "Epoch: 365 | Loss: 0.04736773669719696 | Test loss: 0.12554392218589783\n",
            "Epoch: 366 | Loss: 0.04730157181620598 | Test loss: 0.1252330243587494\n",
            "Epoch: 367 | Loss: 0.0472353920340538 | Test loss: 0.12492211908102036\n",
            "Epoch: 368 | Loss: 0.047169238328933716 | Test loss: 0.12461123615503311\n",
            "Epoch: 369 | Loss: 0.047103073447942734 | Test loss: 0.12430031597614288\n",
            "Epoch: 370 | Loss: 0.04703690856695175 | Test loss: 0.12398938834667206\n",
            "Epoch: 371 | Loss: 0.04697074741125107 | Test loss: 0.12367852032184601\n",
            "Epoch: 372 | Loss: 0.04690460115671158 | Test loss: 0.12343358993530273\n",
            "Epoch: 373 | Loss: 0.04685414209961891 | Test loss: 0.12318868935108185\n",
            "Epoch: 374 | Loss: 0.046803683042526245 | Test loss: 0.12294378131628036\n",
            "Epoch: 375 | Loss: 0.046753231436014175 | Test loss: 0.12269886583089828\n",
            "Epoch: 376 | Loss: 0.04670276492834091 | Test loss: 0.122453972697258\n",
            "Epoch: 377 | Loss: 0.046652309596538544 | Test loss: 0.12220907211303711\n",
            "Epoch: 378 | Loss: 0.04660185053944588 | Test loss: 0.12196417152881622\n",
            "Epoch: 379 | Loss: 0.04655139893293381 | Test loss: 0.12171925604343414\n",
            "Epoch: 380 | Loss: 0.04650093987584114 | Test loss: 0.12147434055805206\n",
            "Epoch: 381 | Loss: 0.04645047336816788 | Test loss: 0.12122943252325058\n",
            "Epoch: 382 | Loss: 0.04640001803636551 | Test loss: 0.1209845319390297\n",
            "Epoch: 383 | Loss: 0.046349555253982544 | Test loss: 0.12073962390422821\n",
            "Epoch: 384 | Loss: 0.04629909247159958 | Test loss: 0.12049472332000732\n",
            "Epoch: 385 | Loss: 0.04624864086508751 | Test loss: 0.12024979293346405\n",
            "Epoch: 386 | Loss: 0.046198178082704544 | Test loss: 0.12000487744808197\n",
            "Epoch: 387 | Loss: 0.046147722750902176 | Test loss: 0.11975999176502228\n",
            "Epoch: 388 | Loss: 0.04609726741909981 | Test loss: 0.1195150837302208\n",
            "Epoch: 389 | Loss: 0.04604680463671684 | Test loss: 0.11927016079425812\n",
            "Epoch: 390 | Loss: 0.045996345579624176 | Test loss: 0.11902527511119843\n",
            "Epoch: 391 | Loss: 0.04594588279724121 | Test loss: 0.11878037452697754\n",
            "Epoch: 392 | Loss: 0.04589542746543884 | Test loss: 0.11853544414043427\n",
            "Epoch: 393 | Loss: 0.04584496468305588 | Test loss: 0.11829052865505219\n",
            "Epoch: 394 | Loss: 0.045794516801834106 | Test loss: 0.1180456131696701\n",
            "Epoch: 395 | Loss: 0.04574405029416084 | Test loss: 0.11780073493719101\n",
            "Epoch: 396 | Loss: 0.045693591237068176 | Test loss: 0.11755583435297012\n",
            "Epoch: 397 | Loss: 0.04564313963055611 | Test loss: 0.11731092631816864\n",
            "Epoch: 398 | Loss: 0.04559267684817314 | Test loss: 0.11706600338220596\n",
            "Epoch: 399 | Loss: 0.045542217791080475 | Test loss: 0.11682109534740448\n",
            "Epoch: 401 | Loss: 0.04544129967689514 | Test loss: 0.11633126437664032\n",
            "Epoch: 402 | Loss: 0.04539085179567337 | Test loss: 0.11608636379241943\n",
            "Epoch: 403 | Loss: 0.04534038156270981 | Test loss: 0.11584146320819855\n",
            "Epoch: 404 | Loss: 0.04528992623090744 | Test loss: 0.11559655517339706\n",
            "Epoch: 405 | Loss: 0.04523947089910507 | Test loss: 0.11535165458917618\n",
            "Epoch: 406 | Loss: 0.045189011842012405 | Test loss: 0.1151067465543747\n",
            "Epoch: 407 | Loss: 0.04513854905962944 | Test loss: 0.1148618683218956\n",
            "Epoch: 408 | Loss: 0.04509454965591431 | Test loss: 0.1146838441491127\n",
            "Epoch: 409 | Loss: 0.045054562389850616 | Test loss: 0.1145058423280716\n",
            "Epoch: 410 | Loss: 0.045014552772045135 | Test loss: 0.11432783305644989\n",
            "Epoch: 411 | Loss: 0.04497454687952995 | Test loss: 0.11414983123540878\n",
            "Epoch: 412 | Loss: 0.04493454843759537 | Test loss: 0.11397182941436768\n",
            "Epoch: 413 | Loss: 0.04489455372095108 | Test loss: 0.11379382759332657\n",
            "Epoch: 414 | Loss: 0.044854551553726196 | Test loss: 0.11361582577228546\n",
            "Epoch: 415 | Loss: 0.044814541935920715 | Test loss: 0.11343781650066376\n",
            "Epoch: 416 | Loss: 0.04477455094456673 | Test loss: 0.11325981467962265\n",
            "Epoch: 417 | Loss: 0.044734545052051544 | Test loss: 0.11308183521032333\n",
            "Epoch: 418 | Loss: 0.04469455033540726 | Test loss: 0.11290381103754044\n",
            "Epoch: 419 | Loss: 0.04465455189347267 | Test loss: 0.11272580921649933\n",
            "Epoch: 420 | Loss: 0.04461454600095749 | Test loss: 0.11254779994487762\n",
            "Epoch: 421 | Loss: 0.0445745475590229 | Test loss: 0.11236979812383652\n",
            "Epoch: 422 | Loss: 0.04453454539179802 | Test loss: 0.1121918112039566\n",
            "Epoch: 423 | Loss: 0.044494546949863434 | Test loss: 0.1120137944817543\n",
            "Epoch: 424 | Loss: 0.04445454478263855 | Test loss: 0.1118357926607132\n",
            "Epoch: 425 | Loss: 0.044414542615413666 | Test loss: 0.11165778338909149\n",
            "Epoch: 426 | Loss: 0.04437453672289848 | Test loss: 0.11147978156805038\n",
            "Epoch: 427 | Loss: 0.0443345345556736 | Test loss: 0.11130179464817047\n",
            "Epoch: 428 | Loss: 0.04429454356431961 | Test loss: 0.11112377792596817\n",
            "Epoch: 429 | Loss: 0.04425454139709473 | Test loss: 0.11094577610492706\n",
            "Epoch: 430 | Loss: 0.04421453922986984 | Test loss: 0.11076776683330536\n",
            "Epoch: 431 | Loss: 0.04417454078793526 | Test loss: 0.11058976501226425\n",
            "Epoch: 432 | Loss: 0.04413453862071037 | Test loss: 0.11041177809238434\n",
            "Epoch: 433 | Loss: 0.04409453272819519 | Test loss: 0.11023376137018204\n",
            "Epoch: 434 | Loss: 0.0440545380115509 | Test loss: 0.11005575954914093\n",
            "Epoch: 435 | Loss: 0.04401453956961632 | Test loss: 0.10987772792577744\n",
            "Epoch: 436 | Loss: 0.04397452995181084 | Test loss: 0.10969976335763931\n",
            "Epoch: 437 | Loss: 0.04393453150987625 | Test loss: 0.1095217615365982\n",
            "Epoch: 438 | Loss: 0.043894533067941666 | Test loss: 0.1093437448143959\n",
            "Epoch: 439 | Loss: 0.04385453090071678 | Test loss: 0.1091657504439354\n",
            "Epoch: 440 | Loss: 0.043814532458782196 | Test loss: 0.1089877337217331\n",
            "Epoch: 441 | Loss: 0.043774526566267014 | Test loss: 0.10880974680185318\n",
            "Epoch: 442 | Loss: 0.043734531849622726 | Test loss: 0.10863174498081207\n",
            "Epoch: 443 | Loss: 0.04369453340768814 | Test loss: 0.10845372825860977\n",
            "Epoch: 444 | Loss: 0.04365452378988266 | Test loss: 0.10827572643756866\n",
            "Epoch: 445 | Loss: 0.04361452907323837 | Test loss: 0.10809771716594696\n",
            "Epoch: 446 | Loss: 0.04357453063130379 | Test loss: 0.10791973024606705\n",
            "Epoch: 447 | Loss: 0.043534524738788605 | Test loss: 0.10774173587560654\n",
            "Epoch: 448 | Loss: 0.04349452629685402 | Test loss: 0.10756371915340424\n",
            "Epoch: 449 | Loss: 0.043454527854919434 | Test loss: 0.10738573223352432\n",
            "Epoch: 450 | Loss: 0.04341452196240425 | Test loss: 0.10720770061016083\n",
            "Epoch: 451 | Loss: 0.043374527245759964 | Test loss: 0.10702969878911972\n",
            "Epoch: 452 | Loss: 0.04333452507853508 | Test loss: 0.1068517193198204\n",
            "Epoch: 453 | Loss: 0.043294526636600494 | Test loss: 0.1066737174987793\n",
            "Epoch: 454 | Loss: 0.04325452446937561 | Test loss: 0.106495700776577\n",
            "Epoch: 455 | Loss: 0.04321451857686043 | Test loss: 0.1063176840543747\n",
            "Epoch: 456 | Loss: 0.04317452758550644 | Test loss: 0.10613970458507538\n",
            "Epoch: 457 | Loss: 0.04313452169299126 | Test loss: 0.10596170276403427\n",
            "Epoch: 458 | Loss: 0.04309451952576637 | Test loss: 0.10578368604183197\n",
            "Epoch: 459 | Loss: 0.04305451363325119 | Test loss: 0.10560569912195206\n",
            "Epoch: 460 | Loss: 0.043014515191316605 | Test loss: 0.10542766749858856\n",
            "Epoch: 461 | Loss: 0.04297452047467232 | Test loss: 0.10524968802928925\n",
            "Epoch: 462 | Loss: 0.042934514582157135 | Test loss: 0.10507168620824814\n",
            "Epoch: 463 | Loss: 0.04289551451802254 | Test loss: 0.10496147722005844\n",
            "Epoch: 464 | Loss: 0.04286060854792595 | Test loss: 0.10485129058361053\n",
            "Epoch: 465 | Loss: 0.04282570630311966 | Test loss: 0.10474108159542084\n",
            "Epoch: 466 | Loss: 0.04279080033302307 | Test loss: 0.10463090240955353\n",
            "Epoch: 467 | Loss: 0.04275590553879738 | Test loss: 0.10452067852020264\n",
            "Epoch: 468 | Loss: 0.042720992118120193 | Test loss: 0.10441050678491592\n",
            "Epoch: 469 | Loss: 0.0426860973238945 | Test loss: 0.10430029779672623\n",
            "Epoch: 470 | Loss: 0.04265119880437851 | Test loss: 0.10419009625911713\n",
            "Epoch: 471 | Loss: 0.04261629283428192 | Test loss: 0.10407988727092743\n",
            "Epoch: 472 | Loss: 0.04258139431476593 | Test loss: 0.10396971553564072\n",
            "Epoch: 473 | Loss: 0.04254649952054024 | Test loss: 0.10385950654745102\n",
            "Epoch: 474 | Loss: 0.04251159727573395 | Test loss: 0.10374931991100311\n",
            "Epoch: 475 | Loss: 0.04247669130563736 | Test loss: 0.10363911092281342\n",
            "Epoch: 476 | Loss: 0.04244178533554077 | Test loss: 0.1035289317369461\n",
            "Epoch: 477 | Loss: 0.04240688681602478 | Test loss: 0.10341870784759521\n",
            "Epoch: 478 | Loss: 0.042371977120637894 | Test loss: 0.1033085361123085\n",
            "Epoch: 479 | Loss: 0.0423370823264122 | Test loss: 0.1031983271241188\n",
            "Epoch: 480 | Loss: 0.04230218380689621 | Test loss: 0.1030881255865097\n",
            "Epoch: 481 | Loss: 0.04226727783679962 | Test loss: 0.10297791659832001\n",
            "Epoch: 482 | Loss: 0.04223238304257393 | Test loss: 0.1028677448630333\n",
            "Epoch: 483 | Loss: 0.04219748452305794 | Test loss: 0.1027575358748436\n",
            "Epoch: 484 | Loss: 0.04216257855296135 | Test loss: 0.10264734923839569\n",
            "Epoch: 485 | Loss: 0.04212767630815506 | Test loss: 0.102537140250206\n",
            "Epoch: 486 | Loss: 0.04209277033805847 | Test loss: 0.10242696106433868\n",
            "Epoch: 487 | Loss: 0.04205787554383278 | Test loss: 0.10231673717498779\n",
            "Epoch: 488 | Loss: 0.042022962123155594 | Test loss: 0.10220656543970108\n",
            "Epoch: 489 | Loss: 0.0419880673289299 | Test loss: 0.10209635645151138\n",
            "Epoch: 490 | Loss: 0.04195316880941391 | Test loss: 0.10198615491390228\n",
            "Epoch: 491 | Loss: 0.04191826283931732 | Test loss: 0.10187594592571259\n",
            "Epoch: 492 | Loss: 0.04188336431980133 | Test loss: 0.10176577419042587\n",
            "Epoch: 493 | Loss: 0.04184846952557564 | Test loss: 0.10165556520223618\n",
            "Epoch: 494 | Loss: 0.04181356728076935 | Test loss: 0.10154537856578827\n",
            "Epoch: 495 | Loss: 0.04177866131067276 | Test loss: 0.10143516957759857\n",
            "Epoch: 496 | Loss: 0.04174375534057617 | Test loss: 0.10132499039173126\n",
            "Epoch: 497 | Loss: 0.04170885682106018 | Test loss: 0.10121476650238037\n",
            "Epoch: 498 | Loss: 0.041673947125673294 | Test loss: 0.10110459476709366\n",
            "Epoch: 499 | Loss: 0.0416390523314476 | Test loss: 0.10099438577890396\n",
            "Epoch: 500 | Loss: 0.04160415381193161 | Test loss: 0.10088418424129486\n",
            "Epoch: 501 | Loss: 0.04156924784183502 | Test loss: 0.10077397525310516\n",
            "Epoch: 502 | Loss: 0.04153435304760933 | Test loss: 0.10066380351781845\n",
            "Epoch: 503 | Loss: 0.04149945452809334 | Test loss: 0.10055359452962875\n",
            "Epoch: 504 | Loss: 0.04146454855799675 | Test loss: 0.10044340789318085\n",
            "Epoch: 505 | Loss: 0.04142964631319046 | Test loss: 0.10033319890499115\n",
            "Epoch: 506 | Loss: 0.04139474034309387 | Test loss: 0.10022301971912384\n",
            "Epoch: 507 | Loss: 0.04135984554886818 | Test loss: 0.10011279582977295\n",
            "Epoch: 508 | Loss: 0.041324932128190994 | Test loss: 0.10000262409448624\n",
            "Epoch: 509 | Loss: 0.0412900373339653 | Test loss: 0.09989241510629654\n",
            "Epoch: 510 | Loss: 0.04125513881444931 | Test loss: 0.09978221356868744\n",
            "Epoch: 511 | Loss: 0.04122023284435272 | Test loss: 0.09967201203107834\n",
            "Epoch: 512 | Loss: 0.04118533432483673 | Test loss: 0.09956184029579163\n",
            "Epoch: 513 | Loss: 0.04115043953061104 | Test loss: 0.09945162385702133\n",
            "Epoch: 514 | Loss: 0.04111553728580475 | Test loss: 0.09934143722057343\n",
            "Epoch: 515 | Loss: 0.04108063131570816 | Test loss: 0.09923122823238373\n",
            "Epoch: 516 | Loss: 0.04104572534561157 | Test loss: 0.09912104904651642\n",
            "Epoch: 517 | Loss: 0.04101082682609558 | Test loss: 0.09901082515716553\n",
            "Epoch: 518 | Loss: 0.040975917130708694 | Test loss: 0.09890064597129822\n",
            "Epoch: 519 | Loss: 0.040941022336483 | Test loss: 0.09879044443368912\n",
            "Epoch: 520 | Loss: 0.04090612381696701 | Test loss: 0.09868024289608002\n",
            "Epoch: 521 | Loss: 0.04087121784687042 | Test loss: 0.09857004135847092\n",
            "Epoch: 522 | Loss: 0.04083632305264473 | Test loss: 0.0984598696231842\n",
            "Epoch: 523 | Loss: 0.04080142453312874 | Test loss: 0.09834965318441391\n",
            "Epoch: 524 | Loss: 0.04076651856303215 | Test loss: 0.098239466547966\n",
            "Epoch: 525 | Loss: 0.04073161631822586 | Test loss: 0.0981292575597763\n",
            "Epoch: 526 | Loss: 0.04069671034812927 | Test loss: 0.098019078373909\n",
            "Epoch: 527 | Loss: 0.04066181555390358 | Test loss: 0.0979088544845581\n",
            "Epoch: 528 | Loss: 0.040626902133226395 | Test loss: 0.0977986752986908\n",
            "Epoch: 529 | Loss: 0.0405920073390007 | Test loss: 0.0976884737610817\n",
            "Epoch: 530 | Loss: 0.04055710881948471 | Test loss: 0.0975782722234726\n",
            "Epoch: 531 | Loss: 0.04052220284938812 | Test loss: 0.0974680706858635\n",
            "Epoch: 532 | Loss: 0.04048730432987213 | Test loss: 0.09735789895057678\n",
            "Epoch: 533 | Loss: 0.04045240953564644 | Test loss: 0.09724768251180649\n",
            "Epoch: 534 | Loss: 0.04041750729084015 | Test loss: 0.09713749587535858\n",
            "Epoch: 535 | Loss: 0.04038260132074356 | Test loss: 0.09702728688716888\n",
            "Epoch: 536 | Loss: 0.04034769535064697 | Test loss: 0.09691710770130157\n",
            "Epoch: 537 | Loss: 0.04031279683113098 | Test loss: 0.09680688381195068\n",
            "Epoch: 538 | Loss: 0.040277887135744095 | Test loss: 0.09669670462608337\n",
            "Epoch: 539 | Loss: 0.0402429923415184 | Test loss: 0.09658650308847427\n",
            "Epoch: 540 | Loss: 0.04020809382200241 | Test loss: 0.09647630155086517\n",
            "Epoch: 541 | Loss: 0.04017318785190582 | Test loss: 0.09636610001325607\n",
            "Epoch: 542 | Loss: 0.04013829305768013 | Test loss: 0.09625592827796936\n",
            "Epoch: 543 | Loss: 0.04010339453816414 | Test loss: 0.09614574164152145\n",
            "Epoch: 544 | Loss: 0.04006849601864815 | Test loss: 0.09603555500507355\n",
            "Epoch: 545 | Loss: 0.04003359004855156 | Test loss: 0.09592534601688385\n",
            "Epoch: 546 | Loss: 0.03999868780374527 | Test loss: 0.09581514447927475\n",
            "Epoch: 547 | Loss: 0.03996378555893898 | Test loss: 0.09570496529340744\n",
            "Epoch: 548 | Loss: 0.03992888331413269 | Test loss: 0.09559475630521774\n",
            "Epoch: 549 | Loss: 0.0398939847946167 | Test loss: 0.09548455476760864\n",
            "Epoch: 550 | Loss: 0.03985908254981041 | Test loss: 0.09537436813116074\n",
            "Epoch: 551 | Loss: 0.03982418030500412 | Test loss: 0.09526417404413223\n",
            "Epoch: 552 | Loss: 0.03978928178548813 | Test loss: 0.09515395760536194\n",
            "Epoch: 553 | Loss: 0.039754386991262436 | Test loss: 0.09504377096891403\n",
            "Epoch: 554 | Loss: 0.03971948102116585 | Test loss: 0.09493358433246613\n",
            "Epoch: 555 | Loss: 0.03968457505106926 | Test loss: 0.09482337534427643\n",
            "Epoch: 556 | Loss: 0.03964967280626297 | Test loss: 0.09471317380666733\n",
            "Epoch: 557 | Loss: 0.03961477428674698 | Test loss: 0.09460299462080002\n",
            "Epoch: 558 | Loss: 0.03957986459136009 | Test loss: 0.09449278563261032\n",
            "Epoch: 559 | Loss: 0.0395449697971344 | Test loss: 0.09438258409500122\n",
            "Epoch: 560 | Loss: 0.03951007127761841 | Test loss: 0.09427239745855331\n",
            "Epoch: 561 | Loss: 0.03947516530752182 | Test loss: 0.09416220337152481\n",
            "Epoch: 562 | Loss: 0.03944026678800583 | Test loss: 0.09405198693275452\n",
            "Epoch: 563 | Loss: 0.03940536826848984 | Test loss: 0.09394180029630661\n",
            "Epoch: 564 | Loss: 0.03937046602368355 | Test loss: 0.0938316136598587\n",
            "Epoch: 565 | Loss: 0.03933556005358696 | Test loss: 0.093721404671669\n",
            "Epoch: 566 | Loss: 0.03930065780878067 | Test loss: 0.0936112031340599\n",
            "Epoch: 567 | Loss: 0.03926575928926468 | Test loss: 0.0935010239481926\n",
            "Epoch: 568 | Loss: 0.03923085331916809 | Test loss: 0.0933908149600029\n",
            "Epoch: 569 | Loss: 0.0391959547996521 | Test loss: 0.0932806134223938\n",
            "Epoch: 570 | Loss: 0.03916105255484581 | Test loss: 0.09317042678594589\n",
            "Epoch: 571 | Loss: 0.03912615031003952 | Test loss: 0.09306023269891739\n",
            "Epoch: 572 | Loss: 0.03909125179052353 | Test loss: 0.0929500162601471\n",
            "Epoch: 573 | Loss: 0.039056356996297836 | Test loss: 0.09283982962369919\n",
            "Epoch: 574 | Loss: 0.03902145102620125 | Test loss: 0.09272964298725128\n",
            "Epoch: 575 | Loss: 0.03898654505610466 | Test loss: 0.09261943399906158\n",
            "Epoch: 576 | Loss: 0.03895164281129837 | Test loss: 0.09250923246145248\n",
            "Epoch: 577 | Loss: 0.03891674429178238 | Test loss: 0.09239905327558517\n",
            "Epoch: 578 | Loss: 0.03888183459639549 | Test loss: 0.09228884428739548\n",
            "Epoch: 579 | Loss: 0.0388469398021698 | Test loss: 0.09217864274978638\n",
            "Epoch: 580 | Loss: 0.03881204128265381 | Test loss: 0.09206845611333847\n",
            "Epoch: 581 | Loss: 0.03877713531255722 | Test loss: 0.09195826202630997\n",
            "Epoch: 582 | Loss: 0.03874223679304123 | Test loss: 0.09184804558753967\n",
            "Epoch: 583 | Loss: 0.03870733827352524 | Test loss: 0.09173785895109177\n",
            "Epoch: 584 | Loss: 0.03867243602871895 | Test loss: 0.09162767231464386\n",
            "Epoch: 585 | Loss: 0.03863753005862236 | Test loss: 0.09151746332645416\n",
            "Epoch: 586 | Loss: 0.03860262781381607 | Test loss: 0.09140726178884506\n",
            "Epoch: 587 | Loss: 0.03856772929430008 | Test loss: 0.09129708260297775\n",
            "Epoch: 588 | Loss: 0.03853282332420349 | Test loss: 0.09118687361478806\n",
            "Epoch: 589 | Loss: 0.0384979248046875 | Test loss: 0.09107667207717896\n",
            "Epoch: 590 | Loss: 0.03846302255988121 | Test loss: 0.09096648544073105\n",
            "Epoch: 591 | Loss: 0.03842812031507492 | Test loss: 0.09085629135370255\n",
            "Epoch: 592 | Loss: 0.03839322179555893 | Test loss: 0.09074607491493225\n",
            "Epoch: 593 | Loss: 0.03835832700133324 | Test loss: 0.09063588827848434\n",
            "Epoch: 594 | Loss: 0.03832342103123665 | Test loss: 0.09052570164203644\n",
            "Epoch: 595 | Loss: 0.03828851506114006 | Test loss: 0.09041549265384674\n",
            "Epoch: 596 | Loss: 0.03825361281633377 | Test loss: 0.09030529111623764\n",
            "Epoch: 597 | Loss: 0.03821871429681778 | Test loss: 0.09019511193037033\n",
            "Epoch: 598 | Loss: 0.03818380460143089 | Test loss: 0.09008490294218063\n",
            "Epoch: 599 | Loss: 0.0381489098072052 | Test loss: 0.08997470140457153\n",
            "Epoch: 600 | Loss: 0.03811401128768921 | Test loss: 0.08986451476812363\n",
            "Epoch: 601 | Loss: 0.03807910531759262 | Test loss: 0.08975431323051453\n",
            "Epoch: 602 | Loss: 0.03804420679807663 | Test loss: 0.08964410424232483\n",
            "Epoch: 603 | Loss: 0.03800930827856064 | Test loss: 0.08953391760587692\n",
            "Epoch: 604 | Loss: 0.03797440603375435 | Test loss: 0.08942373096942902\n",
            "Epoch: 605 | Loss: 0.03793950006365776 | Test loss: 0.08931352198123932\n",
            "Epoch: 606 | Loss: 0.03790459781885147 | Test loss: 0.08920332044363022\n",
            "Epoch: 607 | Loss: 0.03786969929933548 | Test loss: 0.08909314125776291\n",
            "Epoch: 608 | Loss: 0.03783479332923889 | Test loss: 0.08898293226957321\n",
            "Epoch: 609 | Loss: 0.0377998948097229 | Test loss: 0.08887273073196411\n",
            "Epoch: 610 | Loss: 0.03776499256491661 | Test loss: 0.0887625440955162\n",
            "Epoch: 611 | Loss: 0.03773009032011032 | Test loss: 0.0886523425579071\n",
            "Epoch: 612 | Loss: 0.03769519180059433 | Test loss: 0.08854213356971741\n",
            "Epoch: 613 | Loss: 0.03766029700636864 | Test loss: 0.0884319469332695\n",
            "Epoch: 614 | Loss: 0.03762539103627205 | Test loss: 0.0883217602968216\n",
            "Epoch: 615 | Loss: 0.03759048506617546 | Test loss: 0.0882115513086319\n",
            "Epoch: 616 | Loss: 0.03755558282136917 | Test loss: 0.0881013497710228\n",
            "Epoch: 617 | Loss: 0.03752068430185318 | Test loss: 0.08799117058515549\n",
            "Epoch: 618 | Loss: 0.03748577460646629 | Test loss: 0.08788096159696579\n",
            "Epoch: 619 | Loss: 0.0374508798122406 | Test loss: 0.08777076005935669\n",
            "Epoch: 620 | Loss: 0.03741598129272461 | Test loss: 0.08766057342290878\n",
            "Epoch: 621 | Loss: 0.03738107532262802 | Test loss: 0.08755037188529968\n",
            "Epoch: 622 | Loss: 0.03734617680311203 | Test loss: 0.08744016289710999\n",
            "Epoch: 623 | Loss: 0.03731127828359604 | Test loss: 0.08732997626066208\n",
            "Epoch: 624 | Loss: 0.03727637603878975 | Test loss: 0.08721978962421417\n",
            "Epoch: 625 | Loss: 0.03724147006869316 | Test loss: 0.08710958063602448\n",
            "Epoch: 626 | Loss: 0.03720656782388687 | Test loss: 0.08699937909841537\n",
            "Epoch: 627 | Loss: 0.03717166930437088 | Test loss: 0.08688919991254807\n",
            "Epoch: 628 | Loss: 0.03713676333427429 | Test loss: 0.08677899092435837\n",
            "Epoch: 629 | Loss: 0.0371018648147583 | Test loss: 0.08666878938674927\n",
            "Epoch: 630 | Loss: 0.037067197263240814 | Test loss: 0.08662726730108261\n",
            "Epoch: 631 | Loss: 0.037033338099718094 | Test loss: 0.08651705086231232\n",
            "Epoch: 632 | Loss: 0.036998435854911804 | Test loss: 0.08640686422586441\n",
            "Epoch: 633 | Loss: 0.036964669823646545 | Test loss: 0.08636530488729477\n",
            "Epoch: 634 | Loss: 0.036929916590452194 | Test loss: 0.08625511080026627\n",
            "Epoch: 635 | Loss: 0.036895763128995895 | Test loss: 0.08621358126401901\n",
            "Epoch: 636 | Loss: 0.03686138987541199 | Test loss: 0.08610336482524872\n",
            "Epoch: 637 | Loss: 0.036826856434345245 | Test loss: 0.08606183528900146\n",
            "Epoch: 638 | Loss: 0.03679286316037178 | Test loss: 0.08595161885023117\n",
            "Epoch: 639 | Loss: 0.03675796836614609 | Test loss: 0.08584143966436386\n",
            "Epoch: 640 | Loss: 0.03672432154417038 | Test loss: 0.08579986542463303\n",
            "Epoch: 641 | Loss: 0.03668942302465439 | Test loss: 0.08568969368934631\n",
            "Epoch: 642 | Loss: 0.03665542230010033 | Test loss: 0.08564814180135727\n",
            "Epoch: 643 | Loss: 0.03662090748548508 | Test loss: 0.08553795516490936\n",
            "Epoch: 644 | Loss: 0.03658652305603027 | Test loss: 0.08549639582633972\n",
            "Epoch: 645 | Loss: 0.03655238077044487 | Test loss: 0.08538620173931122\n",
            "Epoch: 646 | Loss: 0.03651762008666992 | Test loss: 0.08534465730190277\n",
            "Epoch: 647 | Loss: 0.03648385405540466 | Test loss: 0.08523445576429367\n",
            "Epoch: 648 | Loss: 0.03644895553588867 | Test loss: 0.08512424677610397\n",
            "Epoch: 649 | Loss: 0.03641509264707565 | Test loss: 0.08508270978927612\n",
            "Epoch: 650 | Loss: 0.03638043254613876 | Test loss: 0.08497253060340881\n",
            "Epoch: 651 | Loss: 0.036346182227134705 | Test loss: 0.08493097871541977\n",
            "Epoch: 652 | Loss: 0.036311905831098557 | Test loss: 0.08482077717781067\n",
            "Epoch: 653 | Loss: 0.03627727925777435 | Test loss: 0.08477924019098282\n",
            "Epoch: 654 | Loss: 0.03624338284134865 | Test loss: 0.08466903120279312\n",
            "Epoch: 655 | Loss: 0.03620847314596176 | Test loss: 0.08455883711576462\n",
            "Epoch: 656 | Loss: 0.036174751818180084 | Test loss: 0.08451729267835617\n",
            "Epoch: 657 | Loss: 0.03613995760679245 | Test loss: 0.08440708369016647\n",
            "Epoch: 658 | Loss: 0.036105841398239136 | Test loss: 0.08436556160449982\n",
            "Epoch: 659 | Loss: 0.03607143089175224 | Test loss: 0.08425535261631012\n",
            "Epoch: 660 | Loss: 0.036036938428878784 | Test loss: 0.08421380817890167\n",
            "Epoch: 661 | Loss: 0.03600289672613144 | Test loss: 0.08410360664129257\n",
            "Epoch: 662 | Loss: 0.035968031734228134 | Test loss: 0.08406206965446472\n",
            "Epoch: 663 | Loss: 0.03593437746167183 | Test loss: 0.08395187556743622\n",
            "Epoch: 664 | Loss: 0.03589947521686554 | Test loss: 0.08384167402982712\n",
            "Epoch: 665 | Loss: 0.035865504294633865 | Test loss: 0.08380012214183807\n",
            "Epoch: 666 | Loss: 0.03583095222711563 | Test loss: 0.08368994295597076\n",
            "Epoch: 667 | Loss: 0.03579659387469292 | Test loss: 0.08364837616682053\n",
            "Epoch: 668 | Loss: 0.03576242923736572 | Test loss: 0.08353818953037262\n",
            "Epoch: 669 | Loss: 0.03572769835591316 | Test loss: 0.08349661529064178\n",
            "Epoch: 670 | Loss: 0.035693906247615814 | Test loss: 0.08338643610477448\n",
            "Epoch: 671 | Loss: 0.03565899655222893 | Test loss: 0.08327624946832657\n",
            "Epoch: 672 | Loss: 0.035625167191028595 | Test loss: 0.08323469758033752\n",
            "Epoch: 673 | Loss: 0.03559047356247902 | Test loss: 0.08312450349330902\n",
            "Epoch: 674 | Loss: 0.03555626422166824 | Test loss: 0.08308294415473938\n",
            "Epoch: 675 | Loss: 0.03552195802330971 | Test loss: 0.08297275751829147\n",
            "Epoch: 676 | Loss: 0.03548736125230789 | Test loss: 0.08293122053146362\n",
            "Epoch: 677 | Loss: 0.0354534275829792 | Test loss: 0.08282099664211273\n",
            "Epoch: 678 | Loss: 0.035418521612882614 | Test loss: 0.08271081745624542\n",
            "Epoch: 679 | Loss: 0.035384830087423325 | Test loss: 0.08266927301883698\n",
            "Epoch: 680 | Loss: 0.035350002348423004 | Test loss: 0.08255907148122787\n",
            "Epoch: 681 | Loss: 0.035315923392772675 | Test loss: 0.08251753449440002\n",
            "Epoch: 682 | Loss: 0.0352814719080925 | Test loss: 0.08240732550621033\n",
            "Epoch: 683 | Loss: 0.03524700924754143 | Test loss: 0.08236578106880188\n",
            "Epoch: 684 | Loss: 0.03521294146776199 | Test loss: 0.08225558698177338\n",
            "Epoch: 685 | Loss: 0.03517811372876167 | Test loss: 0.08221404999494553\n",
            "Epoch: 686 | Loss: 0.03514442220330238 | Test loss: 0.08210383355617523\n",
            "Epoch: 687 | Loss: 0.0351095125079155 | Test loss: 0.08199366927146912\n",
            "Epoch: 688 | Loss: 0.035075593739748 | Test loss: 0.08195210248231888\n",
            "Epoch: 689 | Loss: 0.03504099324345589 | Test loss: 0.08184190839529037\n",
            "Epoch: 690 | Loss: 0.035006679594516754 | Test loss: 0.08180035650730133\n",
            "Epoch: 691 | Loss: 0.03497246652841568 | Test loss: 0.08169016242027283\n",
            "Epoch: 692 | Loss: 0.0349377766251564 | Test loss: 0.08164862543344498\n",
            "Epoch: 693 | Loss: 0.03490393981337547 | Test loss: 0.08153841644525528\n",
            "Epoch: 694 | Loss: 0.03486904129385948 | Test loss: 0.08142823725938797\n",
            "Epoch: 695 | Loss: 0.03483524173498154 | Test loss: 0.08138667792081833\n",
            "Epoch: 696 | Loss: 0.03480052202939987 | Test loss: 0.08127648383378983\n",
            "Epoch: 697 | Loss: 0.034766342490911484 | Test loss: 0.08123494684696198\n",
            "Epoch: 698 | Loss: 0.034731991589069366 | Test loss: 0.08112473785877228\n",
            "Epoch: 699 | Loss: 0.034697435796260834 | Test loss: 0.08108319342136383\n",
            "Epoch: 700 | Loss: 0.03466346859931946 | Test loss: 0.08097299933433533\n",
            "Epoch: 701 | Loss: 0.03462856262922287 | Test loss: 0.08086280524730682\n",
            "Epoch: 702 | Loss: 0.03459491580724716 | Test loss: 0.08082125335931778\n",
            "Epoch: 703 | Loss: 0.03456003591418266 | Test loss: 0.08071105927228928\n",
            "Epoch: 704 | Loss: 0.034526001662015915 | Test loss: 0.08066950738430023\n",
            "Epoch: 705 | Loss: 0.034491509199142456 | Test loss: 0.08055931329727173\n",
            "Epoch: 706 | Loss: 0.03445709869265556 | Test loss: 0.08051777631044388\n",
            "Epoch: 707 | Loss: 0.03442298620939255 | Test loss: 0.08040755987167358\n",
            "Epoch: 708 | Loss: 0.03438819572329521 | Test loss: 0.08036604523658752\n",
            "Epoch: 709 | Loss: 0.03435446694493294 | Test loss: 0.08025582134723663\n",
            "Epoch: 710 | Loss: 0.03431956097483635 | Test loss: 0.08014564216136932\n",
            "Epoch: 711 | Loss: 0.034285664558410645 | Test loss: 0.08010408282279968\n",
            "Epoch: 712 | Loss: 0.034251030534505844 | Test loss: 0.07999388873577118\n",
            "Epoch: 713 | Loss: 0.03421676158905029 | Test loss: 0.07995234429836273\n",
            "Epoch: 714 | Loss: 0.034182511270046234 | Test loss: 0.07984215021133423\n",
            "Epoch: 715 | Loss: 0.034147851169109344 | Test loss: 0.07980061322450638\n",
            "Epoch: 716 | Loss: 0.03411398082971573 | Test loss: 0.07969042658805847\n",
            "Epoch: 717 | Loss: 0.03407908231019974 | Test loss: 0.07958020269870758\n",
            "Epoch: 718 | Loss: 0.03404533118009567 | Test loss: 0.07953865826129913\n",
            "Epoch: 719 | Loss: 0.03401055932044983 | Test loss: 0.07942847162485123\n",
            "Epoch: 720 | Loss: 0.03397642448544502 | Test loss: 0.07938691228628159\n",
            "Epoch: 721 | Loss: 0.03394203260540962 | Test loss: 0.07927673310041428\n",
            "Epoch: 722 | Loss: 0.033907510340213776 | Test loss: 0.07923518121242523\n",
            "Epoch: 723 | Loss: 0.03387351706624031 | Test loss: 0.07912497967481613\n",
            "Epoch: 724 | Loss: 0.033838607370853424 | Test loss: 0.07908343523740768\n",
            "Epoch: 725 | Loss: 0.03380498290061951 | Test loss: 0.07897323369979858\n",
            "Epoch: 726 | Loss: 0.033770088106393814 | Test loss: 0.07886303961277008\n",
            "Epoch: 727 | Loss: 0.033736079931259155 | Test loss: 0.07882149517536163\n",
            "Epoch: 728 | Loss: 0.03370155766606331 | Test loss: 0.07871129363775253\n",
            "Epoch: 729 | Loss: 0.033667173236608505 | Test loss: 0.07866974920034409\n",
            "Epoch: 730 | Loss: 0.0336330309510231 | Test loss: 0.07855955511331558\n",
            "Epoch: 731 | Loss: 0.033598270267248154 | Test loss: 0.07851801812648773\n",
            "Epoch: 732 | Loss: 0.033564500510692596 | Test loss: 0.07840781658887863\n",
            "Epoch: 733 | Loss: 0.033529605716466904 | Test loss: 0.07829762250185013\n",
            "Epoch: 734 | Loss: 0.03349574655294418 | Test loss: 0.07825606316328049\n",
            "Epoch: 735 | Loss: 0.0334610790014267 | Test loss: 0.07814587652683258\n",
            "Epoch: 736 | Loss: 0.03342684358358383 | Test loss: 0.07810431718826294\n",
            "Epoch: 737 | Loss: 0.03339255228638649 | Test loss: 0.07799413055181503\n",
            "Epoch: 738 | Loss: 0.03335793316364288 | Test loss: 0.07795257866382599\n",
            "Epoch: 739 | Loss: 0.03332402929663658 | Test loss: 0.07784239202737808\n",
            "Epoch: 740 | Loss: 0.03328912332653999 | Test loss: 0.07773219794034958\n",
            "Epoch: 741 | Loss: 0.033255405724048615 | Test loss: 0.07769063860177994\n",
            "Epoch: 742 | Loss: 0.033220600336790085 | Test loss: 0.07758044451475143\n",
            "Epoch: 743 | Loss: 0.03318650275468826 | Test loss: 0.07753890007734299\n",
            "Epoch: 744 | Loss: 0.03315207362174988 | Test loss: 0.07742870599031448\n",
            "Epoch: 745 | Loss: 0.03311759606003761 | Test loss: 0.07738716900348663\n",
            "Epoch: 746 | Loss: 0.03308354690670967 | Test loss: 0.07727696001529694\n",
            "Epoch: 747 | Loss: 0.033048685640096664 | Test loss: 0.07723541557788849\n",
            "Epoch: 748 | Loss: 0.033015020191669464 | Test loss: 0.07712522149085999\n",
            "Epoch: 749 | Loss: 0.03298012539744377 | Test loss: 0.07701502740383148\n",
            "Epoch: 750 | Loss: 0.032946161925792694 | Test loss: 0.07697348296642303\n",
            "Epoch: 751 | Loss: 0.032911598682403564 | Test loss: 0.07686327397823334\n",
            "Epoch: 752 | Loss: 0.03287725895643234 | Test loss: 0.07682173699140549\n",
            "Epoch: 753 | Loss: 0.03284307196736336 | Test loss: 0.07671154290437698\n",
            "Epoch: 754 | Loss: 0.03280835971236229 | Test loss: 0.07666999846696854\n",
            "Epoch: 755 | Loss: 0.03277454525232315 | Test loss: 0.07655977457761765\n",
            "Epoch: 756 | Loss: 0.032739631831645966 | Test loss: 0.07644960284233093\n",
            "Epoch: 757 | Loss: 0.032705824822187424 | Test loss: 0.07640805840492249\n",
            "Epoch: 758 | Loss: 0.032671116292476654 | Test loss: 0.07629784941673279\n",
            "Epoch: 759 | Loss: 0.03263692185282707 | Test loss: 0.07625630497932434\n",
            "Epoch: 760 | Loss: 0.03260258957743645 | Test loss: 0.07614611089229584\n",
            "Epoch: 761 | Loss: 0.03256801888346672 | Test loss: 0.07610457390546799\n",
            "Epoch: 762 | Loss: 0.03253406286239624 | Test loss: 0.07599435746669769\n",
            "Epoch: 763 | Loss: 0.03249916434288025 | Test loss: 0.075884148478508\n",
            "Epoch: 764 | Loss: 0.03246549516916275 | Test loss: 0.07584261894226074\n",
            "Epoch: 765 | Loss: 0.03243063762784004 | Test loss: 0.07573243975639343\n",
            "Epoch: 766 | Loss: 0.0323965810239315 | Test loss: 0.07569089531898499\n",
            "Epoch: 767 | Loss: 0.032362114638090134 | Test loss: 0.07558067888021469\n",
            "Epoch: 768 | Loss: 0.03232767805457115 | Test loss: 0.07553914934396744\n",
            "Epoch: 769 | Loss: 0.032293591648340225 | Test loss: 0.07542894035577774\n",
            "Epoch: 770 | Loss: 0.0322587676346302 | Test loss: 0.07538740336894989\n",
            "Epoch: 771 | Loss: 0.03222506120800972 | Test loss: 0.07527719438076019\n",
            "Epoch: 772 | Loss: 0.03219016641378403 | Test loss: 0.07516699284315109\n",
            "Epoch: 773 | Loss: 0.03215624764561653 | Test loss: 0.07512547075748444\n",
            "Epoch: 774 | Loss: 0.03212163969874382 | Test loss: 0.07501526176929474\n",
            "Epoch: 775 | Loss: 0.03208733722567558 | Test loss: 0.07497371733188629\n",
            "Epoch: 776 | Loss: 0.032053105533123016 | Test loss: 0.07486351579427719\n",
            "Epoch: 777 | Loss: 0.03201843053102493 | Test loss: 0.07482197880744934\n",
            "Epoch: 778 | Loss: 0.03198458254337311 | Test loss: 0.07471177726984024\n",
            "Epoch: 779 | Loss: 0.031949687749147415 | Test loss: 0.07460158318281174\n",
            "Epoch: 780 | Loss: 0.031915903091430664 | Test loss: 0.07456003129482269\n",
            "Epoch: 781 | Loss: 0.03188116103410721 | Test loss: 0.07444985210895538\n",
            "Epoch: 782 | Loss: 0.031846996396780014 | Test loss: 0.07440828531980515\n",
            "Epoch: 783 | Loss: 0.0318126380443573 | Test loss: 0.07429809868335724\n",
            "Epoch: 784 | Loss: 0.03177809715270996 | Test loss: 0.0742565244436264\n",
            "Epoch: 785 | Loss: 0.03174411505460739 | Test loss: 0.0741463452577591\n",
            "Epoch: 786 | Loss: 0.031709205359220505 | Test loss: 0.07403616607189178\n",
            "Epoch: 787 | Loss: 0.03167556971311569 | Test loss: 0.07399461418390274\n",
            "Epoch: 788 | Loss: 0.031640682369470596 | Test loss: 0.07388440519571304\n",
            "Epoch: 789 | Loss: 0.03160666301846504 | Test loss: 0.073842853307724\n",
            "Epoch: 790 | Loss: 0.031572166830301285 | Test loss: 0.07373266667127609\n",
            "Epoch: 791 | Loss: 0.03153776004910469 | Test loss: 0.07369112968444824\n",
            "Epoch: 792 | Loss: 0.03150363638997078 | Test loss: 0.07358090579509735\n",
            "Epoch: 793 | Loss: 0.03146884962916374 | Test loss: 0.0735393688082695\n",
            "Epoch: 794 | Loss: 0.03143510967493057 | Test loss: 0.0734291821718216\n",
            "Epoch: 795 | Loss: 0.03140020743012428 | Test loss: 0.0733189806342125\n",
            "Epoch: 796 | Loss: 0.03136632218956947 | Test loss: 0.07327743619680405\n",
            "Epoch: 797 | Loss: 0.031331680715084076 | Test loss: 0.07316723465919495\n",
            "Epoch: 798 | Loss: 0.031297408044338226 | Test loss: 0.0731256827712059\n",
            "Epoch: 799 | Loss: 0.03126315027475357 | Test loss: 0.073015496134758\n",
            "Epoch: 801 | Loss: 0.03119463101029396 | Test loss: 0.07286374270915985\n",
            "Epoch: 802 | Loss: 0.031159725040197372 | Test loss: 0.07275357842445374\n",
            "Epoch: 803 | Loss: 0.0311259925365448 | Test loss: 0.0727120116353035\n",
            "Epoch: 804 | Loss: 0.031091203913092613 | Test loss: 0.07260182499885559\n",
            "Epoch: 805 | Loss: 0.031057080253958702 | Test loss: 0.07256026566028595\n",
            "Epoch: 806 | Loss: 0.031022677198052406 | Test loss: 0.07245006412267685\n",
            "Epoch: 807 | Loss: 0.03098817728459835 | Test loss: 0.07240854203701019\n",
            "Epoch: 808 | Loss: 0.03095415234565735 | Test loss: 0.0722983330488205\n",
            "Epoch: 809 | Loss: 0.03091926872730255 | Test loss: 0.07225678116083145\n",
            "Epoch: 810 | Loss: 0.03088562563061714 | Test loss: 0.07214658707380295\n",
            "Epoch: 811 | Loss: 0.03085072711110115 | Test loss: 0.07203639298677444\n",
            "Epoch: 812 | Loss: 0.03081674501299858 | Test loss: 0.0719948559999466\n",
            "Epoch: 813 | Loss: 0.030782198533415794 | Test loss: 0.0718846470117569\n",
            "Epoch: 814 | Loss: 0.03074783645570278 | Test loss: 0.07184310257434845\n",
            "Epoch: 815 | Loss: 0.030713677406311035 | Test loss: 0.07173290848731995\n",
            "Epoch: 816 | Loss: 0.030678927898406982 | Test loss: 0.0716913565993309\n",
            "Epoch: 817 | Loss: 0.030645150691270828 | Test loss: 0.071581169962883\n",
            "Epoch: 818 | Loss: 0.03061024472117424 | Test loss: 0.0714709609746933\n",
            "Epoch: 819 | Loss: 0.030576402321457863 | Test loss: 0.07142942398786545\n",
            "Epoch: 820 | Loss: 0.03054172359406948 | Test loss: 0.07131922990083694\n",
            "Epoch: 821 | Loss: 0.03050749935209751 | Test loss: 0.0712776854634285\n",
            "Epoch: 822 | Loss: 0.030473193153738976 | Test loss: 0.0711674615740776\n",
            "Epoch: 823 | Loss: 0.03043859638273716 | Test loss: 0.07112595438957214\n",
            "Epoch: 824 | Loss: 0.030404681339859962 | Test loss: 0.07101573050022125\n",
            "Epoch: 825 | Loss: 0.030369769781827927 | Test loss: 0.07090555131435394\n",
            "Epoch: 826 | Loss: 0.030336061492562294 | Test loss: 0.0708639919757843\n",
            "Epoch: 827 | Loss: 0.03030124306678772 | Test loss: 0.0707537978887558\n",
            "Epoch: 828 | Loss: 0.030267158523201942 | Test loss: 0.07071225345134735\n",
            "Epoch: 829 | Loss: 0.03023272193968296 | Test loss: 0.07060205936431885\n",
            "Epoch: 830 | Loss: 0.030198251828551292 | Test loss: 0.0705605149269104\n",
            "Epoch: 831 | Loss: 0.030164187774062157 | Test loss: 0.07045033574104309\n",
            "Epoch: 832 | Loss: 0.030129343271255493 | Test loss: 0.07040876895189285\n",
            "Epoch: 833 | Loss: 0.030095672234892845 | Test loss: 0.07029856741428375\n",
            "Epoch: 834 | Loss: 0.030060768127441406 | Test loss: 0.07018838077783585\n",
            "Epoch: 835 | Loss: 0.03002682328224182 | Test loss: 0.0701468288898468\n",
            "Epoch: 836 | Loss: 0.0299922414124012 | Test loss: 0.07003664970397949\n",
            "Epoch: 837 | Loss: 0.02995791658759117 | Test loss: 0.06999509036540985\n",
            "Epoch: 838 | Loss: 0.02992371842265129 | Test loss: 0.06988489627838135\n",
            "Epoch: 839 | Loss: 0.029889006167650223 | Test loss: 0.0698433369398117\n",
            "Epoch: 840 | Loss: 0.029855191707611084 | Test loss: 0.0697331428527832\n",
            "Epoch: 841 | Loss: 0.029820293188095093 | Test loss: 0.0696229487657547\n",
            "Epoch: 842 | Loss: 0.029786478728055954 | Test loss: 0.06958140432834625\n",
            "Epoch: 843 | Loss: 0.02975175902247429 | Test loss: 0.06947120279073715\n",
            "Epoch: 844 | Loss: 0.029717573896050453 | Test loss: 0.0694296583533287\n",
            "Epoch: 845 | Loss: 0.02968323789536953 | Test loss: 0.0693194642663002\n",
            "Epoch: 846 | Loss: 0.0296486709266901 | Test loss: 0.06927792727947235\n",
            "Epoch: 847 | Loss: 0.029614707455039024 | Test loss: 0.06916771829128265\n",
            "Epoch: 848 | Loss: 0.02957981266081333 | Test loss: 0.06905753165483475\n",
            "Epoch: 849 | Loss: 0.02954614721238613 | Test loss: 0.0690159797668457\n",
            "Epoch: 850 | Loss: 0.029511287808418274 | Test loss: 0.0689057856798172\n",
            "Epoch: 851 | Loss: 0.029477238655090332 | Test loss: 0.06886423379182816\n",
            "Epoch: 852 | Loss: 0.029442761093378067 | Test loss: 0.06875403970479965\n",
            "Epoch: 853 | Loss: 0.029408331960439682 | Test loss: 0.06871248781681061\n",
            "Epoch: 854 | Loss: 0.02937423624098301 | Test loss: 0.0686022937297821\n",
            "Epoch: 855 | Loss: 0.02933942899107933 | Test loss: 0.06856075674295425\n",
            "Epoch: 856 | Loss: 0.02930571138858795 | Test loss: 0.06845054775476456\n",
            "Epoch: 857 | Loss: 0.029270809143781662 | Test loss: 0.06834035366773605\n",
            "Epoch: 858 | Loss: 0.02923690341413021 | Test loss: 0.0682988092303276\n",
            "Epoch: 859 | Loss: 0.029202282428741455 | Test loss: 0.0681886151432991\n",
            "Epoch: 860 | Loss: 0.02916799485683441 | Test loss: 0.06814707070589066\n",
            "Epoch: 861 | Loss: 0.029133755713701248 | Test loss: 0.06803686916828156\n",
            "Epoch: 862 | Loss: 0.029099086299538612 | Test loss: 0.06799532473087311\n",
            "Epoch: 863 | Loss: 0.029065227136015892 | Test loss: 0.0678851306438446\n",
            "Epoch: 864 | Loss: 0.02903033420443535 | Test loss: 0.0677749365568161\n",
            "Epoch: 865 | Loss: 0.028996562585234642 | Test loss: 0.06773339211940765\n",
            "Epoch: 866 | Loss: 0.02896180748939514 | Test loss: 0.06762318313121796\n",
            "Epoch: 867 | Loss: 0.02892765775322914 | Test loss: 0.0675816461443901\n",
            "Epoch: 868 | Loss: 0.028893280774354935 | Test loss: 0.0674714595079422\n",
            "Epoch: 869 | Loss: 0.02885875664651394 | Test loss: 0.06742990016937256\n",
            "Epoch: 870 | Loss: 0.028824755921959877 | Test loss: 0.06731969118118286\n",
            "Epoch: 871 | Loss: 0.028789842501282692 | Test loss: 0.06720949709415436\n",
            "Epoch: 872 | Loss: 0.02875622548162937 | Test loss: 0.0671679675579071\n",
            "Epoch: 873 | Loss: 0.02872132696211338 | Test loss: 0.06705774366855621\n",
            "Epoch: 874 | Loss: 0.02868732251226902 | Test loss: 0.06701621413230896\n",
            "Epoch: 875 | Loss: 0.028652798384428024 | Test loss: 0.06690602004528046\n",
            "Epoch: 876 | Loss: 0.02861841954290867 | Test loss: 0.0668644830584526\n",
            "Epoch: 877 | Loss: 0.028584271669387817 | Test loss: 0.06675426661968231\n",
            "Epoch: 878 | Loss: 0.02854951098561287 | Test loss: 0.06671272963285446\n",
            "Epoch: 879 | Loss: 0.028515750542283058 | Test loss: 0.06660252809524536\n",
            "Epoch: 880 | Loss: 0.02848084643483162 | Test loss: 0.06649234890937805\n",
            "Epoch: 881 | Loss: 0.02844698168337345 | Test loss: 0.06645079702138901\n",
            "Epoch: 882 | Loss: 0.02841232344508171 | Test loss: 0.06634058058261871\n",
            "Epoch: 883 | Loss: 0.0283780749887228 | Test loss: 0.06629905849695206\n",
            "Epoch: 884 | Loss: 0.028343800455331802 | Test loss: 0.06618884950876236\n",
            "Epoch: 885 | Loss: 0.028309166431427002 | Test loss: 0.06614731252193451\n",
            "Epoch: 886 | Loss: 0.028275271877646446 | Test loss: 0.06603710353374481\n",
            "Epoch: 887 | Loss: 0.028240377083420753 | Test loss: 0.06592690199613571\n",
            "Epoch: 888 | Loss: 0.02820664644241333 | Test loss: 0.06588537245988846\n",
            "Epoch: 889 | Loss: 0.028171848505735397 | Test loss: 0.06577517092227936\n",
            "Epoch: 890 | Loss: 0.02813773788511753 | Test loss: 0.06573362648487091\n",
            "Epoch: 891 | Loss: 0.028103316202759743 | Test loss: 0.06562343239784241\n",
            "Epoch: 892 | Loss: 0.02806883119046688 | Test loss: 0.06558187305927277\n",
            "Epoch: 893 | Loss: 0.028034795075654984 | Test loss: 0.06547168642282486\n",
            "Epoch: 894 | Loss: 0.02799992822110653 | Test loss: 0.06543011963367462\n",
            "Epoch: 895 | Loss: 0.027966270223259926 | Test loss: 0.06531994044780731\n",
            "Epoch: 896 | Loss: 0.027931367978453636 | Test loss: 0.0652097538113594\n",
            "Epoch: 897 | Loss: 0.027897397056221962 | Test loss: 0.06516819447278976\n",
            "Epoch: 898 | Loss: 0.027862846851348877 | Test loss: 0.06505800783634186\n",
            "Epoch: 899 | Loss: 0.02782849594950676 | Test loss: 0.06501643359661102\n",
            "Epoch: 900 | Loss: 0.027794325724244118 | Test loss: 0.06490625441074371\n",
            "Epoch: 901 | Loss: 0.02775958739221096 | Test loss: 0.06486472487449646\n",
            "Epoch: 902 | Loss: 0.027725791558623314 | Test loss: 0.06475453078746796\n",
            "Epoch: 903 | Loss: 0.027690891176462173 | Test loss: 0.06464431434869766\n",
            "Epoch: 904 | Loss: 0.02765706181526184 | Test loss: 0.06460276246070862\n",
            "Epoch: 905 | Loss: 0.027622371912002563 | Test loss: 0.06449256837368011\n",
            "Epoch: 906 | Loss: 0.02758815884590149 | Test loss: 0.06445103138685226\n",
            "Epoch: 907 | Loss: 0.027553845196962357 | Test loss: 0.06434081494808197\n",
            "Epoch: 908 | Loss: 0.02751925028860569 | Test loss: 0.06429927796125412\n",
            "Epoch: 909 | Loss: 0.027485316619277 | Test loss: 0.06418909132480621\n",
            "Epoch: 910 | Loss: 0.02745041623711586 | Test loss: 0.06407888978719711\n",
            "Epoch: 911 | Loss: 0.02741672471165657 | Test loss: 0.06403734534978867\n",
            "Epoch: 912 | Loss: 0.027381891384720802 | Test loss: 0.06392714381217957\n",
            "Epoch: 913 | Loss: 0.027347808703780174 | Test loss: 0.06388559192419052\n",
            "Epoch: 914 | Loss: 0.027313362807035446 | Test loss: 0.06377541273832321\n",
            "Epoch: 915 | Loss: 0.02727891504764557 | Test loss: 0.06373386085033417\n",
            "Epoch: 916 | Loss: 0.02724483609199524 | Test loss: 0.06362365186214447\n",
            "Epoch: 917 | Loss: 0.02721000649034977 | Test loss: 0.06358212977647781\n",
            "Epoch: 918 | Loss: 0.027176309376955032 | Test loss: 0.06347192078828812\n",
            "Epoch: 919 | Loss: 0.02714141272008419 | Test loss: 0.06336172670125961\n",
            "Epoch: 920 | Loss: 0.0271074827760458 | Test loss: 0.06332017481327057\n",
            "Epoch: 921 | Loss: 0.027072886005043983 | Test loss: 0.06320997327566147\n",
            "Epoch: 922 | Loss: 0.0270385779440403 | Test loss: 0.06316845118999481\n",
            "Epoch: 923 | Loss: 0.027004361152648926 | Test loss: 0.06305824220180511\n",
            "Epoch: 924 | Loss: 0.0269696656614542 | Test loss: 0.06301669031381607\n",
            "Epoch: 925 | Loss: 0.02693583443760872 | Test loss: 0.06290650367736816\n",
            "Epoch: 926 | Loss: 0.026900935918092728 | Test loss: 0.06279630213975906\n",
            "Epoch: 927 | Loss: 0.02686714567244053 | Test loss: 0.06275476515293121\n",
            "Epoch: 928 | Loss: 0.02683240734040737 | Test loss: 0.06264455616474152\n",
            "Epoch: 929 | Loss: 0.02679823711514473 | Test loss: 0.06260301172733307\n",
            "Epoch: 930 | Loss: 0.026763886213302612 | Test loss: 0.062492817640304565\n",
            "Epoch: 931 | Loss: 0.02672932669520378 | Test loss: 0.06245126575231552\n",
            "Epoch: 932 | Loss: 0.026695359498262405 | Test loss: 0.06234106421470642\n",
            "Epoch: 933 | Loss: 0.026660453528165817 | Test loss: 0.062230873852968216\n",
            "Epoch: 934 | Loss: 0.02662680111825466 | Test loss: 0.06218933314085007\n",
            "Epoch: 935 | Loss: 0.026591932401061058 | Test loss: 0.062079139053821564\n",
            "Epoch: 936 | Loss: 0.02655789814889431 | Test loss: 0.062037594616413116\n",
            "Epoch: 937 | Loss: 0.026523401960730553 | Test loss: 0.06192737817764282\n",
            "Epoch: 938 | Loss: 0.026488998904824257 | Test loss: 0.061885856091976166\n",
            "Epoch: 939 | Loss: 0.02645489014685154 | Test loss: 0.06177564337849617\n",
            "Epoch: 940 | Loss: 0.02642008289694786 | Test loss: 0.06173409894108772\n",
            "Epoch: 941 | Loss: 0.026386354118585587 | Test loss: 0.06162390112876892\n",
            "Epoch: 942 | Loss: 0.026351448148489 | Test loss: 0.06151369959115982\n",
            "Epoch: 943 | Loss: 0.02631755731999874 | Test loss: 0.06147215515375137\n",
            "Epoch: 944 | Loss: 0.026282930746674538 | Test loss: 0.061361975967884064\n",
            "Epoch: 945 | Loss: 0.02624865248799324 | Test loss: 0.06132042407989502\n",
            "Epoch: 946 | Loss: 0.026214396581053734 | Test loss: 0.06121024489402771\n",
            "Epoch: 947 | Loss: 0.026179740205407143 | Test loss: 0.061168670654296875\n",
            "Epoch: 948 | Loss: 0.026145881041884422 | Test loss: 0.06105848029255867\n",
            "Epoch: 949 | Loss: 0.026110976934432983 | Test loss: 0.060948289930820465\n",
            "Epoch: 950 | Loss: 0.02607722207903862 | Test loss: 0.06090673804283142\n",
            "Epoch: 951 | Loss: 0.026042452082037926 | Test loss: 0.060796551406383514\n",
            "Epoch: 952 | Loss: 0.02600831352174282 | Test loss: 0.06075499206781387\n",
            "Epoch: 953 | Loss: 0.025973927229642868 | Test loss: 0.06064480543136597\n",
            "Epoch: 954 | Loss: 0.02593940868973732 | Test loss: 0.060603249818086624\n",
            "Epoch: 955 | Loss: 0.02590540051460266 | Test loss: 0.06049305200576782\n",
            "Epoch: 956 | Loss: 0.025870507583022118 | Test loss: 0.06045151501893997\n",
            "Epoch: 957 | Loss: 0.025836873799562454 | Test loss: 0.06034131720662117\n",
            "Epoch: 958 | Loss: 0.025801967829465866 | Test loss: 0.06023111194372177\n",
            "Epoch: 959 | Loss: 0.025767972692847252 | Test loss: 0.060189567506313324\n",
            "Epoch: 960 | Loss: 0.025733450427651405 | Test loss: 0.06007937341928482\n",
            "Epoch: 961 | Loss: 0.0256990734487772 | Test loss: 0.06003783270716667\n",
            "Epoch: 962 | Loss: 0.0256649199873209 | Test loss: 0.05992763116955757\n",
            "Epoch: 963 | Loss: 0.02563016675412655 | Test loss: 0.05988607555627823\n",
            "Epoch: 964 | Loss: 0.025596395134925842 | Test loss: 0.05977589637041092\n",
            "Epoch: 965 | Loss: 0.02556149661540985 | Test loss: 0.05966568738222122\n",
            "Epoch: 966 | Loss: 0.02552764117717743 | Test loss: 0.05962413549423218\n",
            "Epoch: 967 | Loss: 0.025492969900369644 | Test loss: 0.05951394885778427\n",
            "Epoch: 968 | Loss: 0.02545873448252678 | Test loss: 0.059472404420375824\n",
            "Epoch: 969 | Loss: 0.025424445047974586 | Test loss: 0.059362202882766724\n",
            "Epoch: 970 | Loss: 0.02538982965052128 | Test loss: 0.05932066589593887\n",
            "Epoch: 971 | Loss: 0.02535592019557953 | Test loss: 0.059210460633039474\n",
            "Epoch: 972 | Loss: 0.02532101795077324 | Test loss: 0.05910026282072067\n",
            "Epoch: 973 | Loss: 0.02528730034828186 | Test loss: 0.059058718383312225\n",
            "Epoch: 974 | Loss: 0.025252491235733032 | Test loss: 0.058948516845703125\n",
            "Epoch: 975 | Loss: 0.02521839737892151 | Test loss: 0.05890697240829468\n",
            "Epoch: 976 | Loss: 0.025183964520692825 | Test loss: 0.058796774595975876\n",
            "Epoch: 977 | Loss: 0.02514948509633541 | Test loss: 0.05875522643327713\n",
            "Epoch: 978 | Loss: 0.02511543594300747 | Test loss: 0.058645039796829224\n",
            "Epoch: 979 | Loss: 0.025080587714910507 | Test loss: 0.05860348790884018\n",
            "Epoch: 980 | Loss: 0.02504691109061241 | Test loss: 0.058493297547101974\n",
            "Epoch: 981 | Loss: 0.02501201629638672 | Test loss: 0.058383096009492874\n",
            "Epoch: 982 | Loss: 0.02497805655002594 | Test loss: 0.058341555297374725\n",
            "Epoch: 983 | Loss: 0.024943487718701363 | Test loss: 0.05823137238621712\n",
            "Epoch: 984 | Loss: 0.024909157305955887 | Test loss: 0.05818980187177658\n",
            "Epoch: 985 | Loss: 0.024874964728951454 | Test loss: 0.05807960033416748\n",
            "Epoch: 986 | Loss: 0.024840235710144043 | Test loss: 0.05803806334733963\n",
            "Epoch: 987 | Loss: 0.024806439876556396 | Test loss: 0.057927876710891724\n",
            "Epoch: 988 | Loss: 0.024771535769104958 | Test loss: 0.05781765654683113\n",
            "Epoch: 989 | Loss: 0.02473771944642067 | Test loss: 0.05777612328529358\n",
            "Epoch: 990 | Loss: 0.0247030109167099 | Test loss: 0.057665932923555374\n",
            "Epoch: 991 | Loss: 0.024668816477060318 | Test loss: 0.057624392211437225\n",
            "Epoch: 992 | Loss: 0.024634480476379395 | Test loss: 0.05751417949795723\n",
            "Epoch: 993 | Loss: 0.024599911645054817 | Test loss: 0.05747263878583908\n",
            "Epoch: 994 | Loss: 0.024565961211919785 | Test loss: 0.05736243724822998\n",
            "Epoch: 995 | Loss: 0.024531055241823196 | Test loss: 0.05725225806236267\n",
            "Epoch: 996 | Loss: 0.02449738048017025 | Test loss: 0.05721070617437363\n",
            "Epoch: 997 | Loss: 0.024462532252073288 | Test loss: 0.057100486010313034\n",
            "Epoch: 998 | Loss: 0.02442847564816475 | Test loss: 0.057058971375226974\n",
            "Epoch: 999 | Loss: 0.02439401112496853 | Test loss: 0.05694875866174698\n",
            "Epoch: 1000 | Loss: 0.02435956709086895 | Test loss: 0.05690721794962883\n",
            "Epoch: 1001 | Loss: 0.024325478821992874 | Test loss: 0.05679702013731003\n",
            "Epoch: 1002 | Loss: 0.024290667846798897 | Test loss: 0.05675547197461128\n",
            "Epoch: 1003 | Loss: 0.024256955832242966 | Test loss: 0.05664528161287308\n",
            "Epoch: 1004 | Loss: 0.024222057312726974 | Test loss: 0.05653507634997368\n",
            "Epoch: 1005 | Loss: 0.02418813668191433 | Test loss: 0.05649354308843613\n",
            "Epoch: 1006 | Loss: 0.02415352500975132 | Test loss: 0.056383341550827026\n",
            "Epoch: 1007 | Loss: 0.02411923184990883 | Test loss: 0.056341785937547684\n",
            "Epoch: 1008 | Loss: 0.02408500388264656 | Test loss: 0.05623159557580948\n",
            "Epoch: 1009 | Loss: 0.024050328880548477 | Test loss: 0.05619003251194954\n",
            "Epoch: 1010 | Loss: 0.024016480892896652 | Test loss: 0.05607985332608223\n",
            "Epoch: 1011 | Loss: 0.023981576785445213 | Test loss: 0.055969662964344025\n",
            "Epoch: 1012 | Loss: 0.02394779585301876 | Test loss: 0.055928103625774384\n",
            "Epoch: 1013 | Loss: 0.023913055658340454 | Test loss: 0.05581791326403618\n",
            "Epoch: 1014 | Loss: 0.023878900334239006 | Test loss: 0.055776339024305344\n",
            "Epoch: 1015 | Loss: 0.023844534531235695 | Test loss: 0.05566616728901863\n",
            "Epoch: 1016 | Loss: 0.02380998805165291 | Test loss: 0.05562462657690048\n",
            "Epoch: 1017 | Loss: 0.02377600036561489 | Test loss: 0.05551443621516228\n",
            "Epoch: 1018 | Loss: 0.0237411018460989 | Test loss: 0.05540422722697258\n",
            "Epoch: 1019 | Loss: 0.02370746247470379 | Test loss: 0.05536267161369324\n",
            "Epoch: 1020 | Loss: 0.02367258258163929 | Test loss: 0.055252473801374435\n",
            "Epoch: 1021 | Loss: 0.023638557642698288 | Test loss: 0.055210936814546585\n",
            "Epoch: 1022 | Loss: 0.023604054003953934 | Test loss: 0.055100731551647186\n",
            "Epoch: 1023 | Loss: 0.023569650948047638 | Test loss: 0.05505918711423874\n",
            "Epoch: 1024 | Loss: 0.02353552170097828 | Test loss: 0.05494900420308113\n",
            "Epoch: 1025 | Loss: 0.023500746116042137 | Test loss: 0.054907459765672684\n",
            "Epoch: 1026 | Loss: 0.02346699871122837 | Test loss: 0.054797254502773285\n",
            "Epoch: 1027 | Loss: 0.02343210019171238 | Test loss: 0.054687052965164185\n",
            "Epoch: 1028 | Loss: 0.023398209363222122 | Test loss: 0.05464550852775574\n",
            "Epoch: 1029 | Loss: 0.023363569751381874 | Test loss: 0.05453531816601753\n",
            "Epoch: 1030 | Loss: 0.023329313844442368 | Test loss: 0.054493773728609085\n",
            "Epoch: 1031 | Loss: 0.023295046761631966 | Test loss: 0.05438356474041939\n",
            "Epoch: 1032 | Loss: 0.02326040528714657 | Test loss: 0.054342031478881836\n",
            "Epoch: 1033 | Loss: 0.02322651818394661 | Test loss: 0.054231829941272736\n",
            "Epoch: 1034 | Loss: 0.023191621527075768 | Test loss: 0.054121632128953934\n",
            "Epoch: 1035 | Loss: 0.023157883435487747 | Test loss: 0.05408008024096489\n",
            "Epoch: 1036 | Loss: 0.02312309481203556 | Test loss: 0.05396988242864609\n",
            "Epoch: 1037 | Loss: 0.023088976740837097 | Test loss: 0.05392835661768913\n",
            "Epoch: 1038 | Loss: 0.023054569959640503 | Test loss: 0.05381814390420914\n",
            "Epoch: 1039 | Loss: 0.02302006632089615 | Test loss: 0.05377660319209099\n",
            "Epoch: 1040 | Loss: 0.022986043244600296 | Test loss: 0.05366641283035278\n",
            "Epoch: 1041 | Loss: 0.022951165214180946 | Test loss: 0.053624849766492844\n",
            "Epoch: 1042 | Loss: 0.022917520254850388 | Test loss: 0.05351467803120613\n",
            "Epoch: 1043 | Loss: 0.02288261614739895 | Test loss: 0.05340446159243584\n",
            "Epoch: 1044 | Loss: 0.02284863591194153 | Test loss: 0.05336291715502739\n",
            "Epoch: 1045 | Loss: 0.02281409129500389 | Test loss: 0.053252726793289185\n",
            "Epoch: 1046 | Loss: 0.022779732942581177 | Test loss: 0.05321118235588074\n",
            "Epoch: 1047 | Loss: 0.022745568305253983 | Test loss: 0.05310098081827164\n",
            "Epoch: 1048 | Loss: 0.022710826247930527 | Test loss: 0.05305942893028259\n",
            "Epoch: 1049 | Loss: 0.02267703041434288 | Test loss: 0.052949242293834686\n",
            "Epoch: 1050 | Loss: 0.022642139345407486 | Test loss: 0.05283904820680618\n",
            "Epoch: 1051 | Loss: 0.022608298808336258 | Test loss: 0.05279749631881714\n",
            "Epoch: 1052 | Loss: 0.02257360890507698 | Test loss: 0.05268728733062744\n",
            "Epoch: 1053 | Loss: 0.022539397701621056 | Test loss: 0.052645765244960785\n",
            "Epoch: 1054 | Loss: 0.022505097091197968 | Test loss: 0.05253554508090019\n",
            "Epoch: 1055 | Loss: 0.02247048355638981 | Test loss: 0.05249400809407234\n",
            "Epoch: 1056 | Loss: 0.022436564788222313 | Test loss: 0.05238381028175354\n",
            "Epoch: 1057 | Loss: 0.022401660680770874 | Test loss: 0.05227361246943474\n",
            "Epoch: 1058 | Loss: 0.02236795797944069 | Test loss: 0.05223206430673599\n",
            "Epoch: 1059 | Loss: 0.022333137691020966 | Test loss: 0.05212188512086868\n",
            "Epoch: 1060 | Loss: 0.02229905314743519 | Test loss: 0.05208033323287964\n",
            "Epoch: 1061 | Loss: 0.02226460725069046 | Test loss: 0.05197015404701233\n",
            "Epoch: 1062 | Loss: 0.02223013900220394 | Test loss: 0.05192858725786209\n",
            "Epoch: 1063 | Loss: 0.02219608798623085 | Test loss: 0.05181838944554329\n",
            "Epoch: 1064 | Loss: 0.022161245346069336 | Test loss: 0.051776837557554245\n",
            "Epoch: 1065 | Loss: 0.022127559408545494 | Test loss: 0.05166664719581604\n",
            "Epoch: 1066 | Loss: 0.022092660889029503 | Test loss: 0.05155646800994873\n",
            "Epoch: 1067 | Loss: 0.02205871418118477 | Test loss: 0.05151490122079849\n",
            "Epoch: 1068 | Loss: 0.022024136036634445 | Test loss: 0.051404714584350586\n",
            "Epoch: 1069 | Loss: 0.021989809349179268 | Test loss: 0.051363151520490646\n",
            "Epoch: 1070 | Loss: 0.02195560745894909 | Test loss: 0.05125296115875244\n",
            "Epoch: 1071 | Loss: 0.021920906379818916 | Test loss: 0.05121142417192459\n",
            "Epoch: 1072 | Loss: 0.02188708260655403 | Test loss: 0.05110122635960579\n",
            "Epoch: 1073 | Loss: 0.02185218036174774 | Test loss: 0.05099101737141609\n",
            "Epoch: 1074 | Loss: 0.0218183733522892 | Test loss: 0.05094947665929794\n",
            "Epoch: 1075 | Loss: 0.021783655509352684 | Test loss: 0.05083928257226944\n",
            "Epoch: 1076 | Loss: 0.02174947038292885 | Test loss: 0.05079774186015129\n",
            "Epoch: 1077 | Loss: 0.021715128794312477 | Test loss: 0.05068754032254219\n",
            "Epoch: 1078 | Loss: 0.021680567413568497 | Test loss: 0.050645988434553146\n",
            "Epoch: 1079 | Loss: 0.02164660580456257 | Test loss: 0.05053580552339554\n",
            "Epoch: 1080 | Loss: 0.021611705422401428 | Test loss: 0.05042559653520584\n",
            "Epoch: 1081 | Loss: 0.021578039973974228 | Test loss: 0.050384052097797394\n",
            "Epoch: 1082 | Loss: 0.02154317870736122 | Test loss: 0.05027385801076889\n",
            "Epoch: 1083 | Loss: 0.021509135141968727 | Test loss: 0.05023231357336044\n",
            "Epoch: 1084 | Loss: 0.021474653854966164 | Test loss: 0.05012211948633194\n",
            "Epoch: 1085 | Loss: 0.021440228447318077 | Test loss: 0.05008057504892349\n",
            "Epoch: 1086 | Loss: 0.021406125277280807 | Test loss: 0.049970369786024094\n",
            "Epoch: 1087 | Loss: 0.021371323615312576 | Test loss: 0.04992884397506714\n",
            "Epoch: 1088 | Loss: 0.02133760042488575 | Test loss: 0.049818623811006546\n",
            "Epoch: 1089 | Loss: 0.02130269818007946 | Test loss: 0.04970841854810715\n",
            "Epoch: 1090 | Loss: 0.021268798038363457 | Test loss: 0.0496668815612793\n",
            "Epoch: 1091 | Loss: 0.021234173327684402 | Test loss: 0.049556683748960495\n",
            "Epoch: 1092 | Loss: 0.021199887618422508 | Test loss: 0.04951513558626175\n",
            "Epoch: 1093 | Loss: 0.021165648475289345 | Test loss: 0.04940494894981384\n",
            "Epoch: 1094 | Loss: 0.021130988374352455 | Test loss: 0.0493633933365345\n",
            "Epoch: 1095 | Loss: 0.02109711989760399 | Test loss: 0.049253206700086594\n",
            "Epoch: 1096 | Loss: 0.021062226966023445 | Test loss: 0.04914300516247749\n",
            "Epoch: 1097 | Loss: 0.02102845534682274 | Test loss: 0.049101464450359344\n",
            "Epoch: 1098 | Loss: 0.02099369652569294 | Test loss: 0.048991281539201736\n",
            "Epoch: 1099 | Loss: 0.020959556102752686 | Test loss: 0.0489497110247612\n",
            "Epoch: 1100 | Loss: 0.02092517353594303 | Test loss: 0.0488395169377327\n",
            "Epoch: 1101 | Loss: 0.02089063450694084 | Test loss: 0.04879797250032425\n",
            "Epoch: 1102 | Loss: 0.020856648683547974 | Test loss: 0.048687778413295746\n",
            "Epoch: 1103 | Loss: 0.020821744576096535 | Test loss: 0.048577576875686646\n",
            "Epoch: 1104 | Loss: 0.020788121968507767 | Test loss: 0.0485360324382782\n",
            "Epoch: 1105 | Loss: 0.020753219723701477 | Test loss: 0.04842584207653999\n",
            "Epoch: 1106 | Loss: 0.020719215273857117 | Test loss: 0.04838430881500244\n",
            "Epoch: 1107 | Loss: 0.02068468928337097 | Test loss: 0.04827408120036125\n",
            "Epoch: 1108 | Loss: 0.020650310441851616 | Test loss: 0.0482325553894043\n",
            "Epoch: 1109 | Loss: 0.02061617001891136 | Test loss: 0.0481223464012146\n",
            "Epoch: 1110 | Loss: 0.020581401884555817 | Test loss: 0.04808080196380615\n",
            "Epoch: 1111 | Loss: 0.020547647029161453 | Test loss: 0.047970615327358246\n",
            "Epoch: 1112 | Loss: 0.020512741059064865 | Test loss: 0.04786039516329765\n",
            "Epoch: 1113 | Loss: 0.020478874444961548 | Test loss: 0.047818876802921295\n",
            "Epoch: 1114 | Loss: 0.020444219931960106 | Test loss: 0.0477086678147316\n",
            "Epoch: 1115 | Loss: 0.020409967750310898 | Test loss: 0.04766712710261345\n",
            "Epoch: 1116 | Loss: 0.02037569135427475 | Test loss: 0.04755692556500435\n",
            "Epoch: 1117 | Loss: 0.020341066643595695 | Test loss: 0.0475153811275959\n",
            "Epoch: 1118 | Loss: 0.020307164639234543 | Test loss: 0.0474051907658577\n",
            "Epoch: 1119 | Loss: 0.02027226611971855 | Test loss: 0.047294992953538895\n",
            "Epoch: 1120 | Loss: 0.020238537341356277 | Test loss: 0.047253452241420746\n",
            "Epoch: 1121 | Loss: 0.020203733816742897 | Test loss: 0.047143250703811646\n",
            "Epoch: 1122 | Loss: 0.020169630646705627 | Test loss: 0.0471016988158226\n",
            "Epoch: 1123 | Loss: 0.020135212689638138 | Test loss: 0.0469915047287941\n",
            "Epoch: 1124 | Loss: 0.020100727677345276 | Test loss: 0.046949952840805054\n",
            "Epoch: 1125 | Loss: 0.02006668969988823 | Test loss: 0.04683975502848625\n",
            "Epoch: 1126 | Loss: 0.020031820982694626 | Test loss: 0.0467982180416584\n",
            "Epoch: 1127 | Loss: 0.01999816671013832 | Test loss: 0.046688012778759\n",
            "Epoch: 1128 | Loss: 0.01996326446533203 | Test loss: 0.0465778224170208\n",
            "Epoch: 1129 | Loss: 0.019929300993680954 | Test loss: 0.04653624817728996\n",
            "Epoch: 1130 | Loss: 0.019894743338227272 | Test loss: 0.04642607644200325\n",
            "Epoch: 1131 | Loss: 0.019860386848449707 | Test loss: 0.0463845357298851\n",
            "Epoch: 1132 | Loss: 0.019826212897896767 | Test loss: 0.0462743416428566\n",
            "Epoch: 1133 | Loss: 0.019791483879089355 | Test loss: 0.04623277112841606\n",
            "Epoch: 1134 | Loss: 0.01975768432021141 | Test loss: 0.046122580766677856\n",
            "Epoch: 1135 | Loss: 0.019722791388630867 | Test loss: 0.046012382954359055\n",
            "Epoch: 1136 | Loss: 0.019688956439495087 | Test loss: 0.045970845967531204\n",
            "Epoch: 1137 | Loss: 0.01965426094830036 | Test loss: 0.0458606481552124\n",
            "Epoch: 1138 | Loss: 0.019620049744844437 | Test loss: 0.045819103717803955\n",
            "Epoch: 1139 | Loss: 0.019585730507969856 | Test loss: 0.04570891708135605\n",
            "Epoch: 1140 | Loss: 0.019551146775484085 | Test loss: 0.0456673689186573\n",
            "Epoch: 1141 | Loss: 0.019517207518219948 | Test loss: 0.045557163655757904\n",
            "Epoch: 1142 | Loss: 0.019482305273413658 | Test loss: 0.045446962118148804\n",
            "Epoch: 1143 | Loss: 0.01944860816001892 | Test loss: 0.045405417680740356\n",
            "Epoch: 1144 | Loss: 0.01941377855837345 | Test loss: 0.04529522731900215\n",
            "Epoch: 1145 | Loss: 0.019379712641239166 | Test loss: 0.045253682881593704\n",
            "Epoch: 1146 | Loss: 0.019345253705978394 | Test loss: 0.04514347389340401\n",
            "Epoch: 1147 | Loss: 0.019310805946588516 | Test loss: 0.045101940631866455\n",
            "Epoch: 1148 | Loss: 0.019276728853583336 | Test loss: 0.044991739094257355\n",
            "Epoch: 1149 | Loss: 0.019241899251937866 | Test loss: 0.044950198382139206\n",
            "Epoch: 1150 | Loss: 0.019208211451768875 | Test loss: 0.04483998939394951\n",
            "Epoch: 1151 | Loss: 0.019173303619027138 | Test loss: 0.044729799032211304\n",
            "Epoch: 1152 | Loss: 0.019139375537633896 | Test loss: 0.04468826204538345\n",
            "Epoch: 1153 | Loss: 0.01910477876663208 | Test loss: 0.044578056782484055\n",
            "Epoch: 1154 | Loss: 0.019070465117692947 | Test loss: 0.04453650861978531\n",
            "Epoch: 1155 | Loss: 0.019036252051591873 | Test loss: 0.0444263219833374\n",
            "Epoch: 1156 | Loss: 0.019001564010977745 | Test loss: 0.04438475891947746\n",
            "Epoch: 1157 | Loss: 0.018967729061841965 | Test loss: 0.04427458718419075\n",
            "Epoch: 1158 | Loss: 0.018932823091745377 | Test loss: 0.044164370745420456\n",
            "Epoch: 1159 | Loss: 0.018899032846093178 | Test loss: 0.04412282630801201\n",
            "Epoch: 1160 | Loss: 0.018864300101995468 | Test loss: 0.044012635946273804\n",
            "Epoch: 1161 | Loss: 0.018830131739377975 | Test loss: 0.043971091508865356\n",
            "Epoch: 1162 | Loss: 0.01879577711224556 | Test loss: 0.043860889971256256\n",
            "Epoch: 1163 | Loss: 0.018761225044727325 | Test loss: 0.04381933808326721\n",
            "Epoch: 1164 | Loss: 0.018727242946624756 | Test loss: 0.043709151446819305\n",
            "Epoch: 1165 | Loss: 0.018692348152399063 | Test loss: 0.0435989573597908\n",
            "Epoch: 1166 | Loss: 0.018658697605133057 | Test loss: 0.04355739802122116\n",
            "Epoch: 1167 | Loss: 0.018623817712068558 | Test loss: 0.043447189033031464\n",
            "Epoch: 1168 | Loss: 0.018589798361063004 | Test loss: 0.043405670672655106\n",
            "Epoch: 1169 | Loss: 0.018555304035544395 | Test loss: 0.04329545423388481\n",
            "Epoch: 1170 | Loss: 0.018520886078476906 | Test loss: 0.04325391724705696\n",
            "Epoch: 1171 | Loss: 0.01848677359521389 | Test loss: 0.04314371943473816\n",
            "Epoch: 1172 | Loss: 0.018451979383826256 | Test loss: 0.043102167546749115\n",
            "Epoch: 1173 | Loss: 0.018418248742818832 | Test loss: 0.04299197718501091\n",
            "Epoch: 1174 | Loss: 0.018383344635367393 | Test loss: 0.0428817942738533\n",
            "Epoch: 1175 | Loss: 0.018349451944231987 | Test loss: 0.04284024238586426\n",
            "Epoch: 1176 | Loss: 0.018314816057682037 | Test loss: 0.04273006319999695\n",
            "Epoch: 1177 | Loss: 0.01828053966164589 | Test loss: 0.04268849641084671\n",
            "Epoch: 1178 | Loss: 0.018246296793222427 | Test loss: 0.04257829859852791\n",
            "Epoch: 1179 | Loss: 0.018211644142866135 | Test loss: 0.042536746710538864\n",
            "Epoch: 1180 | Loss: 0.01817776821553707 | Test loss: 0.04242655634880066\n",
            "Epoch: 1181 | Loss: 0.01814286969602108 | Test loss: 0.04231637716293335\n",
            "Epoch: 1182 | Loss: 0.018109114840626717 | Test loss: 0.04227481037378311\n",
            "Epoch: 1183 | Loss: 0.018074344843626022 | Test loss: 0.04216461628675461\n",
            "Epoch: 1184 | Loss: 0.018040208145976067 | Test loss: 0.042123060673475266\n",
            "Epoch: 1185 | Loss: 0.018005816265940666 | Test loss: 0.04201287031173706\n",
            "Epoch: 1186 | Loss: 0.017971305176615715 | Test loss: 0.04197133332490921\n",
            "Epoch: 1187 | Loss: 0.01793729141354561 | Test loss: 0.04186113923788071\n",
            "Epoch: 1188 | Loss: 0.017902398481965065 | Test loss: 0.04181956499814987\n",
            "Epoch: 1189 | Loss: 0.017868772149086 | Test loss: 0.04170939326286316\n",
            "Epoch: 1190 | Loss: 0.01783386431634426 | Test loss: 0.04159919545054436\n",
            "Epoch: 1191 | Loss: 0.017799871042370796 | Test loss: 0.04155765101313591\n",
            "Epoch: 1192 | Loss: 0.017765337601304054 | Test loss: 0.04144744947552681\n",
            "Epoch: 1193 | Loss: 0.017730968073010445 | Test loss: 0.041405897587537766\n",
            "Epoch: 1194 | Loss: 0.017696814611554146 | Test loss: 0.04129571467638016\n",
            "Epoch: 1195 | Loss: 0.017662061378359795 | Test loss: 0.04125417023897171\n",
            "Epoch: 1196 | Loss: 0.017628289759159088 | Test loss: 0.04114396125078201\n",
            "Epoch: 1197 | Loss: 0.0175933875143528 | Test loss: 0.04103376343846321\n",
            "Epoch: 1198 | Loss: 0.017559533938765526 | Test loss: 0.04099222272634506\n",
            "Epoch: 1199 | Loss: 0.01752486266195774 | Test loss: 0.04088203236460686\n",
            "Epoch: 1201 | Loss: 0.017456334084272385 | Test loss: 0.040730275213718414\n",
            "Epoch: 1202 | Loss: 0.017421722412109375 | Test loss: 0.04068875312805176\n",
            "Epoch: 1203 | Loss: 0.017387809231877327 | Test loss: 0.040578536689281464\n",
            "Epoch: 1204 | Loss: 0.017352908849716187 | Test loss: 0.04046833515167236\n",
            "Epoch: 1205 | Loss: 0.017319198697805405 | Test loss: 0.040426790714263916\n",
            "Epoch: 1206 | Loss: 0.01728438213467598 | Test loss: 0.040316592901945114\n",
            "Epoch: 1207 | Loss: 0.017250288277864456 | Test loss: 0.04027504473924637\n",
            "Epoch: 1208 | Loss: 0.017215853556990623 | Test loss: 0.04016486555337906\n",
            "Epoch: 1209 | Loss: 0.017181387171149254 | Test loss: 0.04012330621480942\n",
            "Epoch: 1210 | Loss: 0.017147328704595566 | Test loss: 0.040013112127780914\n",
            "Epoch: 1211 | Loss: 0.017112482339143753 | Test loss: 0.039971571415662766\n",
            "Epoch: 1212 | Loss: 0.017078805714845657 | Test loss: 0.039861373603343964\n",
            "Epoch: 1213 | Loss: 0.017043905332684517 | Test loss: 0.039751190692186356\n",
            "Epoch: 1214 | Loss: 0.017009954899549484 | Test loss: 0.03970962017774582\n",
            "Epoch: 1215 | Loss: 0.01697538234293461 | Test loss: 0.039599429816007614\n",
            "Epoch: 1216 | Loss: 0.01694103702902794 | Test loss: 0.03955788165330887\n",
            "Epoch: 1217 | Loss: 0.01690686121582985 | Test loss: 0.03944769501686096\n",
            "Epoch: 1218 | Loss: 0.016872137784957886 | Test loss: 0.03940613940358162\n",
            "Epoch: 1219 | Loss: 0.016838330775499344 | Test loss: 0.03929593041539192\n",
            "Epoch: 1220 | Loss: 0.016803432255983353 | Test loss: 0.03918575122952461\n",
            "Epoch: 1221 | Loss: 0.016769614070653915 | Test loss: 0.03914421796798706\n",
            "Epoch: 1222 | Loss: 0.01673489809036255 | Test loss: 0.039034001529216766\n",
            "Epoch: 1223 | Loss: 0.016700709238648415 | Test loss: 0.038992464542388916\n",
            "Epoch: 1224 | Loss: 0.01666637882590294 | Test loss: 0.038882263004779816\n",
            "Epoch: 1225 | Loss: 0.016631800681352615 | Test loss: 0.03884071111679077\n",
            "Epoch: 1226 | Loss: 0.01659785583615303 | Test loss: 0.03873053193092346\n",
            "Epoch: 1227 | Loss: 0.016562949866056442 | Test loss: 0.03862030431628227\n",
            "Epoch: 1228 | Loss: 0.016529273241758347 | Test loss: 0.03857877850532532\n",
            "Epoch: 1229 | Loss: 0.016494428738951683 | Test loss: 0.03846857696771622\n",
            "Epoch: 1230 | Loss: 0.016460370272397995 | Test loss: 0.03842703625559807\n",
            "Epoch: 1231 | Loss: 0.016425900161266327 | Test loss: 0.03831683471798897\n",
            "Epoch: 1232 | Loss: 0.016391465440392494 | Test loss: 0.03827529028058052\n",
            "Epoch: 1233 | Loss: 0.01635737530887127 | Test loss: 0.038165103644132614\n",
            "Epoch: 1234 | Loss: 0.016322556883096695 | Test loss: 0.038123566657304764\n",
            "Epoch: 1235 | Loss: 0.01628885231912136 | Test loss: 0.038013357669115067\n",
            "Epoch: 1236 | Loss: 0.016253944486379623 | Test loss: 0.037903159856796265\n",
            "Epoch: 1237 | Loss: 0.016220029443502426 | Test loss: 0.03786160424351692\n",
            "Epoch: 1238 | Loss: 0.016185421496629715 | Test loss: 0.03775141388177872\n",
            "Epoch: 1239 | Loss: 0.016151126474142075 | Test loss: 0.03770986199378967\n",
            "Epoch: 1240 | Loss: 0.016116898506879807 | Test loss: 0.03759966418147087\n",
            "Epoch: 1241 | Loss: 0.016082221642136574 | Test loss: 0.03755812719464302\n",
            "Epoch: 1242 | Loss: 0.016048375517129898 | Test loss: 0.03744792938232422\n",
            "Epoch: 1243 | Loss: 0.01601347327232361 | Test loss: 0.03733773157000542\n",
            "Epoch: 1244 | Loss: 0.015979699790477753 | Test loss: 0.03729615360498428\n",
            "Epoch: 1245 | Loss: 0.01594495214521885 | Test loss: 0.03718598559498787\n",
            "Epoch: 1246 | Loss: 0.015910785645246506 | Test loss: 0.03714444115757942\n",
            "Epoch: 1247 | Loss: 0.015876421704888344 | Test loss: 0.03703425079584122\n",
            "Epoch: 1248 | Loss: 0.015841882675886154 | Test loss: 0.03699268028140068\n",
            "Epoch: 1249 | Loss: 0.015807893127202988 | Test loss: 0.036882489919662476\n",
            "Epoch: 1250 | Loss: 0.015772998332977295 | Test loss: 0.036772288382053375\n",
            "Epoch: 1251 | Loss: 0.015739355236291885 | Test loss: 0.03673075884580612\n",
            "Epoch: 1252 | Loss: 0.01570446975529194 | Test loss: 0.03662055730819702\n",
            "Epoch: 1253 | Loss: 0.015670450404286385 | Test loss: 0.03657899424433708\n",
            "Epoch: 1254 | Loss: 0.015635941177606583 | Test loss: 0.03646882623434067\n",
            "Epoch: 1255 | Loss: 0.015601545572280884 | Test loss: 0.03642727807164192\n",
            "Epoch: 1256 | Loss: 0.015567416325211525 | Test loss: 0.03631708025932312\n",
            "Epoch: 1257 | Loss: 0.015532639808952808 | Test loss: 0.03627553582191467\n",
            "Epoch: 1258 | Loss: 0.015498891472816467 | Test loss: 0.036165326833724976\n",
            "Epoch: 1259 | Loss: 0.015463987365365028 | Test loss: 0.036055129021406174\n",
            "Epoch: 1260 | Loss: 0.01543011236935854 | Test loss: 0.03601359575986862\n",
            "Epoch: 1261 | Loss: 0.01539546251296997 | Test loss: 0.035903383046388626\n",
            "Epoch: 1262 | Loss: 0.015361204743385315 | Test loss: 0.03586184233427048\n",
            "Epoch: 1263 | Loss: 0.015326937660574913 | Test loss: 0.035751648247241974\n",
            "Epoch: 1264 | Loss: 0.015292299911379814 | Test loss: 0.035710107535123825\n",
            "Epoch: 1265 | Loss: 0.015258421190083027 | Test loss: 0.03559989854693413\n",
            "Epoch: 1266 | Loss: 0.015223512426018715 | Test loss: 0.035489700734615326\n",
            "Epoch: 1267 | Loss: 0.015189772471785545 | Test loss: 0.03544817119836807\n",
            "Epoch: 1268 | Loss: 0.015154987573623657 | Test loss: 0.035337965935468674\n",
            "Epoch: 1269 | Loss: 0.01512086670845747 | Test loss: 0.03529641777276993\n",
            "Epoch: 1270 | Loss: 0.015086461789906025 | Test loss: 0.03518623113632202\n",
            "Epoch: 1271 | Loss: 0.015051963739097118 | Test loss: 0.035144656896591187\n",
            "Epoch: 1272 | Loss: 0.015017936937510967 | Test loss: 0.03503449633717537\n",
            "Epoch: 1273 | Loss: 0.014983056113123894 | Test loss: 0.03499293327331543\n",
            "Epoch: 1274 | Loss: 0.014949413016438484 | Test loss: 0.03488273546099663\n",
            "Epoch: 1275 | Loss: 0.01491450984030962 | Test loss: 0.03477254509925842\n",
            "Epoch: 1276 | Loss: 0.014880528673529625 | Test loss: 0.034731000661849976\n",
            "Epoch: 1277 | Loss: 0.014845984987914562 | Test loss: 0.034620802849531174\n",
            "Epoch: 1278 | Loss: 0.014811624772846699 | Test loss: 0.03457924723625183\n",
            "Epoch: 1279 | Loss: 0.014777451753616333 | Test loss: 0.034469056874513626\n",
            "Epoch: 1280 | Loss: 0.014742719009518623 | Test loss: 0.03442750498652458\n",
            "Epoch: 1281 | Loss: 0.014708934351801872 | Test loss: 0.03431730717420578\n",
            "Epoch: 1282 | Loss: 0.01467402745038271 | Test loss: 0.03420709818601608\n",
            "Epoch: 1283 | Loss: 0.014640195295214653 | Test loss: 0.03416557237505913\n",
            "Epoch: 1284 | Loss: 0.014605514705181122 | Test loss: 0.03405536338686943\n",
            "Epoch: 1285 | Loss: 0.014571284875273705 | Test loss: 0.03401382640004158\n",
            "Epoch: 1286 | Loss: 0.014536982402205467 | Test loss: 0.03390362858772278\n",
            "Epoch: 1287 | Loss: 0.014502379111945629 | Test loss: 0.033862076699733734\n",
            "Epoch: 1288 | Loss: 0.01446845568716526 | Test loss: 0.033751893788576126\n",
            "Epoch: 1289 | Loss: 0.01443355344235897 | Test loss: 0.03364170342683792\n",
            "Epoch: 1290 | Loss: 0.014399850741028786 | Test loss: 0.03360014408826828\n",
            "Epoch: 1291 | Loss: 0.014365026727318764 | Test loss: 0.03348997235298157\n",
            "Epoch: 1292 | Loss: 0.014330940321087837 | Test loss: 0.03344840556383133\n",
            "Epoch: 1293 | Loss: 0.01429650466889143 | Test loss: 0.03333820775151253\n",
            "Epoch: 1294 | Loss: 0.014262044802308083 | Test loss: 0.033296652138233185\n",
            "Epoch: 1295 | Loss: 0.014227977022528648 | Test loss: 0.03318646550178528\n",
            "Epoch: 1296 | Loss: 0.014193138107657433 | Test loss: 0.03314492106437683\n",
            "Epoch: 1297 | Loss: 0.014159453101456165 | Test loss: 0.03303471952676773\n",
            "Epoch: 1298 | Loss: 0.014124554581940174 | Test loss: 0.03292452543973923\n",
            "Epoch: 1299 | Loss: 0.014090606942772865 | Test loss: 0.032882969826459885\n",
            "Epoch: 1300 | Loss: 0.014056024141609669 | Test loss: 0.03277278691530228\n",
            "Epoch: 1301 | Loss: 0.014021704904735088 | Test loss: 0.03273124247789383\n",
            "Epoch: 1302 | Loss: 0.013987499289214611 | Test loss: 0.03262104466557503\n",
            "Epoch: 1303 | Loss: 0.013952797278761864 | Test loss: 0.03257947415113449\n",
            "Epoch: 1304 | Loss: 0.013918980956077576 | Test loss: 0.03246930241584778\n",
            "Epoch: 1305 | Loss: 0.013884072192013264 | Test loss: 0.032359104603528976\n",
            "Epoch: 1306 | Loss: 0.01385026890784502 | Test loss: 0.03231755644083023\n",
            "Epoch: 1307 | Loss: 0.013815549202263355 | Test loss: 0.03220735862851143\n",
            "Epoch: 1308 | Loss: 0.013781366869807243 | Test loss: 0.032165806740522385\n",
            "Epoch: 1309 | Loss: 0.013747021555900574 | Test loss: 0.03205562382936478\n",
            "Epoch: 1310 | Loss: 0.013712462969124317 | Test loss: 0.03201407939195633\n",
            "Epoch: 1311 | Loss: 0.013678496703505516 | Test loss: 0.03190387412905693\n",
            "Epoch: 1312 | Loss: 0.013643595390021801 | Test loss: 0.03179367259144783\n",
            "Epoch: 1313 | Loss: 0.013609932735562325 | Test loss: 0.03175212815403938\n",
            "Epoch: 1314 | Loss: 0.013575072400271893 | Test loss: 0.031641941517591476\n",
            "Epoch: 1315 | Loss: 0.013541030697524548 | Test loss: 0.03160039335489273\n",
            "Epoch: 1316 | Loss: 0.01350654661655426 | Test loss: 0.03149018436670303\n",
            "Epoch: 1317 | Loss: 0.013472122140228748 | Test loss: 0.03144865483045578\n",
            "Epoch: 1318 | Loss: 0.013438018038868904 | Test loss: 0.03133844584226608\n",
            "Epoch: 1319 | Loss: 0.013403216376900673 | Test loss: 0.031296901404857635\n",
            "Epoch: 1320 | Loss: 0.013369491323828697 | Test loss: 0.031186699867248535\n",
            "Epoch: 1321 | Loss: 0.013334590010344982 | Test loss: 0.031076502054929733\n",
            "Epoch: 1322 | Loss: 0.013300687074661255 | Test loss: 0.03103495202958584\n",
            "Epoch: 1323 | Loss: 0.0132660623639822 | Test loss: 0.030924778431653976\n",
            "Epoch: 1324 | Loss: 0.013231784105300903 | Test loss: 0.030883217230439186\n",
            "Epoch: 1325 | Loss: 0.013197538442909718 | Test loss: 0.030773013830184937\n",
            "Epoch: 1326 | Loss: 0.013162882998585701 | Test loss: 0.030731480568647385\n",
            "Epoch: 1327 | Loss: 0.01312901359051466 | Test loss: 0.030621284618973732\n",
            "Epoch: 1328 | Loss: 0.013094114139676094 | Test loss: 0.030511099845170975\n",
            "Epoch: 1329 | Loss: 0.013060351833701134 | Test loss: 0.03046952560544014\n",
            "Epoch: 1330 | Loss: 0.01302559394389391 | Test loss: 0.030359338968992233\n",
            "Epoch: 1331 | Loss: 0.012991437688469887 | Test loss: 0.03031778894364834\n",
            "Epoch: 1332 | Loss: 0.012957069091498852 | Test loss: 0.03020760416984558\n",
            "Epoch: 1333 | Loss: 0.012922537513077259 | Test loss: 0.03016604855656624\n",
            "Epoch: 1334 | Loss: 0.012888537719845772 | Test loss: 0.03005584515631199\n",
            "Epoch: 1335 | Loss: 0.01285363920032978 | Test loss: 0.02994566038250923\n",
            "Epoch: 1336 | Loss: 0.012820012867450714 | Test loss: 0.029904121533036232\n",
            "Epoch: 1337 | Loss: 0.012785108759999275 | Test loss: 0.029793912544846535\n",
            "Epoch: 1338 | Loss: 0.012751109898090363 | Test loss: 0.029752373695373535\n",
            "Epoch: 1339 | Loss: 0.012716586701571941 | Test loss: 0.029642170295119286\n",
            "Epoch: 1340 | Loss: 0.012682202272117138 | Test loss: 0.029600614681839943\n",
            "Epoch: 1341 | Loss: 0.012648063711822033 | Test loss: 0.02949044108390808\n",
            "Epoch: 1342 | Loss: 0.012613298371434212 | Test loss: 0.02944887802004814\n",
            "Epoch: 1343 | Loss: 0.012579533271491528 | Test loss: 0.029338687658309937\n",
            "Epoch: 1344 | Loss: 0.01254463754594326 | Test loss: 0.029228484258055687\n",
            "Epoch: 1345 | Loss: 0.012510770931839943 | Test loss: 0.029186945408582687\n",
            "Epoch: 1346 | Loss: 0.01247610803693533 | Test loss: 0.029076749458909035\n",
            "Epoch: 1347 | Loss: 0.012441864237189293 | Test loss: 0.02903519943356514\n",
            "Epoch: 1348 | Loss: 0.012407584115862846 | Test loss: 0.028925007209181786\n",
            "Epoch: 1349 | Loss: 0.012372957542538643 | Test loss: 0.028883475810289383\n",
            "Epoch: 1350 | Loss: 0.012339059263467789 | Test loss: 0.028773266822099686\n",
            "Epoch: 1351 | Loss: 0.012304151430726051 | Test loss: 0.028663069009780884\n",
            "Epoch: 1352 | Loss: 0.012270430102944374 | Test loss: 0.028621505945920944\n",
            "Epoch: 1353 | Loss: 0.012235632166266441 | Test loss: 0.028511321172118187\n",
            "Epoch: 1354 | Loss: 0.012201527133584023 | Test loss: 0.028469771146774292\n",
            "Epoch: 1355 | Loss: 0.012167108245193958 | Test loss: 0.02835957333445549\n",
            "Epoch: 1356 | Loss: 0.012132621370255947 | Test loss: 0.02831803634762764\n",
            "Epoch: 1357 | Loss: 0.012098582461476326 | Test loss: 0.028207844123244286\n",
            "Epoch: 1358 | Loss: 0.012063717469573021 | Test loss: 0.028166288509964943\n",
            "Epoch: 1359 | Loss: 0.012030057609081268 | Test loss: 0.028056055307388306\n",
            "Epoch: 1360 | Loss: 0.011995160952210426 | Test loss: 0.02794589474797249\n",
            "Epoch: 1361 | Loss: 0.011961189098656178 | Test loss: 0.02790435031056404\n",
            "Epoch: 1362 | Loss: 0.011926629580557346 | Test loss: 0.027794158086180687\n",
            "Epoch: 1363 | Loss: 0.011892282404005527 | Test loss: 0.027752583846449852\n",
            "Epoch: 1364 | Loss: 0.011858103796839714 | Test loss: 0.027642399072647095\n",
            "Epoch: 1365 | Loss: 0.01182338036596775 | Test loss: 0.027600860223174095\n",
            "Epoch: 1366 | Loss: 0.011789577081799507 | Test loss: 0.02749066986143589\n",
            "Epoch: 1367 | Loss: 0.011754677630960941 | Test loss: 0.02738046646118164\n",
            "Epoch: 1368 | Loss: 0.011720849201083183 | Test loss: 0.0273389033973217\n",
            "Epoch: 1369 | Loss: 0.011686149053275585 | Test loss: 0.027228742837905884\n",
            "Epoch: 1370 | Loss: 0.011651946231722832 | Test loss: 0.02718718722462654\n",
            "Epoch: 1371 | Loss: 0.011617625132203102 | Test loss: 0.02707698382437229\n",
            "Epoch: 1372 | Loss: 0.011583036743104458 | Test loss: 0.027035444974899292\n",
            "Epoch: 1373 | Loss: 0.011549100279808044 | Test loss: 0.026925235986709595\n",
            "Epoch: 1374 | Loss: 0.011514198035001755 | Test loss: 0.026815038174390793\n",
            "Epoch: 1375 | Loss: 0.011480512097477913 | Test loss: 0.026773501187562943\n",
            "Epoch: 1376 | Loss: 0.011445674113929272 | Test loss: 0.026663292199373245\n",
            "Epoch: 1377 | Loss: 0.011411604471504688 | Test loss: 0.026621753349900246\n",
            "Epoch: 1378 | Loss: 0.01137714646756649 | Test loss: 0.026511555537581444\n",
            "Epoch: 1379 | Loss: 0.011342699639499187 | Test loss: 0.026470016688108444\n",
            "Epoch: 1380 | Loss: 0.011308628134429455 | Test loss: 0.026359815150499344\n",
            "Epoch: 1381 | Loss: 0.011273792013525963 | Test loss: 0.026318270713090897\n",
            "Epoch: 1382 | Loss: 0.01124009769409895 | Test loss: 0.026208072900772095\n",
            "Epoch: 1383 | Loss: 0.01120519544929266 | Test loss: 0.026097875088453293\n",
            "Epoch: 1384 | Loss: 0.011171268299221992 | Test loss: 0.026056325063109398\n",
            "Epoch: 1385 | Loss: 0.011136669665575027 | Test loss: 0.02594614028930664\n",
            "Epoch: 1386 | Loss: 0.011102361604571342 | Test loss: 0.025904566049575806\n",
            "Epoch: 1387 | Loss: 0.011068146675825119 | Test loss: 0.02579440549015999\n",
            "Epoch: 1388 | Loss: 0.011033458635210991 | Test loss: 0.02575284242630005\n",
            "Epoch: 1389 | Loss: 0.010999620892107487 | Test loss: 0.025642644613981247\n",
            "Epoch: 1390 | Loss: 0.010964717715978622 | Test loss: 0.025532448664307594\n",
            "Epoch: 1391 | Loss: 0.010930929332971573 | Test loss: 0.025490909814834595\n",
            "Epoch: 1392 | Loss: 0.01089619193226099 | Test loss: 0.025380712002515793\n",
            "Epoch: 1393 | Loss: 0.010862023569643497 | Test loss: 0.02533915638923645\n",
            "Epoch: 1394 | Loss: 0.01082766242325306 | Test loss: 0.025228966027498245\n",
            "Epoch: 1395 | Loss: 0.010793117806315422 | Test loss: 0.0251874141395092\n",
            "Epoch: 1396 | Loss: 0.01075914315879345 | Test loss: 0.02507721818983555\n",
            "Epoch: 1397 | Loss: 0.010724237188696861 | Test loss: 0.02496700920164585\n",
            "Epoch: 1398 | Loss: 0.010690595954656601 | Test loss: 0.024925481528043747\n",
            "Epoch: 1399 | Loss: 0.010655724443495274 | Test loss: 0.02481527253985405\n",
            "Epoch: 1400 | Loss: 0.010621682740747929 | Test loss: 0.0247737355530262\n",
            "Epoch: 1401 | Loss: 0.010587191209197044 | Test loss: 0.024663537740707397\n",
            "Epoch: 1402 | Loss: 0.010552778840065002 | Test loss: 0.024621987715363503\n",
            "Epoch: 1403 | Loss: 0.010518664494156837 | Test loss: 0.024511802941560745\n",
            "Epoch: 1404 | Loss: 0.010483875870704651 | Test loss: 0.0244702510535717\n",
            "Epoch: 1405 | Loss: 0.010450141504406929 | Test loss: 0.02436005510389805\n",
            "Epoch: 1406 | Loss: 0.010415233671665192 | Test loss: 0.024249881505966187\n",
            "Epoch: 1407 | Loss: 0.01038134004920721 | Test loss: 0.0242083128541708\n",
            "Epoch: 1408 | Loss: 0.010346716269850731 | Test loss: 0.024098116904497147\n",
            "Epoch: 1409 | Loss: 0.010312443599104881 | Test loss: 0.024056559428572655\n",
            "Epoch: 1410 | Loss: 0.010278185829520226 | Test loss: 0.023946374654769897\n",
            "Epoch: 1411 | Loss: 0.01024353876709938 | Test loss: 0.023904824629426003\n",
            "Epoch: 1412 | Loss: 0.010209662839770317 | Test loss: 0.0237946268171072\n",
            "Epoch: 1413 | Loss: 0.010174764320254326 | Test loss: 0.023684436455368996\n",
            "Epoch: 1414 | Loss: 0.010141006670892239 | Test loss: 0.023642878979444504\n",
            "Epoch: 1415 | Loss: 0.010106234811246395 | Test loss: 0.023532694205641747\n",
            "Epoch: 1416 | Loss: 0.010072106495499611 | Test loss: 0.0234911497682333\n",
            "Epoch: 1417 | Loss: 0.010037708096206188 | Test loss: 0.023380953818559647\n",
            "Epoch: 1418 | Loss: 0.010003196075558662 | Test loss: 0.02333938516676426\n",
            "Epoch: 1419 | Loss: 0.009969188831746578 | Test loss: 0.023229211568832397\n",
            "Epoch: 1420 | Loss: 0.009934291243553162 | Test loss: 0.023187648504972458\n",
            "Epoch: 1421 | Loss: 0.009900657460093498 | Test loss: 0.0230774637311697\n",
            "Epoch: 1422 | Loss: 0.00986576173454523 | Test loss: 0.022967267781496048\n",
            "Epoch: 1423 | Loss: 0.009831766597926617 | Test loss: 0.022925715893507004\n",
            "Epoch: 1424 | Loss: 0.009797231294214725 | Test loss: 0.02281551994383335\n",
            "Epoch: 1425 | Loss: 0.009762861765921116 | Test loss: 0.0227739866822958\n",
            "Epoch: 1426 | Loss: 0.009728707373142242 | Test loss: 0.02266378328204155\n",
            "Epoch: 1427 | Loss: 0.009693953208625317 | Test loss: 0.022622257471084595\n",
            "Epoch: 1428 | Loss: 0.009660175070166588 | Test loss: 0.022512037307024002\n",
            "Epoch: 1429 | Loss: 0.00962528120726347 | Test loss: 0.022401850670576096\n",
            "Epoch: 1430 | Loss: 0.009591431356966496 | Test loss: 0.0223603006452322\n",
            "Epoch: 1431 | Loss: 0.009556754492223263 | Test loss: 0.022250091657042503\n",
            "Epoch: 1432 | Loss: 0.009522520937025547 | Test loss: 0.02220856584608555\n",
            "Epoch: 1433 | Loss: 0.009488226845860481 | Test loss: 0.02209835685789585\n",
            "Epoch: 1434 | Loss: 0.009453615173697472 | Test loss: 0.022056812420487404\n",
            "Epoch: 1435 | Loss: 0.009419701062142849 | Test loss: 0.021946609020233154\n",
            "Epoch: 1436 | Loss: 0.009384801611304283 | Test loss: 0.021836411207914352\n",
            "Epoch: 1437 | Loss: 0.009351085871458054 | Test loss: 0.021794861182570457\n",
            "Epoch: 1438 | Loss: 0.009316272102296352 | Test loss: 0.021684687584638596\n",
            "Epoch: 1439 | Loss: 0.009282185696065426 | Test loss: 0.021643126383423805\n",
            "Epoch: 1440 | Loss: 0.00924774818122387 | Test loss: 0.021532922983169556\n",
            "Epoch: 1441 | Loss: 0.0092132817953825 | Test loss: 0.021491389721632004\n",
            "Epoch: 1442 | Loss: 0.009179223328828812 | Test loss: 0.02138119377195835\n",
            "Epoch: 1443 | Loss: 0.009144370444118977 | Test loss: 0.021339643746614456\n",
            "Epoch: 1444 | Loss: 0.009110702201724052 | Test loss: 0.02122943475842476\n",
            "Epoch: 1445 | Loss: 0.009075800888240337 | Test loss: 0.021119248121976852\n",
            "Epoch: 1446 | Loss: 0.00904183741658926 | Test loss: 0.021077698096632957\n",
            "Epoch: 1447 | Loss: 0.009007277898490429 | Test loss: 0.0209675133228302\n",
            "Epoch: 1448 | Loss: 0.008972937241196632 | Test loss: 0.020925957709550858\n",
            "Epoch: 1449 | Loss: 0.008938746526837349 | Test loss: 0.020815754309296608\n",
            "Epoch: 1450 | Loss: 0.008904037065804005 | Test loss: 0.020774226635694504\n",
            "Epoch: 1451 | Loss: 0.008870227262377739 | Test loss: 0.02066403068602085\n",
            "Epoch: 1452 | Loss: 0.008835317566990852 | Test loss: 0.020553821697831154\n",
            "Epoch: 1453 | Loss: 0.008801509626209736 | Test loss: 0.020512282848358154\n",
            "Epoch: 1454 | Loss: 0.008766795508563519 | Test loss: 0.020402079448103905\n",
            "Epoch: 1455 | Loss: 0.008732601068913937 | Test loss: 0.020360523834824562\n",
            "Epoch: 1456 | Loss: 0.008698273450136185 | Test loss: 0.0202503502368927\n",
            "Epoch: 1457 | Loss: 0.00866369716823101 | Test loss: 0.02020878717303276\n",
            "Epoch: 1458 | Loss: 0.00862974114716053 | Test loss: 0.020098591223359108\n",
            "Epoch: 1459 | Loss: 0.008594844490289688 | Test loss: 0.019988393411040306\n",
            "Epoch: 1460 | Loss: 0.008561169728636742 | Test loss: 0.019946854561567307\n",
            "Epoch: 1461 | Loss: 0.008526316843926907 | Test loss: 0.019836658611893654\n",
            "Epoch: 1462 | Loss: 0.008492263033986092 | Test loss: 0.01979510858654976\n",
            "Epoch: 1463 | Loss: 0.008457791991531849 | Test loss: 0.019684916362166405\n",
            "Epoch: 1464 | Loss: 0.00842335820198059 | Test loss: 0.019643384963274002\n",
            "Epoch: 1465 | Loss: 0.00838927086442709 | Test loss: 0.019533175975084305\n",
            "Epoch: 1466 | Loss: 0.008354445919394493 | Test loss: 0.01949162408709526\n",
            "Epoch: 1467 | Loss: 0.008320735767483711 | Test loss: 0.019381415098905563\n",
            "Epoch: 1468 | Loss: 0.008285840973258018 | Test loss: 0.019271230325102806\n",
            "Epoch: 1469 | Loss: 0.008251926861703396 | Test loss: 0.01922968029975891\n",
            "Epoch: 1470 | Loss: 0.008217317052185535 | Test loss: 0.01911948248744011\n",
            "Epoch: 1471 | Loss: 0.00818302109837532 | Test loss: 0.01907794550061226\n",
            "Epoch: 1472 | Loss: 0.008148789405822754 | Test loss: 0.018967753276228905\n",
            "Epoch: 1473 | Loss: 0.00811411626636982 | Test loss: 0.018926197662949562\n",
            "Epoch: 1474 | Loss: 0.008080266416072845 | Test loss: 0.018815964460372925\n",
            "Epoch: 1475 | Loss: 0.008045373484492302 | Test loss: 0.018705803900957108\n",
            "Epoch: 1476 | Loss: 0.008011586964130402 | Test loss: 0.01866425946354866\n",
            "Epoch: 1477 | Loss: 0.007976839318871498 | Test loss: 0.018554067239165306\n",
            "Epoch: 1478 | Loss: 0.007942684926092625 | Test loss: 0.01851249299943447\n",
            "Epoch: 1479 | Loss: 0.007908310741186142 | Test loss: 0.018402308225631714\n",
            "Epoch: 1480 | Loss: 0.007873778231441975 | Test loss: 0.018360769376158714\n",
            "Epoch: 1481 | Loss: 0.007839785888791084 | Test loss: 0.01825057901442051\n",
            "Epoch: 1482 | Loss: 0.0078048864379525185 | Test loss: 0.01814037561416626\n",
            "Epoch: 1483 | Loss: 0.007771248463541269 | Test loss: 0.01809881255030632\n",
            "Epoch: 1484 | Loss: 0.007736357860267162 | Test loss: 0.017988651990890503\n",
            "Epoch: 1485 | Loss: 0.00770234689116478 | Test loss: 0.01794709637761116\n",
            "Epoch: 1486 | Loss: 0.007667834404855967 | Test loss: 0.01783689297735691\n",
            "Epoch: 1487 | Loss: 0.007633437868207693 | Test loss: 0.01779535412788391\n",
            "Epoch: 1488 | Loss: 0.007599309086799622 | Test loss: 0.017685145139694214\n",
            "Epoch: 1489 | Loss: 0.007564532104879618 | Test loss: 0.017643606290221214\n",
            "Epoch: 1490 | Loss: 0.0075307851657271385 | Test loss: 0.01753341034054756\n",
            "Epoch: 1491 | Loss: 0.007495882920920849 | Test loss: 0.017423201352357864\n",
            "Epoch: 1492 | Loss: 0.007462003733962774 | Test loss: 0.017381662502884865\n",
            "Epoch: 1493 | Loss: 0.00742735480889678 | Test loss: 0.017271464690566063\n",
            "Epoch: 1494 | Loss: 0.007393099367618561 | Test loss: 0.017229925841093063\n",
            "Epoch: 1495 | Loss: 0.00735883554443717 | Test loss: 0.017119724303483963\n",
            "Epoch: 1496 | Loss: 0.007324191741645336 | Test loss: 0.017078179866075516\n",
            "Epoch: 1497 | Loss: 0.007290305104106665 | Test loss: 0.016967982053756714\n",
            "Epoch: 1498 | Loss: 0.007255404256284237 | Test loss: 0.016857784241437912\n",
            "Epoch: 1499 | Loss: 0.0072216675616800785 | Test loss: 0.016816234216094017\n",
            "Epoch: 1500 | Loss: 0.007186878472566605 | Test loss: 0.01670604944229126\n",
            "Epoch: 1501 | Loss: 0.007152761332690716 | Test loss: 0.016664475202560425\n",
            "Epoch: 1502 | Loss: 0.007118356879800558 | Test loss: 0.016554314643144608\n",
            "Epoch: 1503 | Loss: 0.007083857897669077 | Test loss: 0.016512751579284668\n",
            "Epoch: 1504 | Loss: 0.007049829699099064 | Test loss: 0.016402553766965866\n",
            "Epoch: 1505 | Loss: 0.00701494887471199 | Test loss: 0.01636102795600891\n",
            "Epoch: 1506 | Loss: 0.006981306709349155 | Test loss: 0.016250818967819214\n",
            "Epoch: 1507 | Loss: 0.006946398876607418 | Test loss: 0.016140621155500412\n",
            "Epoch: 1508 | Loss: 0.0069124214351177216 | Test loss: 0.01609906554222107\n",
            "Epoch: 1509 | Loss: 0.006877870764583349 | Test loss: 0.015988875180482864\n",
            "Epoch: 1510 | Loss: 0.006843519397079945 | Test loss: 0.01594732329249382\n",
            "Epoch: 1511 | Loss: 0.006809352431446314 | Test loss: 0.015837127342820168\n",
            "Epoch: 1512 | Loss: 0.00677460664883256 | Test loss: 0.015795577317476273\n",
            "Epoch: 1513 | Loss: 0.00674082338809967 | Test loss: 0.015685390681028366\n",
            "Epoch: 1514 | Loss: 0.006705933716148138 | Test loss: 0.015575182624161243\n",
            "Epoch: 1515 | Loss: 0.006672080606222153 | Test loss: 0.015533643774688244\n",
            "Epoch: 1516 | Loss: 0.006637400947511196 | Test loss: 0.015423446893692017\n",
            "Epoch: 1517 | Loss: 0.006603178568184376 | Test loss: 0.015381896868348122\n",
            "Epoch: 1518 | Loss: 0.006568872835487127 | Test loss: 0.01527171116322279\n",
            "Epoch: 1519 | Loss: 0.006534276995807886 | Test loss: 0.015230161137878895\n",
            "Epoch: 1520 | Loss: 0.006500349845737219 | Test loss: 0.015119964256882668\n",
            "Epoch: 1521 | Loss: 0.006465443875640631 | Test loss: 0.015009790658950806\n",
            "Epoch: 1522 | Loss: 0.006431739777326584 | Test loss: 0.014968222007155418\n",
            "Epoch: 1523 | Loss: 0.006396924611181021 | Test loss: 0.014858025126159191\n",
            "Epoch: 1524 | Loss: 0.006362842861562967 | Test loss: 0.014816468581557274\n",
            "Epoch: 1525 | Loss: 0.00632839510217309 | Test loss: 0.014706283807754517\n",
            "Epoch: 1526 | Loss: 0.006293938495218754 | Test loss: 0.014664733782410622\n",
            "Epoch: 1527 | Loss: 0.006259871181100607 | Test loss: 0.01455453597009182\n",
            "Epoch: 1528 | Loss: 0.006225032266229391 | Test loss: 0.014512998051941395\n",
            "Epoch: 1529 | Loss: 0.0061913421377539635 | Test loss: 0.014402789063751698\n",
            "Epoch: 1530 | Loss: 0.006156443618237972 | Test loss: 0.014292603358626366\n",
            "Epoch: 1531 | Loss: 0.006122505757957697 | Test loss: 0.014251058921217918\n",
            "Epoch: 1532 | Loss: 0.006087916903197765 | Test loss: 0.014140862040221691\n",
            "Epoch: 1533 | Loss: 0.006053595803678036 | Test loss: 0.014099294319748878\n",
            "Epoch: 1534 | Loss: 0.006019397638738155 | Test loss: 0.013989120721817017\n",
            "Epoch: 1535 | Loss: 0.005984690971672535 | Test loss: 0.013947558589279652\n",
            "Epoch: 1536 | Loss: 0.005950864404439926 | Test loss: 0.01383737288415432\n",
            "Epoch: 1537 | Loss: 0.005915970541536808 | Test loss: 0.013727176003158092\n",
            "Epoch: 1538 | Loss: 0.00588216632604599 | Test loss: 0.013685625977814198\n",
            "Epoch: 1539 | Loss: 0.005847440101206303 | Test loss: 0.01357542909681797\n",
            "Epoch: 1540 | Loss: 0.005813261028379202 | Test loss: 0.013533895835280418\n",
            "Epoch: 1541 | Loss: 0.00577891618013382 | Test loss: 0.013423693366348743\n",
            "Epoch: 1542 | Loss: 0.00574435293674469 | Test loss: 0.013382166624069214\n",
            "Epoch: 1543 | Loss: 0.005710383411496878 | Test loss: 0.013271945528686047\n",
            "Epoch: 1544 | Loss: 0.005675490014255047 | Test loss: 0.01316176075488329\n",
            "Epoch: 1545 | Loss: 0.0056418320164084435 | Test loss: 0.01312020979821682\n",
            "Epoch: 1546 | Loss: 0.005606961902230978 | Test loss: 0.013010000810027122\n",
            "Epoch: 1547 | Loss: 0.005572921130806208 | Test loss: 0.012968474999070168\n",
            "Epoch: 1548 | Loss: 0.005538438446819782 | Test loss: 0.01285826601088047\n",
            "Epoch: 1549 | Loss: 0.005504013504832983 | Test loss: 0.012816721573472023\n",
            "Epoch: 1550 | Loss: 0.005469909869134426 | Test loss: 0.012706518173217773\n",
            "Epoch: 1551 | Loss: 0.005435112863779068 | Test loss: 0.012664979323744774\n",
            "Epoch: 1552 | Loss: 0.005401390604674816 | Test loss: 0.012554770335555077\n",
            "Epoch: 1553 | Loss: 0.0053664809092879295 | Test loss: 0.01244459766894579\n",
            "Epoch: 1554 | Loss: 0.005332584492862225 | Test loss: 0.012403035536408424\n",
            "Epoch: 1555 | Loss: 0.005297956522554159 | Test loss: 0.012292832136154175\n",
            "Epoch: 1556 | Loss: 0.005263681057840586 | Test loss: 0.012251299805939198\n",
            "Epoch: 1557 | Loss: 0.005229432135820389 | Test loss: 0.01214110292494297\n",
            "Epoch: 1558 | Loss: 0.005194769706577063 | Test loss: 0.0120995519682765\n",
            "Epoch: 1559 | Loss: 0.005160911474376917 | Test loss: 0.011989342980086803\n",
            "Epoch: 1560 | Loss: 0.0051260096952319145 | Test loss: 0.011879158206284046\n",
            "Epoch: 1561 | Loss: 0.005092238541692495 | Test loss: 0.011837607249617577\n",
            "Epoch: 1562 | Loss: 0.005057486705482006 | Test loss: 0.01172742247581482\n",
            "Epoch: 1563 | Loss: 0.00502333790063858 | Test loss: 0.011685865931212902\n",
            "Epoch: 1564 | Loss: 0.004988954868167639 | Test loss: 0.011575663462281227\n",
            "Epoch: 1565 | Loss: 0.004954436328262091 | Test loss: 0.011534136720001698\n",
            "Epoch: 1566 | Loss: 0.004920437000691891 | Test loss: 0.01142393983900547\n",
            "Epoch: 1567 | Loss: 0.004885525908321142 | Test loss: 0.011348068714141846\n",
            "Epoch: 1568 | Loss: 0.004851267673075199 | Test loss: 0.011237871833145618\n",
            "Epoch: 1569 | Loss: 0.004817450884729624 | Test loss: 0.011196320876479149\n",
            "Epoch: 1570 | Loss: 0.004782737232744694 | Test loss: 0.011086148209869862\n",
            "Epoch: 1571 | Loss: 0.0047485483810305595 | Test loss: 0.011044586077332497\n",
            "Epoch: 1572 | Loss: 0.004714212380349636 | Test loss: 0.010934382677078247\n",
            "Epoch: 1573 | Loss: 0.004679643549025059 | Test loss: 0.01089285034686327\n",
            "Epoch: 1574 | Loss: 0.004645687993615866 | Test loss: 0.010782653465867043\n",
            "Epoch: 1575 | Loss: 0.004610785748809576 | Test loss: 0.01067246776074171\n",
            "Epoch: 1576 | Loss: 0.004577117506414652 | Test loss: 0.010630893521010876\n",
            "Epoch: 1577 | Loss: 0.004542266018688679 | Test loss: 0.010520708747208118\n",
            "Epoch: 1578 | Loss: 0.00450820242986083 | Test loss: 0.010479157790541649\n",
            "Epoch: 1579 | Loss: 0.00447374302893877 | Test loss: 0.010368973016738892\n",
            "Epoch: 1580 | Loss: 0.004439301788806915 | Test loss: 0.010327416472136974\n",
            "Epoch: 1581 | Loss: 0.004405210725963116 | Test loss: 0.0102172140032053\n",
            "Epoch: 1582 | Loss: 0.004370400216430426 | Test loss: 0.01017568726092577\n",
            "Epoch: 1583 | Loss: 0.004336693324148655 | Test loss: 0.010065490379929543\n",
            "Epoch: 1584 | Loss: 0.004301781766116619 | Test loss: 0.009955281391739845\n",
            "Epoch: 1585 | Loss: 0.0042678723111748695 | Test loss: 0.009913742542266846\n",
            "Epoch: 1586 | Loss: 0.00423326063901186 | Test loss: 0.009803539142012596\n",
            "Epoch: 1587 | Loss: 0.00419896375387907 | Test loss: 0.009761983528733253\n",
            "Epoch: 1588 | Loss: 0.004164738114923239 | Test loss: 0.009651809930801392\n",
            "Epoch: 1589 | Loss: 0.004130060784518719 | Test loss: 0.009610247798264027\n",
            "Epoch: 1590 | Loss: 0.004096207674592733 | Test loss: 0.0095000509172678\n",
            "Epoch: 1591 | Loss: 0.004061309155076742 | Test loss: 0.009389853104948997\n",
            "Epoch: 1592 | Loss: 0.00402753334492445 | Test loss: 0.009348315186798573\n",
            "Epoch: 1593 | Loss: 0.00399278337135911 | Test loss: 0.009238118305802345\n",
            "Epoch: 1594 | Loss: 0.0039586266502738 | Test loss: 0.009196567349135876\n",
            "Epoch: 1595 | Loss: 0.003924256656318903 | Test loss: 0.009086376056075096\n",
            "Epoch: 1596 | Loss: 0.003889723215252161 | Test loss: 0.009044843725860119\n",
            "Epoch: 1597 | Loss: 0.0038557343650609255 | Test loss: 0.008934634737670422\n",
            "Epoch: 1598 | Loss: 0.00382082536816597 | Test loss: 0.008824437856674194\n",
            "Epoch: 1599 | Loss: 0.0037871920503675938 | Test loss: 0.008782869204878807\n",
            "Epoch: 1601 | Loss: 0.003718290477991104 | Test loss: 0.008631139993667603\n",
            "Epoch: 1602 | Loss: 0.0036837817169725895 | Test loss: 0.008520943112671375\n",
            "Epoch: 1603 | Loss: 0.003649384481832385 | Test loss: 0.008479404263198376\n",
            "Epoch: 1604 | Loss: 0.003615252673625946 | Test loss: 0.008369212970137596\n",
            "Epoch: 1605 | Loss: 0.0035804794169962406 | Test loss: 0.008327657356858253\n",
            "Epoch: 1606 | Loss: 0.0035467303823679686 | Test loss: 0.008217424154281616\n",
            "Epoch: 1607 | Loss: 0.0035118362866342068 | Test loss: 0.008107262663543224\n",
            "Epoch: 1608 | Loss: 0.003477951977401972 | Test loss: 0.008065718226134777\n",
            "Epoch: 1609 | Loss: 0.0034433044493198395 | Test loss: 0.007955526933073997\n",
            "Epoch: 1610 | Loss: 0.0034090480767190456 | Test loss: 0.007913952693343163\n",
            "Epoch: 1611 | Loss: 0.003374775405973196 | Test loss: 0.007803767919540405\n",
            "Epoch: 1612 | Loss: 0.0033401413820683956 | Test loss: 0.007762229535728693\n",
            "Epoch: 1613 | Loss: 0.0033062517177313566 | Test loss: 0.0076520382426679134\n",
            "Epoch: 1614 | Loss: 0.003271349472925067 | Test loss: 0.007541835308074951\n",
            "Epoch: 1615 | Loss: 0.0032376102171838284 | Test loss: 0.007500273175537586\n",
            "Epoch: 1616 | Loss: 0.00320282275788486 | Test loss: 0.007390111684799194\n",
            "Epoch: 1617 | Loss: 0.0031687088776379824 | Test loss: 0.007348555140197277\n",
            "Epoch: 1618 | Loss: 0.0031342990696430206 | Test loss: 0.007238352205604315\n",
            "Epoch: 1619 | Loss: 0.0030998014844954014 | Test loss: 0.0071968138217926025\n",
            "Epoch: 1620 | Loss: 0.0030657730530947447 | Test loss: 0.007086604833602905\n",
            "Epoch: 1621 | Loss: 0.0030308954883366823 | Test loss: 0.007045066449791193\n",
            "Epoch: 1622 | Loss: 0.0029972500633448362 | Test loss: 0.0069348691031336784\n",
            "Epoch: 1623 | Loss: 0.0029623478185385466 | Test loss: 0.006824660114943981\n",
            "Epoch: 1624 | Loss: 0.0029283673502504826 | Test loss: 0.006783121731132269\n",
            "Epoch: 1625 | Loss: 0.0028938204050064087 | Test loss: 0.006672924850136042\n",
            "Epoch: 1626 | Loss: 0.0028594627510756254 | Test loss: 0.006631386466324329\n",
            "Epoch: 1627 | Loss: 0.0028253011405467987 | Test loss: 0.00652118306607008\n",
            "Epoch: 1628 | Loss: 0.0027905553579330444 | Test loss: 0.0064796386286616325\n",
            "Epoch: 1629 | Loss: 0.0027567713987082243 | Test loss: 0.006369441747665405\n",
            "Epoch: 1630 | Loss: 0.0027218691539019346 | Test loss: 0.006259244866669178\n",
            "Epoch: 1631 | Loss: 0.002688030945137143 | Test loss: 0.006217694375663996\n",
            "Epoch: 1632 | Loss: 0.0026533431373536587 | Test loss: 0.006107509136199951\n",
            "Epoch: 1633 | Loss: 0.002619124948978424 | Test loss: 0.006065934896469116\n",
            "Epoch: 1634 | Loss: 0.0025848224759101868 | Test loss: 0.005955773405730724\n",
            "Epoch: 1635 | Loss: 0.0025502212811261415 | Test loss: 0.005914211273193359\n",
            "Epoch: 1636 | Loss: 0.0025162927340716124 | Test loss: 0.005804014392197132\n",
            "Epoch: 1637 | Loss: 0.0024813897907733917 | Test loss: 0.0056938170455396175\n",
            "Epoch: 1638 | Loss: 0.002447692211717367 | Test loss: 0.005652278661727905\n",
            "Epoch: 1639 | Loss: 0.0024128661025315523 | Test loss: 0.005542081780731678\n",
            "Epoch: 1640 | Loss: 0.0023787864483892918 | Test loss: 0.005500525236129761\n",
            "Epoch: 1641 | Loss: 0.002344336360692978 | Test loss: 0.005390333943068981\n",
            "Epoch: 1642 | Loss: 0.002309884177520871 | Test loss: 0.005348783917725086\n",
            "Epoch: 1643 | Loss: 0.002275817096233368 | Test loss: 0.005238586571067572\n",
            "Epoch: 1644 | Loss: 0.002240970032289624 | Test loss: 0.005197036080062389\n",
            "Epoch: 1645 | Loss: 0.0022072880528867245 | Test loss: 0.005086851306259632\n",
            "Epoch: 1646 | Loss: 0.0021723962854593992 | Test loss: 0.0049766362644732\n",
            "Epoch: 1647 | Loss: 0.002138445619493723 | Test loss: 0.004935103468596935\n",
            "Epoch: 1648 | Loss: 0.002103863749653101 | Test loss: 0.004824906587600708\n",
            "Epoch: 1649 | Loss: 0.002069540321826935 | Test loss: 0.004783356096595526\n",
            "Epoch: 1650 | Loss: 0.002035337733104825 | Test loss: 0.004673170857131481\n",
            "Epoch: 1651 | Loss: 0.0020006403792649508 | Test loss: 0.004631620831787586\n",
            "Epoch: 1652 | Loss: 0.0019668147433549166 | Test loss: 0.004521423485130072\n",
            "Epoch: 1653 | Loss: 0.0019319094717502594 | Test loss: 0.004411250352859497\n",
            "Epoch: 1654 | Loss: 0.0018981031607836485 | Test loss: 0.004369682166725397\n",
            "Epoch: 1655 | Loss: 0.0018633902072906494 | Test loss: 0.0042594848200678825\n",
            "Epoch: 1656 | Loss: 0.0018292047316208482 | Test loss: 0.004217928741127253\n",
            "Epoch: 1657 | Loss: 0.001794859766960144 | Test loss: 0.004107743501663208\n",
            "Epoch: 1658 | Loss: 0.0017603017622604966 | Test loss: 0.004066193010658026\n",
            "Epoch: 1659 | Loss: 0.0017263360787183046 | Test loss: 0.0039559961296617985\n",
            "Epoch: 1660 | Loss: 0.0016914367442950606 | Test loss: 0.0038457990158349276\n",
            "Epoch: 1661 | Loss: 0.0016577697824686766 | Test loss: 0.0038042485248297453\n",
            "Epoch: 1662 | Loss: 0.00162290851585567 | Test loss: 0.0036940635181963444\n",
            "Epoch: 1663 | Loss: 0.0015888691414147615 | Test loss: 0.003652519080787897\n",
            "Epoch: 1664 | Loss: 0.001554381800815463 | Test loss: 0.003542321966961026\n",
            "Epoch: 1665 | Loss: 0.001519959419965744 | Test loss: 0.0035007535479962826\n",
            "Epoch: 1666 | Loss: 0.0014858640497550368 | Test loss: 0.003390580415725708\n",
            "Epoch: 1667 | Loss: 0.0014510549372062087 | Test loss: 0.003349012229591608\n",
            "Epoch: 1668 | Loss: 0.0014173306990414858 | Test loss: 0.0032388330437242985\n",
            "Epoch: 1669 | Loss: 0.0013824350899085402 | Test loss: 0.0031286359298974276\n",
            "Epoch: 1670 | Loss: 0.0013485297095030546 | Test loss: 0.0030870854388922453\n",
            "Epoch: 1671 | Loss: 0.0013139061629772186 | Test loss: 0.0029768883250653744\n",
            "Epoch: 1672 | Loss: 0.0012796238297596574 | Test loss: 0.002935355994850397\n",
            "Epoch: 1673 | Loss: 0.0012453824747353792 | Test loss: 0.002825152827426791\n",
            "Epoch: 1674 | Loss: 0.0012107171351090074 | Test loss: 0.0027836263179779053\n",
            "Epoch: 1675 | Loss: 0.0011768483091145754 | Test loss: 0.0026734054554253817\n",
            "Epoch: 1676 | Loss: 0.0011419549118727446 | Test loss: 0.002563220215961337\n",
            "Epoch: 1677 | Loss: 0.0011081956326961517 | Test loss: 0.0025216699577867985\n",
            "Epoch: 1678 | Loss: 0.001073425286449492 | Test loss: 0.002411460969597101\n",
            "Epoch: 1679 | Loss: 0.0010392836993560195 | Test loss: 0.0023699342273175716\n",
            "Epoch: 1680 | Loss: 0.0010049014817923307 | Test loss: 0.002259713364765048\n",
            "Epoch: 1681 | Loss: 0.0009703792748041451 | Test loss: 0.0022181749809533358\n",
            "Epoch: 1682 | Loss: 0.0009363748249597847 | Test loss: 0.002107977867126465\n",
            "Epoch: 1683 | Loss: 0.0009014763054437935 | Test loss: 0.0020321309566497803\n",
            "Epoch: 1684 | Loss: 0.0008672110852785408 | Test loss: 0.0019219338428229094\n",
            "Epoch: 1685 | Loss: 0.0008333943551406264 | Test loss: 0.001880383468233049\n",
            "Epoch: 1686 | Loss: 0.000798691064119339 | Test loss: 0.0017701864708214998\n",
            "Epoch: 1687 | Loss: 0.0007644928991794586 | Test loss: 0.0017286359798163176\n",
            "Epoch: 1688 | Loss: 0.0007301606237888336 | Test loss: 0.0016184389824047685\n",
            "Epoch: 1689 | Loss: 0.0006955877179279923 | Test loss: 0.0015769064193591475\n",
            "Epoch: 1690 | Loss: 0.0006616368773393333 | Test loss: 0.0014667033683508635\n",
            "Epoch: 1691 | Loss: 0.0006267346325330436 | Test loss: 0.0013565004337579012\n",
            "Epoch: 1692 | Loss: 0.0005930595216341317 | Test loss: 0.001314955996349454\n",
            "Epoch: 1693 | Loss: 0.0005582116427831352 | Test loss: 0.0012047707568854094\n",
            "Epoch: 1694 | Loss: 0.0005241595208644867 | Test loss: 0.001163220382295549\n",
            "Epoch: 1695 | Loss: 0.0004896812024526298 | Test loss: 0.0010530113941058517\n",
            "Epoch: 1696 | Loss: 0.0004552476166281849 | Test loss: 0.0010114848846569657\n",
            "Epoch: 1697 | Loss: 0.00042115748510695994 | Test loss: 0.0009012639638967812\n",
            "Epoch: 1698 | Loss: 0.00038634316297248006 | Test loss: 0.0008597254636697471\n",
            "Epoch: 1699 | Loss: 0.00035263077006675303 | Test loss: 0.0007495284080505371\n",
            "Epoch: 1700 | Loss: 0.0003177322505507618 | Test loss: 0.0006393313524313271\n",
            "Epoch: 1701 | Loss: 0.0002838142099790275 | Test loss: 0.0005977809196338058\n",
            "Epoch: 1702 | Loss: 0.00024920105352066457 | Test loss: 0.00048760772915557027\n",
            "Epoch: 1703 | Loss: 0.00021491051302291453 | Test loss: 0.0004460453928913921\n",
            "Epoch: 1704 | Loss: 0.00018083005852531642 | Test loss: 0.00026807188987731934\n",
            "Epoch: 1705 | Loss: 0.00015336423530243337 | Test loss: 0.0005101740243844688\n",
            "Epoch: 1706 | Loss: 0.00019887834787368774 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1707 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1708 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1709 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1710 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1711 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1712 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1713 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1714 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1715 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1716 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1717 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1718 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1719 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1720 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1721 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1722 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1723 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1724 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1725 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1726 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1727 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1728 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1729 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1730 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1731 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1732 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1733 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1734 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1735 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1736 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1737 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1738 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1739 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1740 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1741 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1742 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1743 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1744 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1745 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1746 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1747 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1748 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1749 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1750 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1751 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1752 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1753 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1754 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1755 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1756 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1757 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1758 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1759 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1760 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1761 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1762 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1763 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1764 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1765 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1766 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1767 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1768 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1769 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1770 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1771 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1772 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1773 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1774 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1775 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1776 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1777 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1778 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1779 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1780 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1781 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1782 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1783 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1784 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1785 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1786 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1787 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1788 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1789 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1790 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1791 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1792 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1793 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1794 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1795 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1796 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1797 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1798 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1799 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1800 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1801 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1802 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1803 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1804 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1805 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1806 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1807 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1808 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1809 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1810 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1811 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1812 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1813 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1814 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1815 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1816 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1817 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1818 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1819 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1820 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1821 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1822 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1823 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1824 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1825 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1826 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1827 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1828 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1829 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1830 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1831 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1832 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1833 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1834 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1835 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1836 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1837 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1838 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1839 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1840 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1841 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1842 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1843 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1844 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1845 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1846 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1847 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1848 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1849 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1850 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1851 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1852 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1853 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1854 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1855 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1856 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1857 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1858 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1859 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1860 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1861 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1862 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1863 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1864 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1865 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1866 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1867 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1868 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1869 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1870 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1871 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1872 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1873 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1874 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1875 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1876 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1877 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1878 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1879 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1880 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1881 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1882 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1883 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1884 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1885 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1886 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1887 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1888 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1889 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1890 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1891 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1892 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1893 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1894 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1895 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1896 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1897 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1898 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1899 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1900 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1901 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1902 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1903 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1904 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1905 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1906 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1907 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1908 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1909 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1910 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1911 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1912 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1913 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1914 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1915 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1916 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1917 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1918 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1919 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1920 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1921 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1922 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1923 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1924 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1925 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1926 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1927 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1928 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1929 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1930 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1931 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1932 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1933 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1934 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1935 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1936 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1937 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1938 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1939 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1940 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1941 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1942 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1943 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1944 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1945 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1946 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1947 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1948 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1949 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1950 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1951 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1952 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1953 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1954 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1955 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1956 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1957 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1958 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1959 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1960 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1961 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1962 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1963 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1964 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1965 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1966 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1967 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1968 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1969 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1970 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1971 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1972 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1973 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1974 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1975 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1976 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1977 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1978 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1979 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1980 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1981 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1982 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1983 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1984 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1985 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1986 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1987 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1988 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1989 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1990 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1991 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1992 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1993 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1994 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1995 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1996 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1997 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 1998 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 1999 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2001 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2002 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2003 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2004 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2005 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2006 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2007 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2008 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2009 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2010 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2011 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2012 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2013 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2014 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2015 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2016 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2017 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2018 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2019 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2020 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2021 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2022 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2023 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2024 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2025 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2026 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2027 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2028 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2029 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2030 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2031 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2032 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2033 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2034 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2035 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2036 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2037 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2038 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2039 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2040 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2041 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2042 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2043 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2044 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2045 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2046 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2047 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2048 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2049 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2050 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2051 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2052 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2053 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2054 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2055 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2056 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2057 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2058 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2059 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2060 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2061 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2062 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2063 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2064 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2065 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2066 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2067 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2068 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2069 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2070 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2071 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2072 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2073 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2074 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2075 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2076 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2077 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2078 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2079 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2080 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2081 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2082 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2083 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2084 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2085 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2086 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2087 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2088 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2089 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2090 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2091 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2092 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2093 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2094 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2095 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2096 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2097 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2098 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2099 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2100 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2101 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2102 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2103 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2104 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2105 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2106 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2107 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2108 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2109 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2110 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2111 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2112 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2113 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2114 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2115 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2116 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2117 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2118 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2119 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2120 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2121 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2122 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2123 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2124 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2125 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2126 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2127 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2128 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2129 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2130 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2131 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2132 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2133 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2134 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2135 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2136 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2137 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2138 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2139 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2140 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2141 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2142 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2143 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2144 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2145 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2146 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2147 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2148 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2149 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2150 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2151 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2152 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2153 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2154 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2155 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2156 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2157 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2158 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2159 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2160 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2161 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2162 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2163 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2164 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2165 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2166 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2167 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2168 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2169 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2170 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2171 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2172 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2173 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2174 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2175 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2176 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2177 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2178 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2179 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2180 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2181 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2182 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2183 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2184 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2185 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2186 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2187 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2188 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2189 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2190 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2191 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2192 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2193 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2194 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2195 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2196 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2197 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2198 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2199 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2200 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2201 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2202 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2203 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2204 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2205 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2206 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2207 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2208 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2209 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2210 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2211 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2212 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2213 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2214 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2215 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2216 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2217 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2218 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2219 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2220 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2221 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2222 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2223 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2224 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2225 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2226 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2227 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2228 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2229 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2230 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2231 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2232 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2233 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2234 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2235 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2236 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2237 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2238 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2239 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2240 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2241 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2242 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2243 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2244 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2245 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2246 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2247 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2248 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2249 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2250 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2251 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2252 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2253 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2254 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2255 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2256 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2257 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2258 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2259 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2260 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2261 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2262 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2263 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2264 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2265 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2266 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2267 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2268 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2269 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2270 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2271 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2272 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2273 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2274 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2275 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2276 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2277 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2278 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2279 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2280 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2281 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2282 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2283 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2284 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2285 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2286 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2287 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2288 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2289 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2290 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2291 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2292 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2293 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2294 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2295 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2296 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2297 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2298 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2299 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2300 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2301 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2302 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2303 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2304 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2305 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2306 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2307 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2308 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2309 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2310 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2311 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2312 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2313 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2314 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2315 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2316 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2317 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2318 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2319 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2320 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2321 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2322 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2323 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2324 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2325 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2326 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2327 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2328 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2329 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2330 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2331 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2332 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2333 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2334 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2335 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2336 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2337 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2338 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2339 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2340 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2341 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2342 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2343 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2344 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2345 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2346 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2347 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2348 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2349 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2350 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2351 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2352 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2353 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2354 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2355 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2356 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2357 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2358 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2359 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2360 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2361 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2362 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2363 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2364 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2365 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2366 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2367 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2368 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2369 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2370 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2371 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2372 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2373 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2374 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2375 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2376 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2377 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2378 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2379 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2380 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2381 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2382 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2383 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2384 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2385 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2386 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2387 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2388 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2389 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2390 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2391 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2392 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2393 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2394 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2395 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2396 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2397 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2398 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2399 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2401 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2402 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2403 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2404 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2405 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2406 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2407 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2408 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2409 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2410 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2411 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2412 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2413 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2414 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2415 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2416 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2417 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2418 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2419 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2420 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2421 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2422 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2423 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2424 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2425 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2426 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2427 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2428 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2429 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2430 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2431 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2432 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2433 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2434 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2435 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2436 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2437 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2438 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2439 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2440 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2441 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2442 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2443 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2444 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2445 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2446 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2447 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2448 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2449 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2450 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2451 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2452 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2453 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2454 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2455 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2456 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2457 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2458 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2459 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2460 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2461 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2462 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2463 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2464 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2465 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2466 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2467 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2468 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2469 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2470 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2471 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2472 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2473 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2474 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2475 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2476 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2477 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2478 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2479 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2480 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2481 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2482 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2483 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2484 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2485 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2486 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2487 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2488 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2489 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2490 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2491 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2492 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2493 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2494 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2495 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2496 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2497 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2498 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2499 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2500 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2501 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2502 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2503 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2504 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2505 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2506 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2507 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2508 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2509 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2510 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2511 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2512 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2513 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2514 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2515 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2516 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2517 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2518 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2519 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2520 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2521 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2522 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2523 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2524 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2525 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2526 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2527 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2528 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2529 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2530 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2531 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2532 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2533 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2534 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2535 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2536 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2537 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2538 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2539 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2540 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2541 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2542 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2543 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2544 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2545 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2546 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2547 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2548 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2549 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2550 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2551 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2552 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2553 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2554 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2555 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2556 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2557 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2558 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2559 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2560 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2561 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2562 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2563 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2564 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2565 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2566 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2567 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2568 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2569 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2570 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2571 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2572 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2573 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2574 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2575 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2576 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2577 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2578 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2579 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2580 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2581 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2582 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2583 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2584 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2585 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2586 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2587 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2588 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2589 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2590 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2591 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2592 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2593 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2594 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2595 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2596 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2597 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2598 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2599 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2600 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2601 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2602 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2603 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2604 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2605 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2606 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2607 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2608 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2609 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2610 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2611 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2612 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2613 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2614 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2615 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2616 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2617 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2618 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2619 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2620 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2621 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2622 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2623 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2624 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2625 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2626 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2627 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2628 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2629 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2630 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2631 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2632 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2633 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2634 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2635 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2636 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2637 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2638 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2639 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2640 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2641 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2642 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2643 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2644 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2645 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2646 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2647 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2648 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2649 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2650 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2651 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2652 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2653 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2654 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2655 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2656 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2657 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2658 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2659 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2660 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2661 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2662 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2663 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2664 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2665 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2666 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2667 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2668 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2669 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2670 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2671 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2672 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2673 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2674 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2675 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2676 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2677 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2678 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2679 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2680 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2681 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2682 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2683 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2684 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2685 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2686 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2687 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2688 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2689 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2690 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2691 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2692 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2693 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2694 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2695 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2696 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2697 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2698 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2699 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2700 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2701 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2702 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2703 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2704 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2705 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2706 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2707 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2708 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2709 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2710 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2711 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2712 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2713 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2714 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2715 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2716 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2717 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2718 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2719 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2720 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2721 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2722 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2723 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2724 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2725 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2726 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2727 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2728 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2729 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2730 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2731 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2732 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2733 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2734 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2735 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2736 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2737 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2738 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2739 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2740 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2741 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2742 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2743 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2744 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2745 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2746 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2747 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2748 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2749 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2750 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2751 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2752 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2753 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2754 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2755 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2756 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2757 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2758 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2759 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2760 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2761 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2762 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2763 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2764 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2765 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2766 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2767 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2768 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2769 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2770 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2771 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2772 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2773 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2774 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2775 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2776 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2777 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2778 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2779 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2780 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2781 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2782 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2783 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2784 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2785 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2786 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2787 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2788 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2789 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2790 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2791 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2792 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2793 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2794 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2795 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2796 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2797 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2798 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2799 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2801 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2802 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2803 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2804 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2805 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2806 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2807 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2808 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2809 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2810 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2811 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2812 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2813 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2814 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2815 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2816 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2817 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2818 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2819 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2820 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2821 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2822 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2823 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2824 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2825 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2826 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2827 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2828 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2829 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2830 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2831 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2832 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2833 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2834 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2835 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2836 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2837 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2838 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2839 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2840 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2841 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2842 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2843 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2844 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2845 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2846 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2847 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2848 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2849 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2850 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2851 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2852 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2853 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2854 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2855 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2856 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2857 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2858 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2859 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2860 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2861 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2862 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2863 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2864 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2865 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2866 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2867 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2868 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2869 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2870 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2871 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2872 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2873 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2874 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2875 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2876 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2877 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2878 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2879 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2880 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2881 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2882 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2883 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2884 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2885 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2886 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2887 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2888 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2889 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2890 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2891 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2892 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2893 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2894 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2895 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2896 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2897 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2898 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2899 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2900 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2901 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2902 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2903 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2904 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2905 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2906 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2907 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2908 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2909 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2910 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2911 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2912 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2913 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2914 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2915 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2916 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2917 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2918 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2919 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2920 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2921 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2922 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2923 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2924 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2925 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2926 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2927 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2928 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2929 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2930 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2931 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2932 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2933 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2934 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2935 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2936 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2937 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2938 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2939 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2940 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2941 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2942 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2943 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2944 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2945 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2946 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2947 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2948 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2949 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2950 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2951 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2952 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2953 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2954 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2955 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2956 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2957 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2958 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2959 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2960 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2961 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2962 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2963 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2964 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2965 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2966 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2967 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2968 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2969 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2970 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2971 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2972 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2973 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2974 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2975 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2976 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2977 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2978 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2979 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2980 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2981 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2982 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2983 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2984 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2985 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2986 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2987 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2988 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2989 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2990 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2991 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2992 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2993 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2994 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2995 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2996 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2997 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 2998 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 2999 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3000 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3001 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3002 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3003 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3004 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3005 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3006 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3007 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3008 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3009 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3010 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3011 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3012 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3013 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3014 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3015 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3016 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3017 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3018 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3019 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3020 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3021 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3022 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3023 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3024 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3025 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3026 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3027 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3028 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3029 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3030 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3031 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3032 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3033 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3034 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3035 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3036 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3037 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3038 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3039 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3040 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3041 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3042 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3043 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3044 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3045 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3046 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3047 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3048 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3049 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3050 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3051 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3052 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3053 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3054 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3055 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3056 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3057 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3058 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3059 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3060 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3061 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3062 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3063 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3064 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3065 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3066 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3067 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3068 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3069 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3070 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3071 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3072 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3073 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3074 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3075 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3076 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3077 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3078 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3079 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3080 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3081 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3082 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3083 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3084 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3085 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3086 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3087 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3088 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3089 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3090 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3091 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3092 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3093 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3094 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3095 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3096 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3097 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3098 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3099 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3100 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3101 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3102 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3103 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3104 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3105 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3106 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3107 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3108 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3109 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3110 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3111 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3112 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3113 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3114 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3115 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3116 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3117 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3118 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3119 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3120 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3121 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3122 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3123 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3124 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3125 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3126 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3127 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3128 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3129 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3130 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3131 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3132 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3133 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3134 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3135 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3136 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3137 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3138 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3139 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3140 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3141 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3142 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3143 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3144 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3145 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3146 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3147 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3148 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3149 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3150 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3151 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3152 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3153 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3154 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3155 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3156 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3157 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3158 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3159 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3160 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3161 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3162 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3163 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3164 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3165 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3166 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3167 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3168 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3169 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3170 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3171 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3172 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3173 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3174 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3175 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3176 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3177 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3178 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3179 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3180 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3181 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3182 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3183 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3184 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3185 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3186 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3187 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3188 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3189 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3190 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3191 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3192 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3193 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3194 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3195 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3196 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3197 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3198 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3199 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3201 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3202 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3203 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3204 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3205 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3206 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3207 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3208 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3209 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3210 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3211 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3212 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3213 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3214 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3215 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3216 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3217 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3218 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3219 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3220 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3221 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3222 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3223 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3224 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3225 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3226 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3227 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3228 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3229 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3230 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3231 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3232 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3233 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3234 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3235 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3236 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3237 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3238 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3239 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3240 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3241 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3242 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3243 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3244 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3245 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3246 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3247 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3248 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3249 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3250 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3251 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3252 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3253 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3254 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3255 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3256 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3257 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3258 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3259 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3260 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3261 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3262 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3263 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3264 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3265 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3266 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3267 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3268 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3269 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3270 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3271 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3272 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3273 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3274 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3275 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3276 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3277 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3278 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3279 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3280 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3281 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3282 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3283 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3284 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3285 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3286 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3287 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3288 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3289 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3290 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3291 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3292 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3293 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3294 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3295 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3296 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3297 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3298 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3299 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3300 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3301 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3302 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3303 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3304 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3305 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3306 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3307 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3308 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3309 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3310 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3311 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3312 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3313 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3314 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3315 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3316 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3317 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3318 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3319 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3320 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3321 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3322 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3323 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3324 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3325 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3326 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3327 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3328 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3329 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3330 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3331 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3332 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3333 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3334 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3335 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3336 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3337 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3338 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3339 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3340 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3341 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3342 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3343 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3344 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3345 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3346 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3347 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3348 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3349 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3350 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3351 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3352 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3353 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3354 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3355 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3356 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3357 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3358 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3359 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3360 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3361 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3362 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3363 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3364 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3365 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3366 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3367 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3368 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3369 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3370 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3371 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3372 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3373 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3374 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3375 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3376 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3377 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3378 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3379 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3380 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3381 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3382 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3383 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3384 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3385 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3386 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3387 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3388 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3389 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3390 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3391 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3392 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3393 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3394 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3395 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3396 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3397 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3398 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3399 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3400 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3401 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3402 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3403 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3404 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3405 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3406 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3407 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3408 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3409 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3410 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3411 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3412 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3413 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3414 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3415 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3416 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3417 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3418 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3419 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3420 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3421 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3422 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3423 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3424 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3425 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3426 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3427 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3428 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3429 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3430 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3431 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3432 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3433 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3434 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3435 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3436 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3437 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3438 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3439 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3440 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3441 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3442 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3443 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3444 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3445 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3446 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3447 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3448 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3449 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3450 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3451 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3452 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3453 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3454 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3455 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3456 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3457 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3458 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3459 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3460 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3461 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3462 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3463 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3464 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3465 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3466 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3467 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3468 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3469 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3470 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3471 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3472 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3473 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3474 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3475 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3476 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3477 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3478 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3479 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3480 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3481 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3482 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3483 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3484 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3485 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3486 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3487 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3488 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3489 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3490 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3491 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3492 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3493 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3494 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3495 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3496 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3497 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3498 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3499 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3500 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3501 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3502 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3503 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3504 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3505 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3506 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3507 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3508 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3509 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3510 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3511 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3512 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3513 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3514 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3515 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3516 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3517 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3518 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3519 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3520 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3521 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3522 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3523 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3524 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3525 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3526 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3527 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3528 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3529 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3530 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3531 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3532 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3533 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3534 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3535 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3536 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3537 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3538 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3539 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3540 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3541 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3542 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3543 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3544 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3545 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3546 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3547 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3548 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3549 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3550 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3551 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3552 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3553 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3554 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3555 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3556 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3557 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3558 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3559 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3560 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3561 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3562 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3563 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3564 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3565 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3566 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3567 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3568 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3569 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3570 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3571 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3572 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3573 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3574 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3575 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3576 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3577 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3578 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3579 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3580 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3581 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3582 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3583 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3584 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3585 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3586 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3587 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3588 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3589 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3590 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3591 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3592 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3593 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3594 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3595 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3596 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3597 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3598 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3599 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3601 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3602 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3603 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3604 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3605 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3606 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3607 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3608 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3609 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3610 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3611 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3612 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3613 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3614 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3615 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3616 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3617 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3618 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3619 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3620 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3621 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3622 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3623 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3624 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3625 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3626 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3627 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3628 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3629 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3630 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3631 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3632 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3633 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3634 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3635 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3636 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3637 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3638 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3639 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3640 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3641 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3642 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3643 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3644 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3645 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3646 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3647 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3648 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3649 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3650 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3651 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3652 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3653 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3654 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3655 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3656 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3657 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3658 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3659 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3660 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3661 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3662 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3663 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3664 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3665 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3666 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3667 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3668 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3669 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3670 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3671 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3672 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3673 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3674 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3675 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3676 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3677 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3678 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3679 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3680 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3681 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3682 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3683 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3684 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3685 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3686 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3687 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3688 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3689 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3690 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3691 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3692 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3693 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3694 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3695 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3696 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3697 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3698 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3699 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3700 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3701 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3702 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3703 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3704 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3705 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3706 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3707 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3708 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3709 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3710 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3711 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3712 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3713 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3714 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3715 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3716 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3717 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3718 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3719 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3720 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3721 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3722 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3723 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3724 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3725 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3726 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3727 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3728 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3729 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3730 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3731 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3732 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3733 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3734 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3735 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3736 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3737 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3738 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3739 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3740 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3741 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3742 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3743 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3744 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3745 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3746 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3747 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3748 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3749 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3750 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3751 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3752 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3753 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3754 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3755 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3756 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3757 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3758 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3759 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3760 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3761 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3762 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3763 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3764 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3765 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3766 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3767 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3768 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3769 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3770 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3771 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3772 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3773 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3774 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3775 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3776 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3777 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3778 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3779 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3780 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3781 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3782 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3783 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3784 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3785 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3786 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3787 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3788 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3789 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3790 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3791 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3792 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3793 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3794 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3795 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3796 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3797 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3798 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3799 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3800 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3801 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3802 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3803 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3804 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3805 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3806 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3807 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3808 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3809 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3810 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3811 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3812 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3813 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3814 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3815 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3816 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3817 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3818 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3819 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3820 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3821 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3822 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3823 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3824 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3825 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3826 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3827 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3828 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3829 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3830 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3831 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3832 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3833 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3834 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3835 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3836 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3837 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3838 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3839 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3840 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3841 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3842 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3843 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3844 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3845 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3846 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3847 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3848 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3849 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3850 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3851 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3852 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3853 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3854 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3855 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3856 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3857 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3858 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3859 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3860 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3861 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3862 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3863 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3864 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3865 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3866 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3867 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3868 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3869 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3870 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3871 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3872 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3873 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3874 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3875 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3876 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3877 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3878 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3879 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3880 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3881 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3882 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3883 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3884 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3885 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3886 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3887 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3888 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3889 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3890 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3891 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3892 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3893 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3894 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3895 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3896 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3897 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3898 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3899 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3900 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3901 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3902 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3903 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3904 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3905 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3906 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3907 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3908 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3909 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3910 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3911 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3912 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3913 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3914 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3915 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3916 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3917 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3918 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3919 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3920 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3921 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3922 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3923 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3924 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3925 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3926 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3927 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3928 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3929 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3930 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3931 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3932 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3933 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3934 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3935 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3936 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3937 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3938 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3939 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3940 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3941 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3942 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3943 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3944 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3945 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3946 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3947 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3948 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3949 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3950 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3951 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3952 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3953 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3954 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3955 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3956 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3957 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3958 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3959 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3960 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3961 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3962 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3963 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3964 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3965 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3966 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3967 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3968 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3969 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3970 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3971 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3972 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3973 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3974 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3975 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3976 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3977 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3978 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3979 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3980 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3981 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3982 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3983 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3984 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3985 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3986 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3987 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3988 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3989 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3990 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3991 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3992 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3993 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3994 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3995 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3996 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3997 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n",
            "Epoch: 3998 | Loss: 0.0004444979131221771 | Test loss: 0.0005780101055279374\n",
            "Epoch: 3999 | Loss: 0.0007075972971506417 | Test loss: 0.0007690846687182784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    y_preds = model_0(X_test)\n",
        "    plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "V5BUCIY-yqcp",
        "outputId": "9e854a57-6d06-4141-e267-97ab82129ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "loss = np.array(torch.tensor(loss_values).numpy())\n",
        "epoch = np.array(torch.tensor(epoch_count).numpy())\n",
        "test_loss = np.array(torch.tensor(test_loss).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyj0RIIyHJLu",
        "outputId": "5ed4d861-4275-414e-db59-3c2b77de0186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-9b653285bc79>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_loss = np.array(torch.tensor(test_loss).numpy())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Train loss\")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "1A2a0dCEFB0s",
        "outputId": "2059dc74-4dde-4495-b7ee-d5feac3e06e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practicing writing training loops\n",
        "#s = 1000\n",
        "\n",
        "#for epoch in range(epochs):\n",
        "#  y_pred = model_0(X_train)\n",
        "#  loss = loss_fn(y_pred, X_train)\n",
        "#  optimizer.zero_grad()\n",
        "#  loss.backward()\n",
        "#  optimizer.step()\n",
        "#  if epoch % 10 == 0:\n",
        "#    print(loss)"
      ],
      "metadata": {
        "id": "QGuRMTPNaZP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving our PyTorch model\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create models directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create mode save path\n",
        "MODEL_NAME = \"LinearRegression_model3.pth\" #  @param\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(),\n",
        "          f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72-sRzxsibO6",
        "outputId": "99578c4d-aa35-47d6-9938-692568a122bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: models/LinearRegression_model3.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = LinearRegressionModel() # Set model type\n",
        "loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) # Load the model\n",
        "loaded_model.eval() # Set model to evaluation mode\n",
        "loaded_model_y_preds = loaded_model(X_test) # Use the model on the testing data\n",
        "with torch.inference_mode(): \n",
        "    loaded_model_y_preds = loaded_model(X_test)\n",
        "    plot_predictions(predictions=loaded_model_y_preds)\n",
        "# 1. Do the forward pass\n",
        "test_preds = model_0(X_test)\n",
        "\n",
        "# 2. Calculate the loss\n",
        "test_loss = loss_fn(test_preds, y_test)\n",
        "print(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "B9weW5MBmGJf",
        "outputId": "fe9bcf40-11d8-4e09-d9ef-0b3261882b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0008, grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare loaded model preds with orignal preds\n",
        "loaded_model_y_preds == y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bit7TbHtEqi",
        "outputId": "b476036d-81f1-4620-90b2-926a799c8f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aRXH1Kww9Dv",
        "outputId": "8ed01e1d-59d4-40be-c719-6751ac5fef87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8600],\n",
              "        [0.8740],\n",
              "        [0.8880],\n",
              "        [0.9020],\n",
              "        [0.9160],\n",
              "        [0.9300],\n",
              "        [0.9440],\n",
              "        [0.9580],\n",
              "        [0.9720],\n",
              "        [0.9860]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some model preds\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_y_preds = model_0(X_test)\n",
        "\n",
        "loaded_model_y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izbViZU30i90",
        "outputId": "11b8a1f6-0823-49e9-ac9e-6c7612bef3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8593],\n",
              "        [0.8733],\n",
              "        [0.8873],\n",
              "        [0.9013],\n",
              "        [0.9152],\n",
              "        [0.9292],\n",
              "        [0.9432],\n",
              "        [0.9572],\n",
              "        [0.9712],\n",
              "        [0.9852]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  plot_predictions(predictions=loaded_model_y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "I7OrCFEr2VdV",
        "outputId": "7a373b0d-3de0-480c-a5ad-ccd1be279557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAGbCAYAAADgEhWsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtBElEQVR4nO3de3wU9b3/8fcnCZfITZQAQhAQUcGAChGlVoOKVblIrUe5WIVqFR/Iqf6OqFRbQNTaVizHVrSoVax3RfRQpKjlgCJHJAEEhYBFUAEjBE+PF6hCyOf3x8Y0iUl2w+wtu6/n47GPzcx8Z+aTTIA3c/msubsAAABwcDISXQAAAEBjRpgCAAAIgDAFAAAQAGEKAAAgAMIUAABAAFmJ2nG7du28W7duido9AABAxFatWrXb3XNqW5awMNWtWzcVFRUlavcAAAARM7OP6lrGZT4AAIAACFMAAAABEKYAAAACIEwBAAAEQJgCAAAIIOzTfGb2iKRhkna5e14ty03SvZKGSNoraZy7rw5a2BdffKFdu3Zp//79QTeFFNekSRO1b99erVu3TnQpAIA0FElrhDmS7pP05zqWny+pZ8XrFEkPVLwftC+++EI7d+5U586dlZ2drVBeA77L3fXPf/5TO3bskCQCFQAg7sJe5nP3NyT9bz1DRkj6s4eskHSomR0RpKhdu3apc+fOOuSQQwhSqJeZ6ZBDDlHnzp21a9euRJcDAEhD0bhnqrOkbVWmt1fMO2j79+9XdnZ2oKKQXrKzs7kkDABIiLjegG5mV5tZkZkVlZaWhhsbp6qQCvh9AQAkSjTC1A5JXapM51bM+w53f9Dd8909Pyen1o+3AQAAaFSiEabmS7rcQk6V9Lm7l0RhuwAAAEkvbJgys6clvSXpWDPbbmZXmtk1ZnZNxZCFkrZI2izpIUkTYlZtGho3bpyGDRvWoHUGDRqkiRMnxqii+k2cOFGDBg1KyL4BAEiEsK0R3H10mOUu6dqoVdRIhbtnZ+zYsZozZ06Dt3vvvfcq9COO3Lx589SkSZMG7ysRPvzwQ3Xv3l2FhYXKz89PdDkAADRYJH2mEIGSkn9d2VywYIGuuuqqavNqPp24f//+iAJPmzZtGlzLYYcd1uB1AADAweHjZKKkY8eOla9DDz202ryvv/5ahx56qJ5++mmdddZZys7O1uzZs/XZZ59p9OjRys3NVXZ2to4//ng9+uij1bZb8zLfoEGDNGHCBN1yyy1q166d2rdvr0mTJqm8vLzamKqX+bp166Y77rhD48ePV+vWrZWbm6u777672n7ef/99FRQUqHnz5jr22GO1cOFCtWzZst6zaQcOHNCkSZPUtm1btW3bVtdff70OHDhQbcyiRYt0+umnq23btjrssMN07rnnqri4uHJ59+7dJUknn3yyzKzyEmFhYaF+8IMfqF27dmrdurW+//3v66233gp/IAAAaeX1YX1UlmF6fVifhNVAmIqjn//855owYYI2bNigH/7wh/r666/Vr18/LViwQOvXr9d1112n8ePHa/HixfVu58knn1RWVpb+53/+R/fdd5/+8z//U88++2y968ycOVN9+vTR6tWrdfPNN+umm26qDCfl5eW68MILlZWVpRUrVmjOnDm67bbb9M0339S7zXvuuUcPPfSQZs+erbfeeksHDhzQk08+WW3Mnj17dP3112vlypVaunSp2rRpo+HDh2vfvn2SpJUrV0oKha6SkhLNmzdPkvTll1/qsssu07Jly7Ry5UqdeOKJGjJkiD777LN6awIApJfTFr6nLA+9J4y7J+TVv39/r8uGDRvqXNZQEya4Z2aG3uPl+eef99CPNmTr1q0uyWfMmBF23ZEjR/qVV15ZOT127FgfOnRo5XRBQYGfeuqp1dYZPHhwtXUKCgr82muvrZzu2rWrjxo1qto6Rx99tN9+++3u7r5o0SLPzMz07du3Vy5fvny5S/JHH320zlqPOOIIv+OOOyqnDxw44D179vSCgoI61/nqq688IyPDly1b5u7/+tkUFhbWuY67e3l5uXfs2NEff/zxOsdE8/cGANA4LB2a5/tNvnRoXkz3I6nI68g0KX9mavZs6cCB0Hui1bzB+sCBA7rzzjvVt29fHX744WrZsqXmzZunjz/+uN7t9O3bt9p0p06dwn6USn3rbNy4UZ06dVLnzv9qXH/yyScrI6PuX4/PP/9cJSUlGjhwYOW8jIwMnXJK9Y9l/OCDDzRmzBj16NFDrVu3VocOHVReXh72e9y1a5fGjx+vY445Rm3atFGrVq20a9eusOsBANJLwYJ3lVXuKljwbsJqSPkb0MePDwWp8eMTXYnUokWLatMzZszQPffco3vvvVd9+vRRy5Ytdcstt4QNRjVvXDezavdMRWudaBg2bJhyc3M1e/Zsde7cWVlZWerdu3flZb66jB07Vjt37tTMmTPVrVs3NWvWTGeffXbY9QAAiLeUD1OzZoVeyejNN9/U8OHDddlll0kKXXJ9//33K29gj5fjjjtOn3zyiT755BN16tRJklRUVFRv2GrTpo2OOOIIrVixQmeddZakUP0rV67UEUeEPuf6s88+08aNG3X//ffrzDPPlCStXr1aZWVlldtp2rSpJH3nxvU333xTv//97zV06FBJ0s6dO6s9HQkAQLJI+ct8yeyYY47R4sWL9eabb2rjxo2aOHGitm7dGvc6zjnnHB177LEaO3as1q5dqxUrVug//uM/lJWVVW//rOuuu06//e1vNXfuXG3atEnXX399tcDTtm1btWvXTg899JA2b96s119/Xddcc42ysv6V4du3b6/s7Gy98sor2rlzpz7//HNJoZ/NE088oQ0bNqiwsFCjRo2qDF4AACQTwlQC/eIXv9CAAQN0/vnn64wzzlCLFi106aWXxr2OjIwMvfjii/rmm280YMAAjR07VrfeeqvMTM2bN69zvRtuuEE/+clP9NOf/lSnnHKKysvLq9WfkZGhZ599VuvWrVNeXp6uvfZa3X777WrWrFnlmKysLP3+97/Xww8/rE6dOmnEiBGSpEceeURfffWV+vfvr1GjRumKK65Qt27dYvYzAAAkj2Rod9AQ5g3srh0t+fn5XlRUVOuy4uJi9erVK84Voaq1a9fqxBNPVFFRkfr375/ociLC7w0ApIayDFOWS2UmZZUnJqfUZGar3L3Wj+rgzBQkSS+++KJeffVVbd26VUuWLNG4ceN0wgknqF+/fokuDQCQZpYPyVOZhd4bg5S/AR2R+fLLL3XzzTdr27Ztatu2rQYNGqSZM2eG/cxBAACi7ds2BwUJriNShClIki6//HJdfvnliS4DAIBGh8t8AAAAARCmAAAAAiBMAQCAuGhsLQ8iRZgCAABxcdrC95TlofdUQpgCAABx0dhaHkSKp/kAAEBcNLaWB5HizFQj1q1bN82YMSMh+x42bJjGjRuXkH0DAJBMCFNRYmb1voIEj2nTpikv77unRAsLCzVhwoQAVcfP0qVLZWbavXt3oksBACCquMwXJSUlJZVfL1iwQFdddVW1ednZ2VHfZ05OTtS3CQAAGoYzU1HSsWPHytehhx76nXlvvPGG+vfvr+bNm6t79+669dZbtW/fvsr1582bp759+yo7O1uHHXaYCgoKtHPnTs2ZM0e33Xab1q9fX3mWa86cOZK+e5nPzPTggw/q4osvVosWLXTUUUfpiSeeqFbn22+/rX79+ql58+Y66aSTtHDhQpmZli5dWuf3tnfvXo0bN04tW7ZUhw4d9Ktf/eo7Y5544gmdfPLJatWqldq3b6+LL75YO3bskCR9+OGHOvPMMyWFAmDVM3WLFi3S6aefrrZt2+qwww7Tueeeq+Li4ob++AEACZSqLQ8iRZiKg1deeUWXXnqpJk6cqPXr1+uRRx7R3Llzdcstt0iSPv30U40aNUpjx45VcXGx3njjDV122WWSpJEjR+qGG27Qscceq5KSEpWUlGjkyJF17mv69OkaMWKE1q5dq5EjR+qKK67Qxx9/LEn66quvNGzYMB133HFatWqVfvvb3+rGG28MW/+kSZP02muv6YUXXtDixYu1Zs0avfHGG9XG7Nu3T7fddpvWrl2rBQsWaPfu3Ro9erQkqUuXLnrhhRckSevXr1dJSYnuvfdeSdKePXt0/fXXa+XKlVq6dKnatGmj4cOHVwuaAIDklqotDyLm7gl59e/f3+uyYcOGOpc11IQFEzzztkyfsGBC1LYZzvPPP++hH23I6aef7tOnT6825sUXX/QWLVp4eXm5r1q1yiX5hx9+WOv2pk6d6scff/x35nft2tXvvvvuymlJPnny5Mrp/fv3e3Z2tj/++OPu7v7HP/7R27Zt63v37q0c8+STT7okX7JkSa37/vLLL71p06b+xBNPVJvXpk0bHzt2bJ0/g+LiYpfk27Ztc3f3JUuWuCQvLS2tcx1396+++sozMjJ82bJl9Y6rTTR/bwAAkVs6NM/3m3zp0LxElxIzkoq8jkyT8memZq+arQN+QLNXzU5YDatWrdKdd96pli1bVr7GjBmjPXv26NNPP9UJJ5ygwYMHKy8vTxdddJEeeOABlZaWHtS++vbtW/l1VlaWcnJytGvXLknSxo0blZeXV+3+rVNOOaXe7X3wwQfat2+fBg4cWDmvZcuW6tOn+qnc1atXa8SIEeratatatWql/Px8Sao8K1bf9seMGaMePXqodevW6tChg8rLy8OuBwBIHgUL3lVWuVe2Pkg3KR+mxvcfr0zL1Pj+4xNWQ3l5uaZOnap33nmn8rVu3Tr9/e9/V05OjjIzM/Xqq6/q1VdfVd++ffWnP/1JPXv21Nq1axu8ryZNmlSbNjOVl5dH61up1Z49e3TuuefqkEMO0eOPP67CwkItWrRIksJerhs2bJhKS0s1e/Zsvf3221qzZo2ysrK4zAcAaDRS/mm+WUNnadbQWQmtoV+/ftq4caOOPvroOseYmQYOHKiBAwdqypQpOv744/Xss8/qhBNOUNOmTXXgwIHAdRx33HF67LHH9M9//rPy7NTKlSvrXadHjx5q0qSJVqxYoaOOOkpSKDy999576tGjh6TQGa/du3frV7/6lbp37y4pdEN9VU2bNpWkat/HZ599po0bN+r++++vvEF99erVKisrC/y9AgAQLyl/ZioZTJkyRU899ZSmTJmi9957Txs3btTcuXN10003SZJWrFihO+64Q4WFhfr44481f/58bdu2Tb1795YUemrvo48+0urVq7V792598803B1XHmDFjlJmZqauuukobNmzQ3/72t8on88ys1nVatmypK6+8UjfffLNee+01rV+/XldccUW1UHTkkUeqWbNmuu+++7Rlyxa9/PLL+uUvf1ltO127dpWZ6eWXX1Zpaam++uortW3bVu3atdNDDz2kzZs36/XXX9c111yjrKyUz/gAgBRCmIqDc889Vy+//LKWLFmiAQMGaMCAAfr1r3+tI488UpLUpk0bLV++XMOGDVPPnj11ww036Je//KV+/OMfS5IuuugiDRkyRGeffbZycnL09NNPH1QdrVq10l/+8hetX79eJ510km688UZNmzZNktS8efM615sxY4bOPPNMXXjhhTrzzDOVl5enM844o3J5Tk6OHnvsMb300kvq3bu3brvtNv3ud7+rto3OnTvrtttu06233qoOHTpo4sSJysjI0LPPPqt169YpLy9P1157rW6//XY1a9bsoL4/AED0pHu7g4aw0A3q8Zefn+9FRUW1LisuLlavXr3iXFF6+q//+i9deOGF2rVrl9q1a5focgLh9wYAoqcsw5TlUplJWeWJyQrJxMxWuXt+bcs4M5VmHnvsMS1btkwffvihFixYoOuvv17Dhw9v9EEKABBdy4fkqcxC76gfN6ekmZ07d2rq1KkqKSlRx44dNXToUP3mN79JdFkAgCTzbZuDggTX0RgQptLMTTfdVHnjOwAACI7LfAAAAAEQpgAAAAIgTAEAkEZoeRB9hCkAANLIaQvfU5aH3hEdhCkAANIILQ+ij6f5AABII7Q8iD7OTDVCc+fOrfZZenPmzFHLli0DbXPp0qUyM+3evTtoeQAApBXCVBSNGzdOZiYzU5MmTXTUUUdp0qRJ2rNnT0z3O3LkSG3ZsiXi8d26ddOMGTOqzfve976nkpISHX744dEuDwCAlBZRmDKz88xsk5ltNrPJtSzvamaLzWydmS01s9zol9o4DB48WCUlJdqyZYvuuOMO3X///Zo0adJ3xpWVlSlan4uYnZ2t9u3bB9pG06ZN1bFjx2pnvAAAQHhhw5SZZUqaJel8Sb0ljTaz3jWGzZD0Z3fvK2m6pLuiXWhj0axZM3Xs2FFdunTRmDFjdOmll+qll17StGnTlJeXpzlz5qhHjx5q1qyZ9uzZo88//1xXX3212rdvr1atWqmgoEA1PwD6z3/+s7p27apDDjlEw4YN086dO6str+0y38KFC3XKKacoOztbhx9+uIYPH66vv/5agwYN0kcffaQbb7yx8iyaVPtlvnnz5qlPnz5q1qyZunTpojvvvLNaAOzWrZvuuOMOjR8/Xq1bt1Zubq7uvvvuanXMnj1bxxxzjJo3b6527drp3HPPVVlZWVR+1gCAf6HlQeJEcmZqgKTN7r7F3fdJekbSiBpjekv674qvl9SyPG1lZ2dr//79kqStW7fqqaee0vPPP6+1a9eqWbNmGjp0qHbs2KEFCxZozZo1OuOMM3TWWWeppKREkvT2229r3Lhxuvrqq/XOO+9o+PDhmjJlSr37XLRokS644AKdc845WrVqlZYsWaKCggKVl5dr3rx5ys3N1ZQpU1RSUlK5n5pWrVqliy++WD/60Y/07rvv6te//rXuuusu3XfffdXGzZw5U3369NHq1at1880366abbtJbb70lSSoqKtK1116rqVOnatOmTVq8eLHOO++8oD9SAEAtaHmQQO5e70vSv0l6uMr0ZZLuqzHmKUnXVXz9I0ku6fBatnW1pCJJRUceeaTXZcOGDXUua7AJE9wzM0PvMTZ27FgfOnRo5fTbb7/thx9+uF9yySU+depUz8rK8k8//bRy+eLFi71Fixa+d+/eats54YQT/De/+Y27u48ePdoHDx5cbfmVV17poUMX8uijj3qLFi0qp7/3ve/5yJEj66yza9eufvfdd1ebt2TJEpfkpaWl7u4+ZswYP/PMM6uNmTp1qnfu3LnadkaNGlVtzNFHH+233367u7u/8MIL3rp1a//iiy/qrCWaovp7AwCNzNKheb7f5EuH5iW6lJQkqcjryErRugF9kqQCM1uj0NOWOyQdqCW4Peju+e6en5OTE6VdhzF7tnTgQOg9DhYtWqSWLVuqefPmGjhwoM444wz94Q9/kCTl5uaqQ4cOlWNXrVqlvXv3KicnRy1btqx8vffee/rggw8kScXFxRo4cGC1fdScrmnNmjU6++yzA30fxcXFOu2006rN+/73v68dO3boiy++qJzXt2/famM6deqkXbt2SZLOOeccde3aVd27d9ell16qxx57TF9++WWgugAAtStY8K6yyr2y9QHiJ5I+UzskdakynVsxr5K7f6LQGSmZWUtJF7n7/0WpxmDGjw8FqfHj47K7M844Qw8++KCaNGmiTp06qUmTJpXLWrRoUW1seXm5OnTooGXLln1nO61bt455rQer6k3qVb+/b5eVl5dLklq1aqXVq1frjTfe0Guvvaa77rpLt9xyiwoLC9WpU6e41gwAQKxEcmaqUFJPM+tuZk0ljZI0v+oAM2tnZt9u6+eSHolumQHMmiWVlYXe4+CQQw7R0Ucfra5du34naNTUr18/7dy5UxkZGTr66KOrvb59Oq9Xr15asWJFtfVqTtd00kknafHixXUub9q0qQ4c+M6Jw2p69eql5cuXV5v35ptvKjc3V61atap33aqysrJ01lln6a677tK6deu0Z88eLViwIOL1AQBIdmHDlLuXSZoo6RVJxZKec/f1ZjbdzC6oGDZI0iYze19SB0l3xqjelDJ48GCddtppGjFihP76179q69ateuuttzR16tTKs1U/+9nP9Le//U133XWX/v73v+uhhx7Siy++WO92b731Vj3//PP6xS9+oQ0bNmj9+vWaOXOm9u7dKyn0FN6yZcu0Y8eOOpt03nDDDXr99dc1bdo0vf/++3ryySd1zz336Kabbor4+1uwYIHuvfderVmzRh999JGeeuopffnll+rVq1fE2wAAINlFdM+Uuy9092PcvYe731kxb4q7z6/4eq6796wY81N3/yaWRacKM9PChQt11lln6aqrrtKxxx6rSy65RJs2baq8DHbqqafqT3/6kx544AH17dtX8+bN07Rp0+rd7pAhQ/Tiiy/qr3/9q0466SQVFBRoyZIlysgIHe7p06dr27Zt6tGjh+q6d61fv356/vnn9cILLygvL0+TJ0/W5MmTNXHixIi/v0MPPVQvvfSSBg8erOOOO04zZszQww8/rNNPPz3ibQBAOqPdQeNgHqXGkQ2Vn5/vNfspfau4uJizF2gwfm8ApJqyDFOWS2UmZZUn5t9rhJjZKnfPr20ZHycDAECSWj4kT2UWekfyiuRpPgAAkADftjkoSHAdqB9npgAAAAIgTAEAAASQtGHq28aPQCT4fQEAJEpShqkWLVpox44d2rdvnxL1tCEaB3fXvn37tGPHju90mAeAZEXLg9SSlK0RysvLtXv3bn3++ecqKyuLc2VobLKystSmTRu1a9euspcWACQzWh40PvW1RkjKp/kyMjLUvn37yo9UAQAglSwfkqfTFr6n5UPyeFIvBSRlmAIAIJXR8iC1cE0EAAAgAMIUAABAAIQpAACAAAhTAABECS0P0hNhCgCAKDlt4XvK8tA70gdhCgCAKFk+JE9lFnpH+qA1AgAAUULLg/TEmSkAAIAACFMAAAABEKYAAAACIEwBAFCPa6+VsrJC70BtCFMAANRj9mzpwIHQO1AbwhQAAPUYP17KzAy9A7Uxd0/IjvPz872oqCgh+wYAAGgIM1vl7vm1LePMFAAAQACEKQAAgAAIUwAAAAEQpgAAaYmWB4gWwhQAIC3R8gDRQpgCAKQlWh4gWmiNAAAAEAatEQAAAGKEMAUAABAAYQoAACAAwhQAIGXQ7gCJQJgCAKQM2h0gEQhTAICUQbsDJAKtEQAAAMKgNQIAAECMEKYAAAACIEwBAAAEEFGYMrPzzGyTmW02s8m1LD/SzJaY2RozW2dmQ6JfKgAgXdHyAMks7A3oZpYp6X1J50jaLqlQ0mh331BlzIOS1rj7A2bWW9JCd+9W33a5AR0AEKmsrFDLg8xMqaws0dUgHQW9AX2ApM3uvsXd90l6RtKIGmNcUuuKr9tI+uRgiwUAoCZaHiCZZUUwprOkbVWmt0s6pcaYaZJeNbN/l9RC0uDaNmRmV0u6WpKOPPLIhtYKAEhTs2aFXkAyitYN6KMlzXH3XElDJD1uZt/Ztrs/6O757p6fk5MTpV0DAAAkTiRhaoekLlWmcyvmVXWlpOckyd3fktRcUrtoFAgAAJDMIglThZJ6mll3M2sqaZSk+TXGfCzpbEkys14KhanSaBYKAACQjMKGKXcvkzRR0iuSiiU95+7rzWy6mV1QMewGSVeZ2VpJT0sa54n6nBoAQKNBywOkAj6bDwCQMLQ8QGPBZ/MBAJISLQ+QCjgzBQAAEAZnpgAAAGKEMAUAABAAYQoAACAAwhQAIKpod4B0Q5gCAETV7NmhdgezZye6EiA+CFMAgKii3QHSDa0RAAAAwqA1AgAAQIwQpgAAAAIgTAEAAARAmAIAAAiAMAUAiAj9o4DaEaYAABGhfxRQO8IUACAi9I8CakefKQAAgDDoMwUAABAjhCkAAIAACFMAAAABEKYAIM3R8gAIhjAFAGmOlgdAMIQpAEhztDwAgqE1AgAAQBi0RgAAAIgRwhQAAEAAhCkAAIAACFMAkIJodwDED2EKAFIQ7Q6A+CFMAUAKot0BED+0RgAAAAiD1ggAAAAxQpgCAAAIgDAFAAAQAGEKABoRWh4AyYcwBQCNCC0PgORDmAKARoSWB0DyoTUCAABAGLRGAAAAiBHCFAAAQACEKQAAgAAIUwCQBGh5ADReEYUpMzvPzDaZ2WYzm1zL8plm9k7F630z+7+oVwoAKYyWB0DjFTZMmVmmpFmSzpfUW9JoM+tddYy7/z93P9HdT5T0B0nzYlArAKQsWh4AjVckZ6YGSNrs7lvcfZ+kZySNqGf8aElPR6M4AEgXs2ZJZWWhdwCNSyRhqrOkbVWmt1fM+w4z6yqpu6T/rmP51WZWZGZFpaWlDa0VAAAg6UT7BvRRkua6+4HaFrr7g+6e7+75OTk5Ud41AABA/EUSpnZI6lJlOrdiXm1GiUt8AAAgjUQSpgol9TSz7mbWVKHANL/mIDM7TlJbSW9Ft0QAaJxodwCkh7Bhyt3LJE2U9IqkYknPuft6M5tuZhdUGTpK0jOeqA/7A4AkQ7sDID1kRTLI3RdKWlhj3pQa09OiVxYANH7jx4eCFO0OgNRmiTqRlJ+f70VFRQnZNwAAQEOY2Sp3z69tGR8nAwAAEABhCgAAIADCFAAAQACEKQBoIFoeAKiKMAUADUTLAwBVEaYAoIHGj5cyM2l5ACCE1ggAAABh0BoBAAAgRghTAAAAARCmAAAAAiBMAUAFWh4AOBiEKQCoQMsDAAeDMAUAFWh5AOBg0BoBAAAgDFojAAAAxAhhCgAAIADCFAAAQACEKQApjXYHAGKNMAUgpdHuAECsEaYApDTaHQCINVojAAAAhEFrBAAAgBghTAEAAARAmAIAAAiAMAWgUaLlAYBkQZgC0CjR8gBAsiBMAWiUaHkAIFnQGgEAACAMWiMAAADECGEKAAAgAMIUAABAAIQpAEmFlgcAGhvCFICkQssDAI0NYQpAUqHlAYDGhtYIAAAAYdAaAQAAIEYIUwAAAAEQpgAAAAIgTAGIOdodAEhlhCkAMUe7AwCpLKIwZWbnmdkmM9tsZpPrGHOJmW0ws/Vm9lR0ywTQmNHuAEAqC9sawcwyJb0v6RxJ2yUVShrt7huqjOkp6TlJZ7n7P8ysvbvvqm+7tEYAAACNRdDWCAMkbXb3Le6+T9IzkkbUGHOVpFnu/g9JChekAAAAUkUkYaqzpG1VprdXzKvqGEnHmNlyM1thZufVtiEzu9rMisysqLS09OAqBgAASCLRugE9S1JPSYMkjZb0kJkdWnOQuz/o7vnunp+TkxOlXQMAACROJGFqh6QuVaZzK+ZVtV3SfHff7+5bFbrHqmd0SgSQrGh5AACRhalCST3NrLuZNZU0StL8GmNeUuislMysnUKX/bZEr0wAyYiWBwAQQZhy9zJJEyW9IqlY0nPuvt7MppvZBRXDXpH0mZltkLRE0o3u/lmsigaQHGh5AAARtEaIFVojAACAxiJoawQAAADUgTAFAAAQAGEKAAAgAMIUgGpodwAADUOYAlAN7Q4AoGEIUwCqod0BADQMrREAAADCoDUCAABAjBCmAAAAAiBMAQAABECYAtIELQ8AIDYIU0CaoOUBAMQGYQpIE7Q8AIDYoDUCAABAGLRGAAAAiBHCFAAAQACEKQAAgAAIU0AjR8sDAEgswhTQyNHyAAASizAFNHK0PACAxKI1AgAAQBi0RgAAAIgRwhQAAEAAhCkAAIAACFNAEqLdAQA0HoQpIAnR7gAAGg/CFJCEaHcAAI0HrREAAADCoDUCAABAjBCmAAAAAiBMAQAABECYAgAACIAwBcQR/aMAIPUQpoA4on8UAKQewhQQR/SPAoDUQ58pAACAMOgzBQAAECOEKQAAgAAIUwAAAAEQpoAooOUBAKQvwhQQBbQ8AID0RZgCooCWBwCQviIKU2Z2npltMrPNZja5luXjzKzUzN6peP00+qUCyWvWLKmsLPQOAEgvWeEGmFmmpFmSzpG0XVKhmc139w01hj7r7hNjUCMAAEDSiuTM1ABJm919i7vvk/SMpBGxLQsAAKBxiCRMdZa0rcr09op5NV1kZuvMbK6ZdaltQ2Z2tZkVmVlRaWnpQZQLAACQXKJ1A/pfJHVz976SXpP0WG2D3P1Bd8939/ycnJwo7RqIDdodAAAiEUmY2iGp6pmm3Ip5ldz9M3f/pmLyYUn9o1MekDi0OwAARCKSMFUoqaeZdTezppJGSZpfdYCZHVFl8gJJxdErEUgM2h0AACIR9mk+dy8zs4mSXpGUKekRd19vZtMlFbn7fEk/M7MLJJVJ+l9J42JYMxAXs2bR6gAAEJ65e0J2nJ+f70VFRQnZNwAAQEOY2Sp3z69tGR3QAQAAAiBMAQAABECYQtqh5QEAIJoIU0g7tDwAAEQTYQpph5YHAIBo4mk+AACAMHiaDwAAIEYIUwAAAAEQpgAAAAIgTCFl0PIAAJAIhCmkDFoeAAASgTCFlEHLAwBAItAaAQAAIAxaIwAAAMQIYQoAACAAwhQAAEAAhCkkNdodAACSHWEKSY12BwCAZEeYQlKj3QEAINnRGgEAACAMWiMAAADECGEKAAAgAMIUAABAAIQpJAQtDwAAqYIwhYSg5QEAIFUQppAQtDwAAKQKWiMAAACEQWsEAACAGCFMAQAABECYAgAACIAwhaii5QEAIN0QphBVtDwAAKQbwhSiipYHAIB0Q2sEAACAMGiNAAAAECOEKQAAgAAIUwAAAAEQphAW7Q4AAKgbYQph0e4AAIC6EaYQFu0OAACoG60RAAAAwgjcGsHMzjOzTWa22cwm1zPuIjNzM6t1ZwAAAKkmbJgys0xJsySdL6m3pNFm1ruWca0kXSfp7WgXCQAAkKwiOTM1QNJmd9/i7vskPSNpRC3jbpf0G0lfR7E+AACApBZJmOosaVuV6e0V8yqZWT9JXdz95fo2ZGZXm1mRmRWVlpY2uFhEFy0PAAAILvDTfGaWIel3km4IN9bdH3T3fHfPz8nJCbprBETLAwAAgoskTO2Q1KXKdG7FvG+1kpQnaamZfSjpVEnzuQk9+dHyAACA4MK2RjCzLEnvSzpboRBVKGmMu6+vY/xSSZPcvd6+B7RGAAAAjUWg1gjuXiZpoqRXJBVLes7d15vZdDO7ILqlAgAANC5ZkQxy94WSFtaYN6WOsYOClwUAANA48HEyAAAAARCmUhAtDwAAiB/CVAqi5QEAAPFDmEpBtDwAACB+wrZGiBVaIwAAgMYiUGsEAAAA1I0wBQAAEABhCgAAIADCVCNBuwMAAJITYaqRoN0BAADJiTDVSNDuAACA5ERrBAAAgDBojQAAABAjhCkAAIAACFMAAAABEKYSjJYHAAA0boSpBKPlAQAAjRthKsFoeQAAQONGawQAAIAwaI0AAAAQI4QpAACAAAhTAAAAARCmYoB2BwAApA/CVAzQ7gAAgPRBmIoB2h0AAJA+aI0AAAAQBq0RAAAAYoQwBQAAEABhCgAAIADCVAPQ8gAAANREmGoAWh4AAICaCFMNQMsDAABQE60RAAAAwqA1AgAAQIwQpgAAAAIgTAEAAARAmBItDwAAwMEjTImWBwAA4OARpkTLAwAAcPBojQAAABAGrREAAABiJKIwZWbnmdkmM9tsZpNrWX6Nmb1rZu+Y2Ztm1jv6pQIAACSfsGHKzDIlzZJ0vqTekkbXEpaecvc+7n6ipN9K+l20CwUAAEhGkZyZGiBps7tvcfd9kp6RNKLqAHf/ospkC0mJuRELAAAgziIJU50lbasyvb1iXjVmdq2ZfaDQmamfRae8g0fvKAAAEA9RuwHd3We5ew9JN0v6RW1jzOxqMysys6LS0tJo7bpW9I4CAADxEEmY2iGpS5Xp3Ip5dXlG0g9rW+DuD7p7vrvn5+TkRFzkwaB3FAAAiIdIwlShpJ5m1t3MmkoaJWl+1QFm1rPK5FBJf49eiQdn1iyprCz0DgAAECtZ4Qa4e5mZTZT0iqRMSY+4+3ozmy6pyN3nS5poZoMl7Zf0D0ljY1k0AABAsggbpiTJ3RdKWlhj3pQqX18X5boAAAAaBTqgAwAABECYAgAACIAwBQAAEABhCgAAIADCFAAAQACEKQAAgAAIUwAAAAEQpgAAAAIgTAEAAARAmAIAAAiAMAUAABAAYQoAACAAc/fE7NisVNJHMd5NO0m7Y7wPHDyOT/Li2CQ3jk9y4/gkryDHpqu759S2IGFhKh7MrMjd8xNdB2rH8UleHJvkxvFJbhyf5BWrY8NlPgAAgAAIUwAAAAGkeph6MNEFoF4cn+TFsUluHJ/kxvFJXjE5Nil9zxQAAECspfqZKQAAgJgiTAEAAASQEmHKzM4zs01mttnMJteyvJmZPVux/G0z65aAMtNWBMfnP8xsg5mtM7PFZtY1EXWmo3DHpsq4i8zMzYzHveMokuNjZpdU/PlZb2ZPxbvGdBXB32tHmtkSM1tT8XfbkETUmY7M7BEz22Vm79Wx3Mzs9xXHbp2Z9Qu6z0YfpswsU9IsSedL6i1ptJn1rjHsSkn/cPejJc2U9Jv4Vpm+Ijw+ayTlu3tfSXMl/Ta+VaanCI+NzKyVpOskvR3fCtNbJMfHzHpK+rmk09z9eEnXx7vOdBThn51fSHrO3U+SNErS/fGtMq3NkXRePcvPl9Sz4nW1pAeC7rDRhylJAyRtdvct7r5P0jOSRtQYM0LSYxVfz5V0tplZHGtMZ2GPj7svcfe9FZMrJOXGucZ0FcmfHUm6XaH/gHwdz+IQ0fG5StIsd/+HJLn7rjjXmK4iOTYuqXXF120kfRLH+tKau78h6X/rGTJC0p89ZIWkQ83siCD7TIUw1VnStirT2yvm1TrG3cskfS7p8LhUh0iOT1VXSvprTCvCt8Iem4rT313c/eV4FgZJkf3ZOUbSMWa23MxWmFl9/xtH9ERybKZJ+rGZbZe0UNK/x6c0RKCh/y6FlRWoHCCKzOzHkvIlFSS6FkhmliHpd5LGJbgU1C1LoUsVgxQ6o/uGmfVx9/9LZFGQJI2WNMfd7zGzgZIeN7M8dy9PdGGIvlQ4M7VDUpcq07kV82odY2ZZCp1y/Swu1SGS4yMzGyzpVkkXuPs3caot3YU7Nq0k5UlaamYfSjpV0nxuQo+bSP7sbJc03933u/tWSe8rFK4QW5EcmyslPSdJ7v6WpOYKfcguEi+if5caIhXCVKGknmbW3cyaKnSj3/waY+ZLGlvx9b9J+m+nW2m8hD0+ZnaSpNkKBSnu+Yifeo+Nu3/u7u3cvZu7d1PofrYL3L0oMeWmnUj+bntJobNSMrN2Cl322xLHGtNVJMfmY0lnS5KZ9VIoTJXGtUrUZb6kyyue6jtV0ufuXhJkg43+Mp+7l5nZREmvSMqU9Ii7rzez6ZKK3H2+pD8pdIp1s0I3pY1KXMXpJcLjc7eklpKer3gu4GN3vyBhRaeJCI8NEiTC4/OKpB+Y2QZJByTd6O6cdY+xCI/NDZIeMrP/p9DN6OP4T3x8mNnTCv0no13FPWtTJTWRJHf/o0L3sA2RtFnSXkk/CbxPji0AAMDBS4XLfAAAAAlDmAIAAAiAMAUAABAAYQoAACAAwhQAAEAAhCkAAIAACFMAAAAB/H9eclH/7cnW2gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}